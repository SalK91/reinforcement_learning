<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/reinforcement/11_fast_rl/">
      
      
        <link rel="prev" href="../10_offline_rl/">
      
      
        <link rel="next" href="../12_fast_mdps/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>11. Data-Efficient Reinforcement Learning - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-11-data-efficient-reinforcement-learning-bandit-foundations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              11. Data-Efficient Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs &amp; Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-multi-armed-bandit-model" class="md-nav__link">
    <span class="md-ellipsis">
      The Multi-Armed Bandit Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#baseline-approaches-and-their-regret" class="md-nav__link">
    <span class="md-ellipsis">
      Baseline Approaches and Their Regret
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimism-in-the-face-of-uncertainty" class="md-nav__link">
    <span class="md-ellipsis">
      Optimism in the Face of Uncertainty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#119-optimistic-initialization-in-greedy-bandit-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      11.9 Optimistic Initialization in Greedy Bandit Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1110-theoretical-frameworks-regret-and-pac" class="md-nav__link">
    <span class="md-ellipsis">
      11.10 Theoretical Frameworks: Regret and PAC
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-exploration-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Exploration Strategies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-multi-armed-bandit-model" class="md-nav__link">
    <span class="md-ellipsis">
      The Multi-Armed Bandit Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#baseline-approaches-and-their-regret" class="md-nav__link">
    <span class="md-ellipsis">
      Baseline Approaches and Their Regret
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimism-in-the-face-of-uncertainty" class="md-nav__link">
    <span class="md-ellipsis">
      Optimism in the Face of Uncertainty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#119-optimistic-initialization-in-greedy-bandit-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      11.9 Optimistic Initialization in Greedy Bandit Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1110-theoretical-frameworks-regret-and-pac" class="md-nav__link">
    <span class="md-ellipsis">
      11.10 Theoretical Frameworks: Regret and PAC
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-exploration-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Exploration Strategies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/reinforcement/11_fast_rl.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/reinforcement/11_fast_rl.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-11-data-efficient-reinforcement-learning-bandit-foundations">Chapter 11: Data-Efficient Reinforcement Learning — Bandit Foundations<a class="headerlink" href="#chapter-11-data-efficient-reinforcement-learning-bandit-foundations" title="Permanent link">¶</a></h1>
<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>
<h2 id="the-multi-armed-bandit-model">The Multi-Armed Bandit Model<a class="headerlink" href="#the-multi-armed-bandit-model" title="Permanent link">¶</a></h2>
<p>A multi-armed bandit is defined as a tuple <span class="arithmatex">\((\mathcal{A}, \mathcal{R})\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{A} = \{a_1, \dots, a_m\}\)</span> is a known, finite set of actions (arms),</li>
<li><span class="arithmatex">\(R_a(r) = \mathbb{P}[r \mid a]\)</span> is an unknown probability distribution over rewards for each action.</li>
<li>there is no "state".</li>
</ul>
<p>At each timestep <span class="arithmatex">\(t\)</span>, the agent:</p>
<ol>
<li>Chooses an action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span>,</li>
<li>Receives a stochastic reward <span class="arithmatex">\(r_t \sim R_{a_t}\)</span>.</li>
</ol>
<p>Goal: Maximize cumulative reward:<br>
<script type="math/tex; mode=display">
\sum_{t=1}^{T} r_t
</script>
</p>
<p>This simple model embodies the core RL challenges—particularly exploration vs. exploitation—in an isolated setting.</p>
<h3 id="evaluating-algorithms-regret-framework">Evaluating Algorithms: Regret Framework<a class="headerlink" href="#evaluating-algorithms-regret-framework" title="Permanent link">¶</a></h3>
<p>Regret: </p>
<ul>
<li><span class="arithmatex">\(Q(a) = \mathbb{E}[r \mid a]\)</span> be the expected reward for action <span class="arithmatex">\(a\)</span>,</li>
<li><span class="arithmatex">\(a^* = \arg\max_{a \in \mathcal{A}} Q(a)\)</span>,</li>
<li>Optimal Value <span class="arithmatex">\(V^* = Q(a^*)\)</span></li>
</ul>
<p>Then regret is the opportunity loss for one step:
<script type="math/tex; mode=display">
\ell_t = \mathbb{E}[V^* - Q(a_t)]
</script>
</p>
<p>Total Regret is the total opportunity loss: Total regret over <span class="arithmatex">\(T\)</span> timesteps</p>
<p>
<script type="math/tex; mode=display">
L_T = \sum_{t=1}^T \ell_t = \sum_{a \in \mathcal{A}} \mathbb{E}[N_T(a)] \cdot \Delta_a
</script>
Where:</p>
<ul>
<li><span class="arithmatex">\(N_T(a)\)</span>: Number of times arm <span class="arithmatex">\(a\)</span> is selected by time <span class="arithmatex">\(T\)</span>,</li>
<li><span class="arithmatex">\(\Delta_a = V^* - Q(a)\)</span>: Suboptimality gap.</li>
</ul>
<blockquote>
<p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>
</blockquote>
<h2 id="baseline-approaches-and-their-regret">Baseline Approaches and Their Regret<a class="headerlink" href="#baseline-approaches-and-their-regret" title="Permanent link">¶</a></h2>
<h3 id="greedy-algorithm">Greedy Algorithm<a class="headerlink" href="#greedy-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \cdot \mathbb{1}(a_\tau = a)
\]</div>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)
\]</div>
<h4 id="key-insight">Key Insight:<a class="headerlink" href="#key-insight" title="Permanent link">¶</a></h4>
<ul>
<li>Exploits current estimates.</li>
<li>May lock onto suboptimal arms due to early bad luck.</li>
<li>Linear regret in expectation.</li>
</ul>
<h3 id="example">Example:<a class="headerlink" href="#example" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\)</span>, and the first sample of <span class="arithmatex">\(a_1\)</span> yields 0, the greedy agent may ignore it indefinitely.</p>
<h3 id="varepsilon-greedy-algorithm"><span class="arithmatex">\(\varepsilon\)</span>-Greedy Algorithm<a class="headerlink" href="#varepsilon-greedy-algorithm" title="Permanent link">¶</a></h3>
<p>At each timestep:</p>
<ul>
<li>With probability <span class="arithmatex">\(1 - \varepsilon\)</span>: exploit (<span class="arithmatex">\(\arg\max \hat{Q}_t(a)\)</span>),</li>
<li>With probability <span class="arithmatex">\(\varepsilon\)</span>: explore uniformly at random.</li>
</ul>
<h4 id="performance">Performance:<a class="headerlink" href="#performance" title="Permanent link">¶</a></h4>
<ul>
<li>Guarantees exploration.</li>
<li>Linear regret unless <span class="arithmatex">\(\varepsilon\)</span> decays over time.</li>
</ul>
<h3 id="decaying-varepsilon-greedy">Decaying <span class="arithmatex">\(\varepsilon\)</span>-Greedy<a class="headerlink" href="#decaying-varepsilon-greedy" title="Permanent link">¶</a></h3>
<p>Allows <span class="arithmatex">\(\varepsilon_t \to 0\)</span> as <span class="arithmatex">\(t \to \infty\)</span>, enabling convergence.</p>
<h2 id="optimism-in-the-face-of-uncertainty">Optimism in the Face of Uncertainty<a class="headerlink" href="#optimism-in-the-face-of-uncertainty" title="Permanent link">¶</a></h2>
<p>Prefer actions with uncertain but potentially high value:</p>
<p>Why? Two possible outcomes:</p>
<ol>
<li>
<p>Getting a high reward:    If the arm really has a high mean reward.</p>
</li>
<li>
<p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p>
</li>
</ol>
<p>Algorithm: </p>
<ul>
<li>
<p>Estimate an upper confidence bound <span class="arithmatex">\(U_t(a)\)</span> for each action value, such that   <span class="arithmatex">\(Q(a) \le U_t(a)\)</span> with high probability.</p>
</li>
<li>
<p>This depends on the number of times <span class="arithmatex">\(N_t(a)\)</span> action <span class="arithmatex">\(a\)</span> has been selected.</p>
</li>
<li>
<p>Select the action maximizing the Upper Confidence Bound (UCB):</p>
</li>
</ul>
<div class="arithmatex">\[a_t = \arg\max_{a \in \mathcal{A}} \left[ U_t(a) \right]\]</div>
<blockquote>
<p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards <span class="arithmatex">\(X_i \in [0,1]\)</span>,
<script type="math/tex; mode=display">
\mathbb{P}\!\left[ \mathbb{E}[X] > \bar{X}_n + u \right]
\;\le\; \exp(-2 n u^2).
</script>
</p>
<p>Setting the right-hand side equal to <span class="arithmatex">\(\delta\)</span> and solving for <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
u = \sqrt{\frac{\log(1/\delta)}{2n}}.
</script>
Here, <span class="arithmatex">\(\delta\)</span> is the failure probability, and the confidence interval
holds with probability at least <span class="arithmatex">\(1 - \delta\)</span>.
This means that, with probability at least <span class="arithmatex">\(1 - \delta\)</span>,
<script type="math/tex; mode=display">
\bar{X}_n - u \;\le\; \mathbb{E}[X] \;\le\; \bar{X}_n + u.
</script>
</p>
</blockquote>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \left[ \hat{Q}_t(a) + \text{UCB}_t(a) \right]
\]</div>
<h3 id="ucb1-algorithm">UCB1 Algorithm<a class="headerlink" href="#ucb1-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\text{UCB}_t(a) = \hat{Q}_t(a) + \sqrt{\frac{2 \log \frac{1}{\delta} }{N_t(a)}}
\]</div>
<ul>
<li>where <span class="arithmatex">\(\hat{Q}_t(a)\)</span> is empirical average</li>
<li><span class="arithmatex">\(N_t(a)\)</span> is number of samples of <span class="arithmatex">\(a\)</span> after <span class="arithmatex">\(t\)</span> timesteps.</li>
<li>Provable sublinear regret.</li>
<li>Balances estimated value and exploration bonus.</li>
</ul>
<p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p>
<p>1: Initialize for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span>:  <span class="arithmatex">\(\quad N(a) \leftarrow 0,\;\; \hat{Q}(a) \leftarrow 0\)</span>
2: Warm start (sample each arm once):<br>
3: for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span> do<br>
4: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a\)</span>, observe reward <span class="arithmatex">\(r \in [0,1]\)</span><br>
5: <span class="arithmatex">\(\quad N(a) \leftarrow 1\)</span><br>
6: <span class="arithmatex">\(\quad \hat{Q}(a) \leftarrow r\)</span><br>
7: end for<br>
8: Set <span class="arithmatex">\(t \leftarrow |\mathcal{A}|\)</span></p>
<p>9: for <span class="arithmatex">\(t = |\mathcal{A}|+1, |\mathcal{A}|+2, \dots\)</span> do<br>
10: <span class="arithmatex">\(\quad\)</span> Compute UCB for each arm: <span class="arithmatex">\(\quad \mathrm{UCB}_t(a) = \hat{Q}(a) + \sqrt{\frac{2\log t}{N(a)}}\)</span></p>
<p>11: <span class="arithmatex">\(\quad\)</span> Select action:<span class="arithmatex">\(\quad a_t \leftarrow \arg\max_{a \in \mathcal{A}} \mathrm{UCB}_t(a)\)</span></p>
<p>12: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span></p>
<p>13: <span class="arithmatex">\(\quad\)</span> Update count: <span class="arithmatex">\(\quad N(a_t) \leftarrow N(a_t) + 1\)</span></p>
<p>14: <span class="arithmatex">\(\quad\)</span> Update empirical mean (incremental):<br>
<script type="math/tex; mode=display">
\quad \hat{Q}(a_t) \leftarrow \hat{Q}(a_t) + \frac{1}{N(a_t)}\Big(r_t - \hat{Q}(a_t)\Big)
</script>
</p>
<p>15: end for</p>
<h2 id="119-optimistic-initialization-in-greedy-bandit-algorithms">11.9 Optimistic Initialization in Greedy Bandit Algorithms<a class="headerlink" href="#119-optimistic-initialization-in-greedy-bandit-algorithms" title="Permanent link">¶</a></h2>
<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>
<h3 id="motivation">Motivation<a class="headerlink" href="#motivation" title="Permanent link">¶</a></h3>
<p>Greedy algorithms, by default, select actions with the highest estimated value:</p>
<div class="arithmatex">\[
a_t = \arg\max_a \hat{Q}_t(a)
\]</div>
<p>If these <span class="arithmatex">\(\hat{Q}_t(a)\)</span> estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>
<h3 id="algorithmic-details">Algorithmic Details<a class="headerlink" href="#algorithmic-details" title="Permanent link">¶</a></h3>
<p>We initialize:</p>
<ul>
<li><span class="arithmatex">\(\hat{Q}_0(a) = Q_{\text{init}}\)</span> for all <span class="arithmatex">\(a \in \mathcal{A}\)</span>, where <span class="arithmatex">\(Q_{\text{init}}\)</span> is set higher than any reasonable expected reward (e.g., <span class="arithmatex">\(Q_{\text{init}} = 1\)</span> if rewards are bounded in <span class="arithmatex">\([0, 1]\)</span>).</li>
<li><span class="arithmatex">\(N(a) = 1\)</span> to ensure initial update is well-defined.</li>
</ul>
<p>Then we update action values using an incremental Monte Carlo estimate:</p>
<div class="arithmatex">\[
\hat{Q}_{t}(a_t) = \hat{Q}_{t-1}(a_t) + \frac{1}{N_t(a_t)} \left( r_t - \hat{Q}_{t-1}(a_t) \right)
\]</div>
<p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p>
<ul>
<li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li>
<li>Simple to implement: No need for tuning <span class="arithmatex">\(\varepsilon\)</span> or computing uncertainty estimates.</li>
<li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li>
</ul>
<h4 id="key-design-considerations">Key Design Considerations<a class="headerlink" href="#key-design-considerations" title="Permanent link">¶</a></h4>
<ul>
<li>How optimistic is optimistic enough?<br>
  If <span class="arithmatex">\(Q_{\text{init}}\)</span> is not much larger than the true values, the agent may not explore effectively.</li>
<li>What if <span class="arithmatex">\(Q_{\text{init}}\)</span> is too high?<br>
  Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li>
</ul>
<h4 id="function-approximation">Function Approximation<a class="headerlink" href="#function-approximation" title="Permanent link">¶</a></h4>
<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>
<h2 id="1110-theoretical-frameworks-regret-and-pac">11.10 Theoretical Frameworks: Regret and PAC<a class="headerlink" href="#1110-theoretical-frameworks-regret-and-pac" title="Permanent link">¶</a></h2>
<h3 id="regret-based-evaluation">Regret-Based Evaluation<a class="headerlink" href="#regret-based-evaluation" title="Permanent link">¶</a></h3>
<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p>
<ul>
<li>Many small mistakes (frequent near-optimal actions),</li>
<li>A few large mistakes (infrequent but very suboptimal actions).</li>
</ul>
<p>Minimizing regret growth with <span class="arithmatex">\(T\)</span> is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>
<h3 id="probably-approximately-correct-pac-framework">Probably Approximately Correct (PAC) Framework<a class="headerlink" href="#probably-approximately-correct-pac-framework" title="Permanent link">¶</a></h3>
<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p>
<p>An algorithm is <span class="arithmatex">\((\varepsilon, \delta)\)</span>-PAC if, on each time step <span class="arithmatex">\(t\)</span>, it chooses an action <span class="arithmatex">\(a_t\)</span> such that:</p>
<div class="arithmatex">\[
Q(a_t) \ge Q(a^*) - \varepsilon \quad \text{with probability at least } 1 - \delta
\]</div>
<p>on all but a polynomial number of time steps (in <span class="arithmatex">\(|\mathcal{A}|\)</span>, <span class="arithmatex">\(1/\varepsilon\)</span>, <span class="arithmatex">\(1/\delta\)</span>, etc). This ensures:</p>
<ul>
<li>The agent almost always behaves nearly optimally,</li>
<li>With high probability, after a reasonable amount of time.</li>
</ul>
<p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>
<h2 id="comparing-exploration-strategies">Comparing Exploration Strategies<a class="headerlink" href="#comparing-exploration-strategies" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Regret Behavior</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Greedy</td>
<td>Linear</td>
<td>No exploration mechanism</td>
</tr>
<tr>
<td>Constant <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Linear</td>
<td>Fixed chance of exploring</td>
</tr>
<tr>
<td>Decaying <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Sublinear (if tuned)</td>
<td>Requires prior knowledge of reward gaps</td>
</tr>
<tr>
<td>Optimistic Initialization</td>
<td>Sublinear (if optimistic enough)</td>
<td>Simple, effective in tabular settings</td>
</tr>
</tbody>
</table>
<p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>
<h2 id="bayesian-bandits">Bayesian Bandits<a class="headerlink" href="#bayesian-bandits" title="Permanent link">¶</a></h2>
<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in <span class="arithmatex">\([0,1]\)</span>). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>
<h3 id="key-idea-maintain-beliefs-over-arm-reward-distributions">Key Idea: Maintain Beliefs Over Arm Reward Distributions<a class="headerlink" href="#key-idea-maintain-beliefs-over-arm-reward-distributions" title="Permanent link">¶</a></h3>
<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter <span class="arithmatex">\(\\phi_i\)</span> for arm <span class="arithmatex">\(i\)</span>. Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of <span class="arithmatex">\(\\phi_i\)</span>, representing our uncertainty.</p>
<h4 id="prior-and-posterior">Prior and Posterior<a class="headerlink" href="#prior-and-posterior" title="Permanent link">¶</a></h4>
<ul>
<li>Prior: Our initial belief about <span class="arithmatex">\(\\phi_i\)</span> is encoded in a probability distribution <span class="arithmatex">\(p(\\phi_i)\)</span>.</li>
<li>Data: After pulling arm <span class="arithmatex">\(i\)</span> and observing reward <span class="arithmatex">\(r_{i1}\)</span>, we update our belief.</li>
<li>Posterior: The new belief is computed using Bayes' rule:</li>
</ul>
<div class="arithmatex">\[p(\phi_i \mid r_{i1}) =
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
p(r_{i1})
}
=
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
\int p(r_{i1} \mid \phi_i)\, p(\phi_i)\, d\phi_i
}\]</div>
<p>This posterior becomes the new prior for future updates as more data arrives.</p>
<h3 id="practical-considerations">Practical Considerations<a class="headerlink" href="#practical-considerations" title="Permanent link">¶</a></h3>
<p>Computing the posterior <span class="arithmatex">\(p(\phi_i \mid D)\)</span> (where <span class="arithmatex">\(D\)</span> is the observed data for arm <span class="arithmatex">\(i\)</span>) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p>
<ul>
<li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li>
<li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li>
</ul>
<h3 id="why-use-bayesian-bandits">Why Use Bayesian Bandits?<a class="headerlink" href="#why-use-bayesian-bandits" title="Permanent link">¶</a></h3>
<ul>
<li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li>
<li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li>
<li>Captures uncertainty in a principled and statistically coherent manner.</li>
</ul>
<h3 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian bandits treat the reward-generating parameters <span class="arithmatex">\(\phi_i\)</span> as random variables.</li>
<li>We maintain a posterior belief <span class="arithmatex">\(p(\phi_i \mid D)\)</span> using Bayes' rule.</li>
<li>When conjugate priors are used, analytical updates are possible.</li>
<li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li>
</ul>
<h3 id="thompson-sampling">Thompson Sampling:<a class="headerlink" href="#thompson-sampling" title="Permanent link">¶</a></h3>
<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p>
<p>This approach is also known as <em>probability matching</em>: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn’t deterministically select the arm with the highest mean—it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p>
<p>Algorithm: Thompson Sampling:</p>
<p>1: Initialize prior over each arm <span class="arithmatex">\(a\)</span>, <span class="arithmatex">\(p(\mathcal{R}_a)\)</span><br>
2: for iteration <span class="arithmatex">\(= 1, 2, \dots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> For each arm <span class="arithmatex">\(a\)</span> sample a reward distribution <span class="arithmatex">\(\mathcal{R}_a\)</span> from posterior<br>
4: <span class="arithmatex">\(\quad\)</span> Compute action-value function <span class="arithmatex">\(Q(a) = \mathbb{E}[\mathcal{R}_a]\)</span><br>
5: <span class="arithmatex">\(\quad a_t \equiv \arg\max_{a \in \mathcal{A}} Q(a)\)</span><br>
6: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> Update posterior <span class="arithmatex">\(p(\mathcal{R}_a)\)</span> using Bayes Rule<br>
8: end for  </p>
<h3 id="contextual-bandits">Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permanent link">¶</a></h3>
<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context <span class="arithmatex">\(x_t\)</span> drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p>
<p>In this setting, the goal is to learn a context-dependent policy <span class="arithmatex">\(\pi(a \mid x)\)</span> that maps the observed context <span class="arithmatex">\(x_t\)</span> to a suitable arm <span class="arithmatex">\(a_t\)</span>, maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p>
<p>Formally, the interaction at each time step <span class="arithmatex">\(t\)</span> is:</p>
<ol>
<li>Observe context <span class="arithmatex">\(x_t \in \mathcal{X}\)</span></li>
<li>Choose action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span> based on policy <span class="arithmatex">\(\pi(a \mid x_t)\)</span></li>
<li>Receive reward <span class="arithmatex">\(r_t(a_t, x_t)\)</span></li>
</ol>
<p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p>
<div class="arithmatex">\[
\pi^*(x) = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a, x)]
\]</div>
<p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>
<h2 id="mental-map">Mental Map<a class="headerlink" href="#mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                Bandits: Foundations of Data-Efficient RL
     Goal: Understand exploration-exploitation in simplest setting
           Learn to act with minimal data through principled tradeoffs
                                │
                                ▼
               What Are Multi-Armed Bandits (MAB)?
 ┌─────────────────────────────────────────────────────────────┐
 │ Single-state (stateless) decision problems                  │
 │ Fixed set of actions (arms)                                 │
 │ Unknown reward distribution per arm                         │
 │ Choose an action, receive reward, repeat                    │
 │ No transition dynamics — unlike full RL                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Core Objective: Maximize Reward
 ┌─────────────────────────────────────────────────────────────┐
 │ Maximize total reward = minimize regret                     │
 │ Regret = missed opportunity vs optimal action               │
 │ Total regret used to evaluate algorithm efficiency          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                  Basic Bandit Algorithms
 ┌─────────────────────────────────────────────────────────────┐
 │ Greedy: exploit current best estimates (linear regret)      │
 │ ε-Greedy: random exploration with fixed ε                   │
 │ Decaying ε-Greedy: reduces ε over time                      │
 │ Optimistic Initialization: set high initial Q̂ values        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Principle: Optimism in the Face of Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat unvisited arms as potentially good                    │
 │ Upper Confidence Bound (UCB) algorithms                     │
 │ Tradeoff: mean reward + exploration bonus                   │
 │ Guarantees sublinear regret                                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Algorithmic Realization: UCB1
 ┌─────────────────────────────────────────────────────────────┐
 │ UCB_t(a) = Q̂_t(a) + √(2 log t / N_t(a))                     │
 │ Encourages pulling uncertain arms early                     │
 │ Regret ≈ O(√(T log T))                                      │
 │ Theoretically grounded and simple to implement              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Theoretical Frameworks: Regret vs PAC
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap from always acting optimally         │
 │ PAC: guarantees near-optimal behavior with high probability │
 │ Regret cares about sum of mistakes; PAC focuses on steps    │
 │ Both evaluate quality and efficiency of learning            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Bayesian Bandits and Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat arm rewards as random variables                       │
 │ Use prior + observed data → posterior via Bayes rule        │
 │ Conjugate priors simplify computation                       │
 │ Enable principled uncertainty reasoning                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Thompson Sampling (Bayesian)
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample reward distribution from posterior per arm           │
 │ Pull arm with highest sampled reward                        │
 │ Probabilistic optimism: match probability of being best     │
 │ Natural exploration and strong empirical performance        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Probability Matching Perspective
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ sample optimal arm w/ correct frequency │
 │ Avoids hard-coded uncertainty bonuses                       │
 │ Simpler and often better in practice                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                       Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Input context x_t at each timestep                          │
 │ Reward distribution depends on (action, context)            │
 │ Learn policy π(a | x): context-aware decision making        │
 │ Real-world applications: ads, medicine, personalization     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Summary: Bandits as Foundation for Efficient RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Bandits isolate the exploration-exploitation tradeoff       │
 │ Simpler than full RL, but deeply insightful                 │
 │ Concepts generalize to value estimation, uncertainty        │
 │ Key tools: regret, PAC bounds, posterior reasoning          │
 └─────────────────────────────────────────────────────────────┘
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../10_offline_rl/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 10. Offline Reinforcement Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                10. Offline Reinforcement Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../12_fast_mdps/" class="md-footer__link md-footer__link--next" aria-label="Next: 12. Fast Reinforcement Learning in MDPs and Generalization">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                12. Fast Reinforcement Learning in MDPs and Generalization
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>