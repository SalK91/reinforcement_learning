<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/reinforcement/9_rlhf/">
      
      
        <link rel="prev" href="../8_imitation_learning/">
      
      
        <link rel="next" href="../10_offline_rl/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>9. RLHF - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              9. RLHF
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs &amp; Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bradleyterry-preference-modeling-in-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      Bradley–Terry Preference Modeling in RLHF
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-rlhf-training-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      The RLHF Training Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#direct-preference-optimization-rlhf-without-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Direct Preference Optimization: RLHF without RL?
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bradleyterry-preference-modeling-in-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      Bradley–Terry Preference Modeling in RLHF
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-rlhf-training-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      The RLHF Training Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#direct-preference-optimization-rlhf-without-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Direct Preference Optimization: RLHF without RL?
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/reinforcement/9_rlhf.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/reinforcement/9_rlhf.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment">Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment<a class="headerlink" href="#chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment" title="Permanent link">¶</a></h1>
<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for “good” behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment – ensuring an AI’s behavior aligns with human preferences and values – by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute “score” to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p>
<p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>
<h2 id="bradleyterry-preference-modeling-in-rlhf">Bradley–Terry Preference Modeling in RLHF<a class="headerlink" href="#bradleyterry-preference-modeling-in-rlhf" title="Permanent link">¶</a></h2>
<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley–Terry model, a probabilistic model for noisy comparisons. </p>
<p>Consider a <span class="arithmatex">\(K\)</span>-armed bandit with actions <span class="arithmatex">\(b_1, b_2, \dots, b_K\)</span>, and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action <span class="arithmatex">\(b_i\)</span> over <span class="arithmatex">\(b_j\)</span> is modeled as:</p>
<div class="arithmatex">\[
P(b_i \succ b_j)
=
\frac{\exp(r(b_i))}{\exp(r(b_i)) + \exp(r(b_j))}
=
p_{ij}
\]</div>
<p>where <span class="arithmatex">\(r(b)\)</span> is an unobserved scalar reward associated with action <span class="arithmatex">\(b\)</span>. Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p>
<p>Assume we collect a dataset <span class="arithmatex">\(\mathcal{D}\)</span> of <span class="arithmatex">\(N\)</span> comparisons of the form <span class="arithmatex">\((b_i, b_j, \mu)\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mu(1) = 1\)</span> if the human marked <span class="arithmatex">\(b_i \succ b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0.5\)</span> if the human marked <span class="arithmatex">\(b_i = b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0\)</span> if the human marked <span class="arithmatex">\(b_j \succ b_i\)</span></li>
</ul>
<p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p>
<div class="arithmatex">\[
\mathcal{L}
=
-
\sum_{(b_i,b_j,\mu)\in\mathcal{D}}
\left[
\mu(1)\log P(b_i \succ b_j)
+
\mu(2)\log P(b_j \succ b_i)
\right]
\]</div>
<p>Optimizing this loss adjusts the reward function <span class="arithmatex">\(r(\cdot)\)</span> so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p>
<p>Once the reward model is trained using the Bradley–Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley–Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>
<h2 id="the-rlhf-training-pipeline">The RLHF Training Pipeline<a class="headerlink" href="#the-rlhf-training-pipeline" title="Permanent link">¶</a></h2>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../images/llm_rlhf.png" data-desc-position="bottom"><img alt="Alt text" src="../images/llm_rlhf.png"></a></p>
<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p>
<ol>
<li>
<p>Supervised Fine-Tuning (SFT) – Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p>
</li>
<li>
<p>Reward Model Training from Human Preferences – Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function <span class="arithmatex">\(r_\phi(x,y)\)</span> (parameterized by <span class="arithmatex">\(\phi\)</span>) that predicts which response is more preferable for a given input x using Bradley–Terry model.</p>
</li>
<li>
<p>Reinforcement Learning Fine-Tuning – In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy <span class="arithmatex">\(\pi_\theta(y|x)\)</span> (with parameters <span class="arithmatex">\(\theta\)</span>) is updated to maximize the expected reward <span class="arithmatex">\(r_\phi(x,y)\)</span> of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model’s score, the policy might exploit flaws in <span class="arithmatex">\(r_\phi\)</span> (a form of “reward hacking”) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback–Leibler (KL) penalty that keeps the new policy <span class="arithmatex">\(\pi_\theta\)</span> close to the reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span> (often the SFT model). In summary, the RL objective can be written as:</p>
<div class="arithmatex">\[\max_{\pi_\theta} (
\underbrace{
\mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi_\theta(y \mid x)}
}_{\text{Sample from policy}}
\left[
\underbrace{
r_\phi(x,y)
}_{\text{Want high reward}}
\right]
-
\underbrace{
\beta \, \mathbb{D}_{\mathrm{KL}}
\left[
\pi_\theta(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x)
\right]
}_{\text{Keep KL to original model small}})
\]</div>
<p>where <span class="arithmatex">\(\beta&gt;0\)</span> controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if <span class="arithmatex">\(\pi_\theta\)</span> deviates too much from the original model’s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p>
</li>
</ol>
<p>Through this pipeline – SFT, reward modeling, and RL fine-tuning – we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like <span class="arithmatex">\(\beta\)</span> and PPO clip thresholds). The process can be unstable; for instance, if <span class="arithmatex">\(\beta\)</span> is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if <span class="arithmatex">\(\beta\)</span> is too high, the policy might hardly improve at all. Researchers have described RLHF as a “complex and often unstable procedure” that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>
<h2 id="direct-preference-optimization-rlhf-without-rl">Direct Preference Optimization: RLHF without RL?<a class="headerlink" href="#direct-preference-optimization-rlhf-without-rl" title="Permanent link">¶</a></h2>
<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p>
<p>Recall that in the RLHF setting, our goal is to find a policy <span class="arithmatex">\(\pi^*(y|x)\)</span> that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of <span class="arithmatex">\(J(\pi)\)</span> occurs when <span class="arithmatex">\(\pi\)</span> is proportional to the reference policy times an exponential of the reward:</p>
<div class="arithmatex">\[\pi^*(y \mid x) \propto \pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{1}{\beta}\, r_\phi(x, y)\right)\]</div>
<p>This equation gives a closed-form solution for the optimal policy in terms of the reward function <span class="arithmatex">\(r_\phi\)</span>. It makes sense: actions <span class="arithmatex">\(y\)</span> that have higher human-derived reward should be taken with higher probability, but we temper this by <span class="arithmatex">\(\beta\)</span> and weight by the reference probabilities <span class="arithmatex">\(\pi_{\text{ref}}(y|x)\)</span> so that we don’t stray too far. If we were to normalize the right-hand side, we’d write:</p>
<div class="arithmatex">\[\pi^*(y \mid x)
=
\frac{
\pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{r_\phi(x,y)}{\beta}\right)
}{
\sum_{y'} \pi_{\text{ref}}(y' \mid x)\,
\exp\!\left(\frac{r_\phi(x,y')}{\beta}\right)
}\]</div>
<p>Here the denominator is a partition functionsumming over all possible responses <span class="arithmatex">\(y'\)</span> for input <span class="arithmatex">\(x\)</span>. This normalization involves a sum over the entire response space, which is astronomically large for language models – hence we cannot directly compute <span class="arithmatex">\(\pi^*(y|x)\)</span> in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p>
<p>DPO’s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses <span class="arithmatex">\(y_+\)</span> (preferred) and <span class="arithmatex">\(y_-\)</span> (dispreferred) for the same context <span class="arithmatex">\(x\)</span>, the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p>
<div class="arithmatex">\[\frac{\pi^*(y^+ \mid x)}{\pi^\ast(y^- \mid x)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^+)}{\beta}\right)}
{\pi_{\text{ref}}(y^- \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^-)}{\beta}\right)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)}
{\pi_{\text{ref}}(y^- \mid x)}
\exp\!\left(
\frac{1}{\beta}
\big[
r_\phi(x, y^+) - r_\phi(x, y^-)
\big]
\right)\]</div>
<p>Taking the log of both sides, we get a neat relationship:</p>
<div class="arithmatex">\[\frac{1}{\beta}
\big( r_\phi(x, y^{+}) - r_\phi(x, y^{-}) \big)
=
\big[ \log \pi^\ast(y^{+} \mid x) - \log \pi^\ast(y^{-} \mid x) \big]
-
\big[ \log \pi_{\text{ref}}(y^{+} \mid x) - \log \pi_{\text{ref}}(y^{-} \mid x) \big]\]</div>
<p>The term in brackets on the right is the difference in log-probabilities that the optimal policy <span class="arithmatex">\(\pi^*\)</span> assigns to the two responses (which in turn would equal the difference in our learned policy’s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if <span class="arithmatex">\(y_+\)</span> is better than <span class="arithmatex">\(y_-\)</span> by some amount of reward, then the optimal policy should tilt its probabilities in favor of <span class="arithmatex">\(y_+\)</span> by a corresponding factor.</p>
<p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts <span class="arithmatex">\(\pi_\theta\)</span> to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context <span class="arithmatex">\(x\)</span> and two candidate outputs <span class="arithmatex">\(y_+\)</span> (human-preferred) and <span class="arithmatex">\(y_-\)</span> (human-dispreferred), we want the model to assign a higher probability to <span class="arithmatex">\(y_+\)</span> than to <span class="arithmatex">\(y_-\)</span>, with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p>
<p>In practice, the DPO loss for a pair <span class="arithmatex">\((x, y_+, y_-)\)</span> is something like:</p>
<div class="arithmatex">\[\ell_{\text{DPO}}(\theta)
= - \log \sigma \!\left(
\beta\,
\big[ \log \pi_\theta(y^{+} \mid x) - \log \pi_\theta(y^{-} \mid x) \big]
\right)\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is the sigmoid function. This loss is low (i.e. good) when <span class="arithmatex">\(\log \pi_\theta(y_+|x) \gg \log \pi_\theta(y_-|x)\)</span>, meaning the model assigns much higher probability to the preferred outcome – which is what we want. If the model hasn’t yet learned the preference, the loss will be higher, and gradient descent on this loss will push <span class="arithmatex">\(\pi_\theta\)</span> to increase the probability of <span class="arithmatex">\(y_+\)</span> and decrease that of <span class="arithmatex">\(y_-\)</span>. Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy’s logits: effectively, <span class="arithmatex">\(\log \pi_\theta(y|x)\)</span> plays the role of a reward score for how good <span class="arithmatex">\(y\)</span> is, up to the scaling factor <span class="arithmatex">\(1/\beta\)</span>. In fact, the DPO derivation can be seen as combining the preference loss on <span class="arithmatex">\(r_\phi\)</span> with the <span class="arithmatex">\(\pi^*\)</span> solution formula to produce a preference loss on <span class="arithmatex">\(\pi_\theta\)</span>. The original DPO paper calls this approach “your language model is secretly a reward model” – by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p>
<p>## Mental map</p>
<p><code>text
         Reinforcement Learning from Human Feedback (RLHF)
   Goal: Align model behavior with human preferences and values
          when explicit reward design is impractical
                                │
                                ▼
           Why Dense Rewards Are Hard for Language Models
 ┌───────────────────────────────────────────────────────────┐
 │ Open-ended tasks (dialogue, summarization, reasoning)     │
 │ No clear numeric notion of “good” behavior                │
 │ Hand-crafted dense rewards → miss nuance, reward hacking  │
 │ Metrics (BLEU, ROUGE, length) poorly reflect human values │
 │ Human values are subjective, contextual, and fuzzy        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
            From Imitation Learning to Human Preferences
 ┌───────────────────────────────────────────────────────────┐
 │ Behavioral Cloning (IL): imitate demonstrations           │
 │ + Simple, safe, no reward needed                          │
 │ – Cannot exceed expert, sensitive to distribution shift   │
 │ DAgger: fixes BC but requires step-by-step human labeling │
 │ Pairwise preferences = middle ground                      │
 │ → no dense rewards, no per-step supervision               │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Pairwise Preference Feedback (Key Idea)
 ┌───────────────────────────────────────────────────────────┐
 │ Humans compare two outputs and choose the better one      │
 │ Easier than assigning numeric rewards                     │
 │ More informative than raw demonstrations                  │
 │ Scales to complex, open-ended behaviors                   │
 │ Forms basis of reward learning in RLHF                    │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
        Bradley–Terry Model: Preferences → Reward Signal
 ┌───────────────────────────────────────────────────────────┐
 │ Model noisy human comparisons probabilistically           │
 │ P(b_i ≻ b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    │
 │ r(b): latent scalar reward                                │
 │ Fit r(·) by maximizing likelihood / cross-entropy         │
 │ Preferred outputs get higher reward scores                │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
             RLHF Training Pipeline (3 Stages)
 ┌───────────────────────────────────────────────────────────┐
 │ 1. Supervised Fine-Tuning (SFT)                           │
 │    – Behavioral cloning on human-written demos            │
 │    – Produces reference policy π_ref                      │
 │                                                           │
 │ 2. Reward Model Training                                  │
 │    – Human pairwise preferences                           │
 │    – Train r_φ(x,y) via Bradley–Terry loss                │
 │                                                           │
 │ 3. RL Fine-Tuning (PPO)                                   │
 │    – Maximize reward r_φ(x,y)                             │
 │    – KL penalty keeps π_θ close to π_ref                  │
 │    – Prevents reward hacking &amp; language drift             │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              RLHF Objective (KL-Regularized RL)
 ┌───────────────────────────────────────────────────────────┐
 │ Maximize:                                                 │
 │   E[r_φ(x,y)] − β · KL(π_θ || π_ref)                      │
 │ β controls tradeoff:                                      │
 │   Low β → reward hacking / mode collapse                  │
 │   High β → little improvement over SFT                    │
 │ PPO provides stable policy updates                        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
          Limitations of Standard RLHF (PPO-based)
 ┌───────────────────────────────────────────────────────────┐
 │ Requires training multiple models                         │
 │ Many hyperparameters (β, PPO clip, value loss, etc.)      │
 │ Sampling-based RL can be unstable                         │
 │ Expensive and complex pipeline                            │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
      Direct Preference Optimization (DPO): RLHF without RL
 ┌───────────────────────────────────────────────────────────┐
 │ Solve RLHF objective in closed form                       │
 │ Optimal policy:                                           │
 │   π*(y|x) ∝ π_ref(y|x) · exp(r_φ(x,y)/β)                  |
 │ Use probability ratios → normalization cancels            │
 │ Train π_θ directly on preference pairs                    │
 │ Loss: sigmoid on log-prob difference                      │
 │ “Your LM is secretly a reward model”                      │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              DPO vs PPO-based RLHF
 ┌─────────────────────────────┬─────────────────────────────┐
 │ RLHF (PPO)                  │ DPO                         │
 │ + Explicit RL optimization  │ + Pure supervised learning  │
 │ – Complex &amp; unstable        │ – Assumes KL-optimal form   │
 │ – Many hyperparameters      │ + Simple, stable, efficient │
 │                             │ + No separate reward model  │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
              Final Takeaway (Chapter Summary)
 ┌───────────────────────────────────────────────────────────┐
 │ Dense rewards are hard for language tasks                 │
 │ Pairwise preferences provide natural human feedback       │
 │ RLHF learns rewards from preferences + optimizes policy   │
 │ DPO simplifies RLHF by removing explicit RL               │
 │ Together, they extend imitation learning toward           │
 │ scalable value alignment for modern language models       │
 └───────────────────────────────────────────────────────────┘</code></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../8_imitation_learning/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 8. Imitation Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                8. Imitation Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../10_offline_rl/" class="md-footer__link md-footer__link--next" aria-label="Next: 10. Offline Reinforcement Learning">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                10. Offline Reinforcement Learning
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>