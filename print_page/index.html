
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Print/PDF - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("/reinforcement_learning/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#index" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print/PDF
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../reinforcement/1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs & Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reinforcement/14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reinforcement-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction to Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-2_mdp" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. MDPs & Dynamic Programming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-3_modelfree" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Model-Free Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-4_model_free_control" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Model-Free Control
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-5_policy_gradient" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Policy Gradient Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-6_pg2" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Policy Gradient Variance Reduction and Actor-Critic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-7_gae" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. Advances in Policy Optimization – GAE, TRPO, and PPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-8_imitation_learning" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Imitation Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-9_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. RLHF
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-10_offline_rl" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Offline Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-11_fast_rl" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Data-Efficient Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-12_fast_mdps" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Fast Reinforcement Learning in MDPs and Generalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-13_montecarlo" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Monte Carlo Tree Search and Planning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-14_final" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Summary and Overview
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Reinforcement Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reinforcement-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. Introduction to Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-2_mdp" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. MDPs & Dynamic Programming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-3_modelfree" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Model-Free Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-4_model_free_control" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Model-Free Control
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-5_policy_gradient" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Policy Gradient Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-6_pg2" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Policy Gradient Variance Reduction and Actor-Critic
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-7_gae" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. Advances in Policy Optimization – GAE, TRPO, and PPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-8_imitation_learning" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Imitation Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-9_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. RLHF
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-10_offline_rl" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Offline Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-11_fast_rl" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Data-Efficient Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-12_fast_mdps" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Fast Reinforcement Learning in MDPs and Generalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-13_montecarlo" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Monte Carlo Tree Search and Planning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-14_final" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Summary and Overview
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index" heading-number="1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="lectures-on-reinforcement-learning">Lectures on Reinforcement Learning<a class="headerlink" href="#index-lectures-on-reinforcement-learning" title="Permanent link">¶</a></h1>
<p>Welcome to <em>Lectures on Reinforcement Learning (RL)</em>, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and develop modern reinforcement learning methods.</p>
<p>These notes are inspired by and draw heavily on material from:
- Stanford CS234: Reinforcement Learning (Spring 2024)
- Stanford CS224R: Deep Reinforcement Learning (Spring 2025) </p>
<p>The goal is not to reproduce these courses, but to synthesize their core ideas into a coherent, optimization- and mathematics-first perspective suitable for practitioners and researchers.</p></body></html></section>
                    <section class='print-page md-section' id='section-2' heading-number='2'>
                        <h1>Reinforcement Learning<a class='headerlink' href='#section-2' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="reinforcement-1_intro" heading-number="2.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-introduction-to-reinforcement-learning">Chapter 1: Introduction to Reinforcement Learning<a class="headerlink" href="#reinforcement-1_intro-chapter-1-introduction-to-reinforcement-learning" title="Permanent link">¶</a></h1>
<p>Reinforcement Learning is a paradigm in machine learning where an agent learns to make sequential decisions through interaction with an environment. Unlike supervised learning, where the agent learns from labeled examples, or unsupervised learning, where it learns patterns from unlabeled data, reinforcement learning is driven by the goal of maximizing cumulative reward through trial and error. The agent is not told which actions to take but must discover them by exploring the consequences of its actions.</p>
<p>Sequential decision-making under uncertainty is at the heart of reinforcement learning. The agent must balance exploration and exploitation. Exploration is needed to gather information about the environment, while exploitation uses this information to select actions that appear best.</p>
<div class="highlight"><pre><span></span><code> RL vs Supervised Learning (Key Differences)
       ┌─────────────────────────────────────────────────────┐
       │ Supervised Learning:                                │
       │   – Learns from labeled examples (input → target)   │
       │   – Feedback is immediate and correct               │
       │   – IID data; no sequential dependence              │
       │                                                     │
       │ Reinforcement Learning:                             │
       │   – Learns from interaction (trial &amp; error)         │
       │   – Feedback (reward) may be delayed or sparse      │
       │   – Data depends on agent's actions (non-IID)       │
       │   – Must balance exploration vs exploitation        │
       │   – Must solve temporal credit assignment           │
       └─────────────────────────────────────────────────────┘
</code></pre></div>
<p>A key characteristic of reinforcement learning is that the outcome of an action may not be immediately known. Rewards can be delayed, making it hard to determine which past actions are responsible for future outcomes. This challenge is known as temporal credit assignment. Successful reinforcement learning algorithms must learn to attribute long-term consequences to earlier decisions.</p>
<p>At each time step, the agent observes some representation of the world, takes an action, and receives a reward. The world then transitions to a new state. This interaction continues over time, forming an experience trajectory:</p>
<div class="arithmatex">\[
s_0, a_0, r_1, s_1, a_1, r_2, \dots
\]</div>
<p>The agent’s goal is to learn a policy, which is a mapping from states to actions, that maximizes the total reward it collects over time.</p>
<p>The total future reward is defined through the notion of return. The most common formulation is the discounted return:</p>
<div class="arithmatex">\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]</div>
<p>where <span class="arithmatex">\(0 \le \gamma \le 1\)</span> is called the discount factor. It determines how much the agent values immediate rewards compared to future rewards. A smaller <span class="arithmatex">\(\gamma\)</span> encourages short-term decisions, while a larger <span class="arithmatex">\(\gamma\)</span> favors long-term planning.</p>
<p>Reinforcement learning involves four fundamental challenges:</p>
<ol>
<li>Optimization: The agent must find an optimal policy that maximizes expected return.</li>
<li>Delayed consequences: Actions can affect rewards far into the future, making credit assignment difficult.</li>
<li>Exploration: The agent must try actions to learn their consequences, even though some actions may seem suboptimal in the short term.</li>
<li>Generalization: The agent must use limited experience to generalize to states it has never seen before.</li>
</ol>
<p>The main components of a reinforcement learning system are the agent, the environment, actions, states, and rewards. The agent chooses an action based on its current state. The environment responds with the next state and a reward. From this interaction, the agent must infer how to improve its decisions over time.</p>
<p>The concept of state is crucial. A state is a summary of information that can influence future outcomes. In theory, a state is Markov if it satisfies:</p>
<div class="arithmatex">\[
p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t)
\]</div>
<p>where <span class="arithmatex">\(h_t\)</span> is the full history of past observations, actions, and rewards. This means that the future depends only on the current state, not on the entire past. The Markov property is important because it simplifies the learning problem and allows powerful mathematical tools to be applied.</p>
<blockquote>
<p>State (environment)  →  Action (agent)  →  Next State (environment)</p>
</blockquote>
<p>Reinforcement learning is particularly useful in domains where optimal behavior is not easily specified, data is limited or must be collected through interaction, and long-term consequences matter. Examples include robotics, autonomous vehicles, game playing, resource allocation, recommendation systems, and online decision-making.</p>
<h2 id="reinforcement-1_intro-key-concepts">Key Concepts:<a class="headerlink" href="#reinforcement-1_intro-key-concepts" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-1_intro-episodic-vs-continuing">Episodic vs Continuing:<a class="headerlink" href="#reinforcement-1_intro-episodic-vs-continuing" title="Permanent link">¶</a></h3>
<p>Reinforcement learning problems can be episodic or continuing. In episodic tasks, interactions end after a finite number of steps, and the agent resets for a new episode. In continuing tasks, the interactions never formally end, and the agent must learn to behave well indefinitely. In episodic settings, the return is naturally finite. In continuing tasks, discounting or average reward formulations are used to ensure the return is well-defined.</p>
<h3 id="reinforcement-1_intro-types-of-rl-tasks">Types of RL tasks:<a class="headerlink" href="#reinforcement-1_intro-types-of-rl-tasks" title="Permanent link">¶</a></h3>
<p>There are several types of learning tasks in RL:</p>
<ol>
<li>Prediction/Policy Evaluation: Estimating how good a given policy is.</li>
<li>Control: Finding an optimal policy that maximizes expected return.</li>
<li>Planning: Computing optimal policies using a known model of the environment.</li>
</ol>
<h3 id="reinforcement-1_intro-model-based-vs-model-free">Model based vs Model-free<a class="headerlink" href="#reinforcement-1_intro-model-based-vs-model-free" title="Permanent link">¶</a></h3>
<p>Reinforcement learning algorithms can be classified into two major categories: model-based and model-free. Model-based methods assume that the transition dynamics and reward function of the environment are known or learned. They use this information for planning. Model-free methods do not assume access to this knowledge and must learn directly from interaction.</p>
<h3 id="reinforcement-1_intro-on-policy-vs-off-policy">On-policy vs off-policy<a class="headerlink" href="#reinforcement-1_intro-on-policy-vs-off-policy" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>On-policy learning:</p>
<ul>
<li>Direct experience.</li>
<li>Learn to estimate and evaluate a policy from experience obtained from following that policy.</li>
</ul>
</li>
<li>
<p>Off-policy Learning</p>
<ul>
<li>Learn to estimate and evaluate a policy using experience gathered from following a different policy.</li>
</ul>
</li>
</ul>
<h3 id="reinforcement-1_intro-tabular-vs-function-approximation">Tabular vs Function Approximation<a class="headerlink" href="#reinforcement-1_intro-tabular-vs-function-approximation" title="Permanent link">¶</a></h3>
<p>In small environments with a limited number of states and actions, value functions and policies can be represented using tables. This is known as the tabular setting. However, real-world problems often involve very large or continuous state spaces, where it is impossible to maintain a separate entry for every state or action.</p>
<p>In such cases, we approximate the value function or policy using a parameterized function, such as a linear model or neural network. This approach is called function approximation. Function approximation enables generalization: knowledge gained from one state can be applied to many similar states, making learning feasible in large or continuous environments.</p></body></html></section><section class="print-page" id="reinforcement-2_mdp" heading-number="2.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-markov-decision-processes-and-dynamic-programming">Chapter 2: Markov Decision Processes and Dynamic Programming<a class="headerlink" href="#reinforcement-2_mdp-chapter-2-markov-decision-processes-and-dynamic-programming" title="Permanent link">¶</a></h1>
<p>Reinforcement Learning relies on the mathematical framework of Markov Decision Processes (MDPs) to formalize sequential decision-making under uncertainty. The key idea is that an agent interacts with an environment, making decisions that influence both immediate and future rewards.</p>
<blockquote>
<p>Reinforcement Learning is about selecting actions over time to maximize long-term reward.</p>
</blockquote>
<h2 id="reinforcement-2_mdp-the-markovian-hierarchy">The Markovian Hierarchy<a class="headerlink" href="#reinforcement-2_mdp-the-markovian-hierarchy" title="Permanent link">¶</a></h2>
<p>The RL framework is built upon three foundational models, each adding complexity and agency.</p>
<h3 id="reinforcement-2_mdp-the-markov-process">The Markov Process<a class="headerlink" href="#reinforcement-2_mdp-the-markov-process" title="Permanent link">¶</a></h3>
<p>A Markov Process, or Markov Chain, is the simplest model, concerned only with the flow of states. It is defined by the set of States (<span class="arithmatex">\(S\)</span>) and the Transition Model (<span class="arithmatex">\(P(s' \mid s)\)</span>). The defining characteristic is the Markov Property: the next state is independent of the past states, given only the current state.</p>
<div class="arithmatex">\[
P(s_{t+1} \mid s_t, s_{t-1}, \ldots) = P(s_{t+1} \mid s_t)
\]</div>
<blockquote>
<p>The future is conditionally independent of the past given the present. <em>Intuition: MPs describe what happens but do not assign any value to these events.</em></p>
</blockquote>
<h3 id="reinforcement-2_mdp-the-markov-reward-process-mrp">The Markov Reward Process (MRP)<a class="headerlink" href="#reinforcement-2_mdp-the-markov-reward-process-mrp" title="Permanent link">¶</a></h3>
<p>A Markov Reward Process (MRP) extends an MP by adding rewards and discounting. An MRP is a tuple <span class="arithmatex">\((S, P, R, \gamma)\)</span> where <span class="arithmatex">\(R(s)\)</span> is the expected reward for being in state <span class="arithmatex">\(s\)</span> and <span class="arithmatex">\(\gamma\)</span> is the discount factor. The return is:</p>
<div class="arithmatex">\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]</div>
<p>The goal is to compute the value function, which is the expected return starting from a state <span class="arithmatex">\(s\)</span>:</p>
<div class="arithmatex">\[
V(s) = \mathbb{E}[G_t | s_t = s]
\]</div>
<p>The value function satisfies the Bellman Expectation Equation:</p>
<div class="arithmatex">\[
V(s) = R(s) + \gamma \sum_{s'} P(s'|s)V(s')
\]</div>
<p>This recursive structure relates the value of a state to the values of its successor states.</p>
<h3 id="reinforcement-2_mdp-the-markov-decision-process-mdp">The Markov Decision Process (MDP)<a class="headerlink" href="#reinforcement-2_mdp-the-markov-decision-process-mdp" title="Permanent link">¶</a></h3>
<p>An MDP introduces agency. Defined by the tuple <span class="arithmatex">\((S, A, P, R, \gamma)\)</span>, it extends the MRP by giving the agent a set of Actions (<span class="arithmatex">\(A\)</span>) to choose from.</p>
<ul>
<li>Action-Dependent Transition: <span class="arithmatex">\(P(s' \mid s, a)\)</span></li>
<li>Action-Dependent Reward: <span class="arithmatex">\(R(s, a)\)</span></li>
</ul>
<p>The agent's strategy is described by a Policy (<span class="arithmatex">\(\pi(a \mid s)\)</span>), the probability of selecting action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. A key insight is that fixing any policy <span class="arithmatex">\(\pi\)</span> reduces an MDP back into an MRP, allowing all tools developed for MRPs to be applied to the MDP.</p>
<div class="arithmatex">\[
R_\pi(s) = \sum_a \pi(a|s) R(s,a)
\]</div>
<div class="arithmatex">\[
P_\pi(s'|s) = \sum_a \pi(a|s) P(s'|s,a)
\]</div>
<blockquote>
<p>Once actions are introduced in an MDP, it becomes useful to evaluate not only how good a state is, but how good a particular action is <em>relative to the policy’s expected behavior</em>. This leads to the advantage function.</p>
<p>The state-value function measures how good it is to be in a state: <span class="arithmatex">\(V_\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s]\)</span>.</p>
<p>The action-value function measures how good it is to take action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>:<span class="arithmatex">\(Q_\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s,\; a_t = a]\)</span></p>
<p>The <strong>advantage function</strong> compares these two: <span class="arithmatex">\(A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s).\)</span></p>
<p><span class="arithmatex">\(V_\pi(s)\)</span> is how well the policy performs <em>on average</em> from state <span class="arithmatex">\(s\)</span>.</p>
<p><span class="arithmatex">\(Q_\pi(s,a)\)</span> is how well it performs if it specifically takes action <span class="arithmatex">\(a\)</span>.</p>
<p>Therefore, the advantage tells us: How much better or worse action <span class="arithmatex">\(a\)</span> is compared to what the policy would normally do in state <span class="arithmatex">\(s\)</span>.</p>
</blockquote>
<h2 id="reinforcement-2_mdp-value-functions-and-expectation">Value Functions and Expectation<a class="headerlink" href="#reinforcement-2_mdp-value-functions-and-expectation" title="Permanent link">¶</a></h2>
<p>To evaluate a fixed policy <span class="arithmatex">\(\pi\)</span>, we define two inter-related value functions based on the Bellman Expectation Equations.</p>
<h3 id="reinforcement-2_mdp-state-value-function-vpis">State Value Function (<span class="arithmatex">\(V^\pi(s)\)</span>)<a class="headerlink" href="#reinforcement-2_mdp-state-value-function-vpis" title="Permanent link">¶</a></h3>
<p><span class="arithmatex">\(V^\pi(s)\)</span> quantifies the long-term expected return starting from state <span class="arithmatex">\(s\)</span> and strictly following policy <span class="arithmatex">\(\pi\)</span>.
<script type="math/tex; mode=display">
V^\pi(s) = \mathbb{E}[G_t \mid s_t = s, \pi]
</script>
</p>
<blockquote>
<p>How much total reward should I expect if I start in state s and follow policy <span class="arithmatex">\(\pi\)</span>: forever?</p>
</blockquote>
<h3 id="reinforcement-2_mdp-state-action-value-function-qpisa">State-Action Value Function (<span class="arithmatex">\(Q^\pi(s,a)\)</span>)<a class="headerlink" href="#reinforcement-2_mdp-state-action-value-function-qpisa" title="Permanent link">¶</a></h3>
<p><span class="arithmatex">\(Q^\pi(s,a)\)</span> is a more granular measure, quantifying the expected return if the agent takes action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> first, and <em>then</em> follows policy <span class="arithmatex">\(\pi\)</span>.
<script type="math/tex; mode=display">
Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')
</script>
</p>
<blockquote>
<p><em>Intuition:</em> The <span class="arithmatex">\(Q\)</span>-function is the value of doing a specific action; the <span class="arithmatex">\(V\)</span>-function is the value of being in a state (the weighted average of the <span class="arithmatex">\(Q\)</span>-values offered by the policy <span class="arithmatex">\(\pi\)</span> in that state):
<script type="math/tex; mode=display">
V^\pi(s) = \sum_a \pi(a \mid s) Q^\pi(s,a)
</script>
</p>
</blockquote>
<p>The Bellman Expectation Equation for <span class="arithmatex">\(V^\pi\)</span> links the value of a state to the values of the actions chosen by <span class="arithmatex">\(\pi\)</span> and the resulting future states:
<script type="math/tex; mode=display">
V^\pi(s) = \sum_a \pi(a \mid s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]
</script>
</p>
<h2 id="reinforcement-2_mdp-optimal-control-finding-pi">Optimal Control: Finding <span class="arithmatex">\(\pi^*\)</span><a class="headerlink" href="#reinforcement-2_mdp-optimal-control-finding-pi" title="Permanent link">¶</a></h2>
<p>The ultimate goal of solving an MDP is to find the optimal policy (<span class="arithmatex">\(\pi^*\)</span>) that maximizes the expected return from every state <span class="arithmatex">\(s\)</span>.</p>
<div class="arithmatex">\[
\pi^* = \operatorname*{arg\,max}_{\pi} V^\pi(s) \quad \text{for all } s \in S
\]</div>
<p>This optimal policy is characterized by the Optimal Value Functions (<span class="arithmatex">\(V^*\)</span> and <span class="arithmatex">\(Q^*\)</span>).</p>
<h3 id="reinforcement-2_mdp-the-bellman-optimality-equations">The Bellman Optimality Equations<a class="headerlink" href="#reinforcement-2_mdp-the-bellman-optimality-equations" title="Permanent link">¶</a></h3>
<p>These equations are fundamental, describing the unique value functions that arise when acting optimally. Unlike the expectation equations, they contain a <span class="arithmatex">\(\max\)</span> operator, making them non-linear.</p>
<ul>
<li>
<p>Optimal State Value (<span class="arithmatex">\(V^*\)</span>): The optimal value of a state equals the maximum expected return achievable from any single action <span class="arithmatex">\(a\)</span> taken from that state:</p>
<div class="arithmatex">\[
V^*(s) = \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]
\]</div>
</li>
<li>
<p>Optimal Action-Value (<span class="arithmatex">\(Q^*\)</span>): The optimal value of taking action <span class="arithmatex">\(a\)</span> is the immediate reward plus the discounted value of the optimal subsequent actions (<span class="arithmatex">\(\max_{a'}\)</span>) in the next state <span class="arithmatex">\(s'\)</span>:</p>
<div class="arithmatex">\[
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s', a')
\]</div>
</li>
</ul>
<p>Once <span class="arithmatex">\(Q^*\)</span> is known, the optimal policy <span class="arithmatex">\(\pi^*\)</span> is easily extracted by simply choosing the action that maximizes <span class="arithmatex">\(Q^*(s,a)\)</span> in every state:
<script type="math/tex; mode=display">
\pi^*(s) = \operatorname*{arg\,max}_{a} Q^*(s,a)
</script>
</p>
<p>These equations are non-linear due to the max operator and must be solved iteratively.</p>
<h2 id="reinforcement-2_mdp-dynamic-programming-algorithms">Dynamic Programming Algorithms<a class="headerlink" href="#reinforcement-2_mdp-dynamic-programming-algorithms" title="Permanent link">¶</a></h2>
<p>For MDPs where the model (<span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(R\)</span>) is fully known, Dynamic Programming methods are used to solve the Bellman Optimality Equations iteratively.</p>
<h3 id="reinforcement-2_mdp-policy-iteration">Policy Iteration<a class="headerlink" href="#reinforcement-2_mdp-policy-iteration" title="Permanent link">¶</a></h3>
<p>Policy Iteration follows an alternating cycle of Evaluation and Improvement. It takes fewer, but more expensive, iterations to converge.</p>
<ol>
<li>Policy Evaluation: For the current policy <span class="arithmatex">\(\pi_k\)</span>, compute <span class="arithmatex">\(V^{\pi_k}\)</span> by iteratively applying the Bellman Expectation Equation until full convergence. This is the computationally intensive step.
    <script type="math/tex; mode=display">
    V^{\pi_k}(s) \leftarrow \text{solve } V^{\pi_k} = R_{\pi_k} + \gamma P_{\pi_k} V^{\pi_k}
    </script>
</li>
<li>Policy Improvement: Update the policy <span class="arithmatex">\(\pi_{k+1}\)</span> by choosing an action that is greedy with respect to the fully converged <span class="arithmatex">\(V^{\pi_k}\)</span>.
    <script type="math/tex; mode=display">
    \pi_{k+1}(s) \leftarrow \operatorname*{arg\,max}_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi_k}(s') \right]
    </script>
</li>
</ol>
<p>The process repeats until the policy stabilizes (<span class="arithmatex">\(\pi_{k+1} = \pi_k\)</span>), guaranteeing convergence to <span class="arithmatex">\(\pi^*\)</span>.</p>
<h3 id="reinforcement-2_mdp-value-iteration">Value Iteration<a class="headerlink" href="#reinforcement-2_mdp-value-iteration" title="Permanent link">¶</a></h3>
<p>Value Iteration is a single, continuous process that combines evaluation and improvement by repeatedly applying the Bellman Optimality Equation. It takes many, but computationally cheap, iterations.</p>
<ol>
<li>Iterative Update: For every state <span class="arithmatex">\(s\)</span>, update the value function <span class="arithmatex">\(V_k(s)\)</span> using the <span class="arithmatex">\(\max\)</span> operation. This immediately incorporates a greedy improvement step into the value update.
    <script type="math/tex; mode=display">
    V_{k+1}(s) \leftarrow \max_{a} \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]
    </script>
</li>
<li>Convergence: The iterations stop when <span class="arithmatex">\(V_{k+1}\)</span> is sufficiently close to <span class="arithmatex">\(V^*\)</span>.</li>
<li>Extraction: The optimal policy <span class="arithmatex">\(\pi^*\)</span> is then extracted greedily from the final <span class="arithmatex">\(V^*\)</span>.</li>
</ol>
<h3 id="reinforcement-2_mdp-pi-vs-vi">PI vs VI<a class="headerlink" href="#reinforcement-2_mdp-pi-vs-vi" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Policy Iteration (PI)</th>
<th style="text-align: left;">Value Iteration (VI)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Core Idea</td>
<td style="text-align: left;">Evaluate completely, then improve.</td>
<td style="text-align: left;">Greedily improve values in every step.</td>
</tr>
<tr>
<td style="text-align: left;">Equation</td>
<td style="text-align: left;">Uses Bellman Expectation (inner loop)</td>
<td style="text-align: left;">Uses Bellman Optimality (max)</td>
</tr>
<tr>
<td style="text-align: left;">Convergence</td>
<td style="text-align: left;">Few, large policy steps. Policy guaranteed to stabilize faster.</td>
<td style="text-align: left;">Many, small value steps. Value function converges slowly to <span class="arithmatex">\(V^*\)</span>.</td>
</tr>
<tr>
<td style="text-align: left;">Cost</td>
<td style="text-align: left;">High cost per iteration (due to full evaluation).</td>
<td style="text-align: left;">Low cost per iteration (due to one-step backup).</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-2_mdp-mdps-mental-map">MDPs Mental Map<a class="headerlink" href="#reinforcement-2_mdp-mdps-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                   Markov Decision Processes (MDPs)
        Formalizing Sequential Decision-Making under Uncertainty
                                  │
                                  ▼
                       Progression of Markov Models
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Process (MP): States &amp; Transition Probabilities │
       │   [S, P(s'|s)] — No rewards, no decisions               │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Reward Process (MRP): MP + Rewards + γ          │
       │  [S, P(s'|s), R(s), γ]                                  │
       │    Value Function: V(s) = E[Gt | st = s]                │
       │     Bellman Expectation Eqn:                            │
       │     V(s) = R(s) + γ ∑ P(s'|s)V(s')                      │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
       ┌─────────────────────────────────────────────────────────┐
       │  Markov Decision Process (MDP): MRP + Actions           │
       │   [S, A, P(s'|s,a), R(s,a), γ]                          │
       │    Adds Agency: Agent chooses actions                   │
       └─────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                             Policy π(a|s)
                        Agent’s decision strategy
                                  │
                                  ▼
                          Value Functions
     ┌────────────────────────────────────────────────────────────────┐
     │ State Value Vπ(s): Expected return following π                 │
     │ Qπ(s,a): Expected return from (s,a) then follow π              │
     │ Relationship: Vπ(s) = ∑ π(a|s) Qπ(s,a)                         │
     └────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                    Bellman Expectation Equations
     ┌────────────────────────────────────────────────────────────────┐
     │ Vπ(s) = ∑ π(a|s)[R(s,a) + γ ∑ P(s'|s,a)Vπ(s')]                 │
     │ Qπ(s,a) = R(s,a) + γ ∑ P(s'|s,a) Vπ(s')                        │
     └────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
               Goal: Find Optimal Policy π*
     ┌─────────────────────────────────────────────────────────────┐
     │ π*(s) = argmaxₐ Q*(s,a)                                     │
     │ V*(s): Max possible value from state s under the optimal    |
     |        policy                                               │
     │ Q*(s,a): Max possible return state s by taking action a     |
     |          and thereafter following the optimal policy        │
     └─────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                      Bellman Optimality Equations
     ┌─────────────────────────────────────────────────────────────┐
     │ V*(s) = maxₐ [R(s,a) + γ ∑ P(s'|s,a)V*(s')]                 │
     │ Q*(s,a) = R(s,a) + γ ∑ P(s'|s,a) maxₐ' Q*(s',a')            │
     └─────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                 Solution when Model (P,R) is known:
                    Dynamic Programming (DP)
     ┌───────────────────────────────────────────────┬───────────────────┐
     │ Policy Iteration                              │ Value Iteration - │
     │ (Alternating Evaluation &amp; Improvement)        │ Single update step│
     │                                               │ repeatedly        │
     └───────────────────────────────────────────────┴───────────────────┘
          │                                               │
          ▼                                               ▼
  ┌─────────────────┐                          ┌─────────────────────────┐
  │ Policy Eval     │                          │ Bellman Optimality      │
  │ Using Vπ until  │                          │ Update every iteration  │
  │ convergence     │                          │ V_(k+1) = max_a[....]   │
  └─────────────────┘                          └─────────────────────────┘
          │                                               │
          ▼                                               ▼
  ┌─────────────────┐                          ┌─────────────────────────┐
  │ Policy          │                          │ After convergence:      │
  │ Improvement:    │                          │ extract π* from Q*      │
  │ π_(k+1)=argmax Q│                          └─────────────────────────┘
  └─────────────────┘
                                  │
                                  ▼
             Outcome: Optimal Policy and Value Functions
       ┌─────────────────────────────────────────────────────┐
       │ π*(s) — Best action at each state                   │
       │ V*(s) — Max return achievable                       │
       │ Q*(s,a) — Max return from (s,a)                     │
       └─────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-3_modelfree" heading-number="2.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy">Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy<a class="headerlink" href="#reinforcement-3_modelfree-chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy" title="Permanent link">¶</a></h1>
<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function <span class="arithmatex">\(V^\pi(s)\)</span> or <span class="arithmatex">\(Q^\pi(s,a)\)</span> for a given policy <span class="arithmatex">\(\pi\)</span> using data of the form:</p>
<div class="arithmatex">\[
s_0, a_0, r_1, s_1, a_1, r_2, s_2, \dots
\]</div>
<p>The true value of a state under policy <span class="arithmatex">\(\pi\)</span> is still defined by the expected return:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]
\]</div>
<p>but the agent must approximate this expectation using sampled experience.</p>
<p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p>
<ol>
<li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li>
<li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li>
</ol>
<h2 id="reinforcement-3_modelfree-monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation<a class="headerlink" href="#reinforcement-3_modelfree-monte-carlo-policy-evaluation" title="Permanent link">¶</a></h2>
<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function <span class="arithmatex">\(V^\pi(s)\)</span> is the expected return, we can approximate it by simply averaging the observed returns (<span class="arithmatex">\(G_t\)</span>) from many episodes that start at state <span class="arithmatex">\(s\)</span>.</p>
<div class="arithmatex">\[
V^\pi(s) \approx \text{Average of observed returns } G_t \text{ starting from } s
\]</div>
<h3 id="reinforcement-3_modelfree-key-properties-of-mc">Key Properties of MC<a class="headerlink" href="#reinforcement-3_modelfree-key-properties-of-mc" title="Permanent link">¶</a></h3>
<ol>
<li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (<span class="arithmatex">\(s_T\)</span>) to calculate the full return <span class="arithmatex">\(G_t\)</span>.</li>
<li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li>
</ol>
<p>We can maintain the value estimates <span class="arithmatex">\(V(s)\)</span> using counts and sums, or through incremental updates.</p>
<h4 id="reinforcement-3_modelfree-a-first-visit-vs-every-visit-mc">A. First-Visit vs. Every-Visit MC<a class="headerlink" href="#reinforcement-3_modelfree-a-first-visit-vs-every-visit-mc" title="Permanent link">¶</a></h4>
<p>When computing the return <span class="arithmatex">\(G_t\)</span> for a state <span class="arithmatex">\(s\)</span> in a single trajectory, a state might be visited multiple times.</p>
<ul>
<li>First-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> only the first time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: First-Visit MC is an unbiased estimator of <span class="arithmatex">\(V^\pi(s)\)</span>. It is also consistent (converges to the true value as data <span class="arithmatex">\(\rightarrow \infty\)</span>) by the Law of Large Numbers.</li>
</ul>
</li>
<li>Every-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> every time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li>
</ul>
</li>
</ul>
<h4 id="reinforcement-3_modelfree-b-incremental-monte-carlo">B. Incremental Monte Carlo<a class="headerlink" href="#reinforcement-3_modelfree-b-incremental-monte-carlo" title="Permanent link">¶</a></h4>
<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p>
<div class="arithmatex">\[
V(s) \leftarrow V(s) + \alpha \left[ G_t - V(s) \right]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(G_t\)</span>: The actual observed return (our target).</li>
<li><span class="arithmatex">\(V(s)\)</span>: Our current estimate (our old value).</li>
<li><span class="arithmatex">\(\alpha\)</span>: The learning rate (<span class="arithmatex">\(\alpha \in (0, 1]\)</span>), which can be fixed or decayed.</li>
</ul>
<p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (<span class="arithmatex">\(V^\pi\)</span>), the learning rate <span class="arithmatex">\(\alpha_t\)</span> (which may be <span class="arithmatex">\(1/N(s)\)</span> or a fixed constant) must satisfy the following conditions:</p>
<ol>
<li>The sum of all learning rates for state <span class="arithmatex">\(s\)</span> must diverge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s) = \infty\)</span></li>
<li>The sum of the squared learning rates must converge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s)^2 &lt; \infty\)</span></li>
</ol>
<h2 id="reinforcement-3_modelfree-temporal-difference-td-learning">Temporal Difference (TD) Learning<a class="headerlink" href="#reinforcement-3_modelfree-temporal-difference-td-learning" title="Permanent link">¶</a></h2>
<p>While MC uses the full return <span class="arithmatex">\(G_t\)</span>, TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>
<h3 id="reinforcement-3_modelfree-bootstrapping-the-core-idea">Bootstrapping: The Core Idea<a class="headerlink" href="#reinforcement-3_modelfree-bootstrapping-the-core-idea" title="Permanent link">¶</a></h3>
<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the <em>next</em> state, <span class="arithmatex">\(V(s_{t+1})\)</span>, to update the value of the <em>current</em> state, <span class="arithmatex">\(V(s_t)\)</span>. The standard TD algorithm is TD(0) (or one-step TD).</p>
<h3 id="reinforcement-3_modelfree-the-td0-update-rule">The TD(0) Update Rule<a class="headerlink" href="#reinforcement-3_modelfree-the-td0-update-rule" title="Permanent link">¶</a></h3>
<p>The TD(0) update replaces the full return <span class="arithmatex">\(G_t\)</span> with the TD Target (<span class="arithmatex">\(r_t + \gamma V(s_{t+1})\)</span>):</p>
<div class="arithmatex">\[
V(s_t) \leftarrow V(s_t) + \alpha \left[ \underbrace{r_{t+1} + \gamma V(s_{t+1})}_{\text{TD Target}} - V(s_t) \right]
\]</div>
<p>The term inside the brackets is the TD Error (<span class="arithmatex">\(\delta_t\)</span>):
<script type="math/tex; mode=display">
\delta_t = (r_{t+1} + \gamma V(s_{t+1})) - V(s_t)
</script>
This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>
<h3 id="reinforcement-3_modelfree-td-vs-monte-carlo">TD vs. Monte Carlo<a class="headerlink" href="#reinforcement-3_modelfree-td-vs-monte-carlo" title="Permanent link">¶</a></h3>
<p>The distinction between TD and MC centers on what is used as the target value:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Monte Carlo (MC)</th>
<th style="text-align: left;">Temporal Difference (TD)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: left;"><span class="arithmatex">\(G_t\)</span> (Full observed return to episode end)</td>
<td style="text-align: left;"><span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span> (One-step return + estimated future value)</td>
</tr>
<tr>
<td style="text-align: left;">Bootstrapping</td>
<td style="text-align: left;">No (waits until episode end)</td>
<td style="text-align: left;">Yes (uses <span class="arithmatex">\(V(s_{t+1})\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;">Bias</td>
<td style="text-align: left;">Unbiased (First-Visit MC)</td>
<td style="text-align: left;">Biased (because <span class="arithmatex">\(V(s_{t+1})\)</span> is an estimate)</td>
</tr>
<tr>
<td style="text-align: left;">Variance</td>
<td style="text-align: left;">High Variance (Return <span class="arithmatex">\(G_t\)</span> is a sum of many random steps)</td>
<td style="text-align: left;">Low Variance (TD target depends on only one random reward/next state)</td>
</tr>
<tr>
<td style="text-align: left;">Convergence</td>
<td style="text-align: left;">Consistent (converges to true <span class="arithmatex">\(V^\pi\)</span>)</td>
<td style="text-align: left;">TD(0) converges to true <span class="arithmatex">\(V^\pi\)</span> in the tabular case</td>
</tr>
</tbody>
</table>
<p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>
<blockquote>
<h2 id="reinforcement-3_modelfree-example-setup">Example Setup<a class="headerlink" href="#reinforcement-3_modelfree-example-setup" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-3_modelfree-parameters">Parameters<a class="headerlink" href="#reinforcement-3_modelfree-parameters" title="Permanent link">¶</a></h3>
<ul>
<li>States (<span class="arithmatex">\(S\)</span>): <span class="arithmatex">\(s_A, s_B, s_C\)</span></li>
<li>Discount Factor (<span class="arithmatex">\(\gamma\)</span>): <span class="arithmatex">\(0.9\)</span></li>
<li>Learning Rate (<span class="arithmatex">\(\alpha\)</span>): <span class="arithmatex">\(0.5\)</span> (Used for TD updates)</li>
<li>Initial Value Estimates (<span class="arithmatex">\(V_0\)</span>): <span class="arithmatex">\(V(s_A)=0, V(s_B)=0, V(s_C)=0\)</span></li>
</ul>
<h3 id="reinforcement-3_modelfree-episodes-and-returns">Episodes and Returns<a class="headerlink" href="#reinforcement-3_modelfree-episodes-and-returns" title="Permanent link">¶</a></h3>
<p>The full return (<span class="arithmatex">\(G_t\)</span>) is calculated for every visit in every episode:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Episode (E)</th>
<th style="text-align: left;">Trajectory (State <span class="arithmatex">\(\xrightarrow{r}\)</span> Next State)</th>
<th style="text-align: left;">Visit Time (<span class="arithmatex">\(t\)</span>)</th>
<th style="text-align: left;">State (<span class="arithmatex">\(s_t\)</span>)</th>
<th style="text-align: left;">Full Return (<span class="arithmatex">\(G_t\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B \xrightarrow{r=0} s_C \xrightarrow{r=5} s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.508}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.12}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.8}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{2.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{5.2}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{8.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C \xrightarrow{r=-5} s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.31}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{-4.1}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{1.0}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-3_modelfree-1-first-visit-monte-carlo-mc">1. First-Visit Monte Carlo (MC)<a class="headerlink" href="#reinforcement-3_modelfree-1-first-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: Only the first return for a state in any given episode is used.</p>
<h3 id="reinforcement-3_modelfree-a-data-selection-and-counts-ns">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-a-data-selection-and-counts-ns" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508\)</span> (E1), <span class="arithmatex">\(5.2\)</span> (E2)</td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12\)</span> (E1), <span class="arithmatex">\(6.31\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(12.43\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8\)</span> (E1), <span class="arithmatex">\(8.0\)</span> (E2), <span class="arithmatex">\(-4.1\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{12.43}{2} = \mathbf{6.215} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
</script>
</p>
<h2 id="reinforcement-3_modelfree-2-every-visit-monte-carlo-mc">2. Every-Visit Monte Carlo (MC)<a class="headerlink" href="#reinforcement-3_modelfree-2-every-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: The return from every time a state is encountered in any episode is used.</p>
<h3 id="reinforcement-3_modelfree-a-data-selection-and-counts-ns_1">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-a-data-selection-and-counts-ns_1" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508, 5.2\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12, 2.0, 6.31, 1.0\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(15.43\)</span></td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8, 8.0, -4.1\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns_1">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-vs-sum-g_t-ns_1" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{15.43}{4} = \mathbf{3.858} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
\]</div>
<h2 id="reinforcement-3_modelfree-3-temporal-difference-td0">3. Temporal Difference (TD(0))<a class="headerlink" href="#reinforcement-3_modelfree-3-temporal-difference-td0" title="Permanent link">¶</a></h2>
<p>Rule: The value is updated after every step using the TD Target (<span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span>) and the learning rate <span class="arithmatex">\(\alpha\)</span>. The updated <span class="arithmatex">\(V(s)\)</span> estimates are carried over to the next step and episode.</p>
<h3 id="reinforcement-3_modelfree-a-step-by-step-td-calculation-summary">A. Step-by-Step TD Calculation Summary<a class="headerlink" href="#reinforcement-3_modelfree-a-step-by-step-td-calculation-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Step</th>
<th style="text-align: left;">Transition</th>
<th style="text-align: left;">Old <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;">New <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_A)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_B)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_C)\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=0} s_C\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=5} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E1-4</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C\)</span></td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C\)</span></td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=-5} s_B\)</span></td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">3.663</td>
</tr>
<tr>
<td style="text-align: left;">E3-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">3.663</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-3_modelfree-b-final-estimates-v_td0">B. Final Estimates (<span class="arithmatex">\(V_{TD(0)}\)</span>)<a class="headerlink" href="#reinforcement-3_modelfree-b-final-estimates-v_td0" title="Permanent link">¶</a></h3>
<blockquote>
<div class="arithmatex">\[
V(s_A) = \mathbf{0.375} \\
V(s_B) = \mathbf{4.431} \\
V(s_C) = \mathbf{3.663}
\]</div>
</blockquote>
<h2 id="reinforcement-3_modelfree-comparison-of-results">Comparison of Results<a class="headerlink" href="#reinforcement-3_modelfree-comparison-of-results" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">State</th>
<th style="text-align: left;">First-Visit MC</th>
<th style="text-align: left;">Every-Visit MC</th>
<th style="text-align: left;">TD(0) (<span class="arithmatex">\(\alpha=0.5, \gamma=0.9\)</span>)</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">TD heavily penalized <span class="arithmatex">\(s_A\)</span> in E2 (Target 0.25), while MC averaged the full observed high returns.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;">6.215</td>
<td style="text-align: left;">3.858</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">All methods are close for <span class="arithmatex">\(s_C\)</span>.</td>
</tr>
</tbody>
</table>
<p>This comparison illustrates the bias-variance trade-off:
* MC uses the sample return (<span class="arithmatex">\(G_t\)</span>), which has high variance but is an unbiased target (First-Visit).
* TD uses a bootstrapped estimate (<span class="arithmatex">\(r + \gamma V(s')\)</span>), which has lower variance but introduces bias by relying on an estimated successor value.</p>
</blockquote>
<h2 id="reinforcement-3_modelfree-model-free-prediction-mental-map">Model Free Prediction Mental Map<a class="headerlink" href="#reinforcement-3_modelfree-model-free-prediction-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>            Model-Free Prediction
     (Policy Evaluation without Model P or R)
                        │
                        ▼
                Goal: Estimate
       ┌───────────────────────────────────┐
       │ State Value: Vπ(s)                │
       │ Action Value: Qπ(s,a)             │
       └───────────────────────────────────┘
                        │
              Using Sampled Experience
        (s₀,a₀,r₁,s₁,a₁,r₂,... from π)
                        │
                        ▼
            Two Families of Methods
    ┌───────────────────────────────┬───────────────────────────────┐
    │ Monte Carlo (MC)              │ Temporal Difference (TD)      │
    │ "Learn from full episodes"    │ "Learn step-by-step"          │
    └───────────────────────────────┴───────────────────────────────┘
                        │                           │
                        │                           │
                        ▼                           ▼
             Monte Carlo (MC)              Temporal Difference (TD)
      ┌─────────────────────────┐       ┌────────────────────────────┐
      │ Needs full episodes     │       │Works on incomplete episodes│
      │ No bootstrapping        │       │Uses bootstrapping          │
      │ High variance           │       │Low variance                │
      │ Unbiased (first visit)  │       │Biased                      │
      └─────────────────────────┘       └────────────────────────────┘
                        │                           │
                        │                           │
        ┌───────────────┴───────────────┐           │
        │                               │           │
        ▼                               ▼           ▼
 First-Visit MC                  Every-Visit MC     TD(0) Update Rule
 (One update per episode          (Multiple updates │ V(s) ← V(s) +
  per state)                      per episode)      │ α[ r + γV(s') − V(s) ]
                        │                           │
                        │                           │
                        └──────────┬────────────────┘
                                   │
                                   ▼
                        Comparison (Bias–Variance)
               ┌─────────────────────────────────────────┐
               │ MC: Unbiased, High variance             │
               │ TD: Biased, Lower variance              │
               │ MC: Not bootstrapping                   │
               │ TD: Bootstraps using V(s’)              │
               │ MC: Episodic only                       │
               │ TD: Works for continuing tasks          │
               └─────────────────────────────────────────┘
                                   │
                                   ▼
                      Outcome: Learned Value Function
               ┌───────────────────────────────────────┐
               │ Vπ(s) or Qπ(s,a) from real experience │
               │ (No model of environment required)    │
               └───────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-4_model_free_control" heading-number="2.4"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-4-model-free-control-learning-optimal-behavior-without-a-model">Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model<a class="headerlink" href="#reinforcement-4_model_free_control-chapter-4-model-free-control-learning-optimal-behavior-without-a-model" title="Permanent link">¶</a></h1>
<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy <span class="arithmatex">\(\pi^*\)</span> without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>
<h2 id="reinforcement-4_model_free_control-from-state-values-to-action-values">From State Values to Action Values<a class="headerlink" href="#reinforcement-4_model_free_control-from-state-values-to-action-values" title="Permanent link">¶</a></h2>
<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p>
<p>
<script type="math/tex; mode=display">
\pi_{k+1}(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi_k}(s') \right]
</script>
This update requires two things:</p>
<ul>
<li>the transition probabilities <span class="arithmatex">\(P(s'|s,a)\)</span></li>
<li>the expected reward R(s,a)$</li>
</ul>
<p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p>
<p>Instead of learning the state-value function <span class="arithmatex">\(V^\pi(s)\)</span> and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.
<script type="math/tex; mode=display">
Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]
</script>
</p>
<p>The Model-Free Policy Iteration loop:</p>
<ol>
<li>Policy Evaluation: Compute <span class="arithmatex">\(Q^{\pi}\)</span> from experience.</li>
<li>Policy Improvement: Update the policy <span class="arithmatex">\(\pi\)</span> given the estimated <span class="arithmatex">\(Q^{\pi}\)</span>.</li>
</ol>
<p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>
<h2 id="reinforcement-4_model_free_control-exploration-vs-exploitation">Exploration vs. Exploitation<a class="headerlink" href="#reinforcement-4_model_free_control-exploration-vs-exploitation" title="Permanent link">¶</a></h2>
<p>To learn optimal behavior, the agent must balance two goals:</p>
<ol>
<li>Exploitation: choose actions believed to yield high rewards.</li>
<li>Exploration: try actions whose consequences are uncertain or poorly understood.</li>
</ol>
<p>A common solution is the <span class="arithmatex">\(\epsilon\)</span>-greedy policy:</p>
<p>With probability <span class="arithmatex">\(1 - \epsilon\)</span>, choose the action with the highest estimated value.<br>
With probability <span class="arithmatex">\(\epsilon\)</span>, choose a random action.</p>
<p>Formally:</p>
<div class="arithmatex">\[
\pi(a|s) =
\begin{cases}
1 - \epsilon + \frac{\epsilon}{|A|} &amp; \text{if } a = \arg\max_{a'} Q(s,a') \\
\frac{\epsilon}{|A|} &amp; \text{otherwise}
\end{cases}
\]</div>
<p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>
<h2 id="reinforcement-4_model_free_control-monte-carlo-control">Monte Carlo Control<a class="headerlink" href="#reinforcement-4_model_free_control-monte-carlo-control" title="Permanent link">¶</a></h2>
<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating <span class="arithmatex">\(V(s)\)</span>, it estimates <span class="arithmatex">\(Q(s,a)\)</span> using sampled returns.</p>
<p>Monte Carlo Policy Evaluation, Now for Q:      <br>
1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  <span class="arithmatex">\(k=1\)</span>,  Input <span class="arithmatex">\(\epsilon=1\)</span>, <span class="arithmatex">\(\pi\)</span><br>
2: loop over epiosdes     <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
5: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
6: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
9: <span class="arithmatex">\(\quad\quad\)</span>       end if<br>
10: <span class="arithmatex">\(\quad\)</span>  end for<br>
11: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span><br>
12: end loop</p>
<p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using <span class="arithmatex">\(\epsilon\)</span>-greedy policies for exploration.</p>
<ul>
<li>Policy Evaluation: <span class="arithmatex">\(Q(s, a)\)</span> is updated using the full return (<span class="arithmatex">\(G_t\)</span>) observed after the state-action pair <span class="arithmatex">\((s_t, a_t)\)</span> has occurred in an episode. The incremental update uses the formula <span class="arithmatex">\(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\)</span>. </li>
<li>Policy Improvement: The new policy <span class="arithmatex">\(\pi_{k+1}\)</span> is set to be <span class="arithmatex">\(\epsilon\)</span>-greedy with respect to the updated <span class="arithmatex">\(Q\)</span> function.</li>
</ul>
<h3 id="reinforcement-4_model_free_control-greedy-in-the-limit-of-infinite-exploration-glie">Greedy in the Limit of Infinite Exploration (GLIE)<a class="headerlink" href="#reinforcement-4_model_free_control-greedy-in-the-limit-of-infinite-exploration-glie" title="Permanent link">¶</a></h3>
<p>For Monte Carlo Control to converge to the optimal action-value function <span class="arithmatex">\(Q^*(s, a)\)</span>, the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p>
<ol>
<li>Infinite Visits: All state-action pairs <span class="arithmatex">\((s, a)\)</span> must be visited an infinite number of times (<span class="arithmatex">\(\lim_{i \rightarrow \infty} N_i(s, a) \rightarrow \infty\)</span>).</li>
<li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li>
</ol>
<p>A simple strategy to satisfy GLIE is to use an <span class="arithmatex">\(\epsilon\)</span>-greedy policy where <span class="arithmatex">\(\epsilon\)</span> is decayed over time, such as <span class="arithmatex">\(\epsilon_i = 1/i\)</span> (where <span class="arithmatex">\(i\)</span> is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function <span class="arithmatex">\(Q^*(s, a)\)</span>.</p>
<p>Monte Carlo Online Control/On Policy Improvement:    </p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  Set <span class="arithmatex">\(k=1\)</span>, <span class="arithmatex">\(\epsilon=1\)</span>.   <br>
2: <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> // Create initial <span class="arithmatex">\(\epsilon\)</span> - greedy policy.<br>
3: loop over epiosdes   <br>
4: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
5: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
6: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
7: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
9: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
10: <span class="arithmatex">\(\quad\quad\)</span>       end if <br>
11: <span class="arithmatex">\(\quad\)</span>  end for  <br>
12:  <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> //Policy improvement<br>
12: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{k}\)</span>   <br>
13: end loop    </p>
<p>This process gradually adjusts the policy and the value estimates until they converge.</p>
<h2 id="reinforcement-4_model_free_control-iv-temporal-difference-td-control">IV. Temporal Difference (TD) Control<a class="headerlink" href="#reinforcement-4_model_free_control-iv-temporal-difference-td-control" title="Permanent link">¶</a></h2>
<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>
<h3 id="reinforcement-4_model_free_control-on-policy-td-control-sarsa">On-Policy TD Control: SARSA<a class="headerlink" href="#reinforcement-4_model_free_control-on-policy-td-control-sarsa" title="Permanent link">¶</a></h3>
<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy <em>currently being followed</em> (<span class="arithmatex">\(\pi\)</span>). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p>
<p>The update for the action-value <span class="arithmatex">\(Q(s_t, a_t)\)</span> uses the value of the <em>next</em> state-action pair, <span class="arithmatex">\((s_{t+1}, a_{t+1})\)</span>, selected by the current policy <span class="arithmatex">\(\pi\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</div>
<p>The TD Target here is <span class="arithmatex">\(r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})\)</span>. SARSA learns <span class="arithmatex">\(Q^{\pi}\)</span> while <span class="arithmatex">\(\pi\)</span> is improved greedily with respect to <span class="arithmatex">\(Q^{\pi}\)</span>, allowing it to find the optimal policy <span class="arithmatex">\(\pi^*\)</span>.</p>
<p>1: Set initial <span class="arithmatex">\(\epsilon\)</span>-greedy policy <span class="arithmatex">\(\pi\)</span> randomly, <span class="arithmatex">\(t=0\)</span>, initial state <span class="arithmatex">\(s_t=s_0\)</span>    <br>
2: Take <span class="arithmatex">\(a_t \sim \pi(s_t)\)</span>      <br>
3: Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>       <br>
4: loop       <br>
5: <span class="arithmatex">\(\quad\)</span> Take action <span class="arithmatex">\(a_{t+1} \sim \pi(s_{t+1})\)</span> // Sample action from policy        <br>
6: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_{t+1}, s_{t+2})\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Update <span class="arithmatex">\(Q\)</span> given <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\)</span>:    <script type="math/tex; mode=display">
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]</script>
8: <span class="arithmatex">\(\quad\)</span> Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p>
<div class="arithmatex">\[\forall s \in S,\;\;
\pi(s) =
\begin{cases}
\arg\max\limits_a Q(s,a) &amp; \text{with probability } 1 - \epsilon \\
\text{a random action}   &amp; \text{with probability } \epsilon
\end{cases}\]</div>
<p>9: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{t}\)</span>           <br>
10: end loop        </p>
<h3 id="reinforcement-4_model_free_control-b-off-policy-td-control-q-learning">B. Off-Policy TD Control: Q-Learning<a class="headerlink" href="#reinforcement-4_model_free_control-b-off-policy-td-control-q-learning" title="Permanent link">¶</a></h3>
<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (<span class="arithmatex">\(\pi^*\)</span>, the <em>target policy</em>) using experience gathered by a different behavior policy (<span class="arithmatex">\(\pi_b\)</span>).</p>
<p>In Q-Learning, the agent acts using a soft, exploratory <span class="arithmatex">\(\pi_b\)</span> (like <span class="arithmatex">\(\epsilon\)</span>-greedy) but the value function update is based on the <em>best</em> possible action from the next state, effectively estimating <span class="arithmatex">\(Q^*\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]</div>
<p>The key difference is the target: Q-Learning uses the value of the max action (<span class="arithmatex">\(\max_{a'} Q(s_{t+1}, a')\)</span>), regardless of what action was actually taken in the next step. This makes it a greedy update towards <span class="arithmatex">\(Q^*\)</span>.</p>
<p>Q-Learning (Off-Policy TD Control):</p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0 \quad \forall s \in S, a \in A\)</span>, set <span class="arithmatex">\(t = 0\)</span>, initial state <span class="arithmatex">\(s_t = s_0\)</span>       <br>
2: Set <span class="arithmatex">\(\pi_b\)</span> to be <span class="arithmatex">\(\epsilon\)</span>-greedy w.r.t. <span class="arithmatex">\(Q\)</span>     <br>
3: loop   <br>
4: <span class="arithmatex">\(\quad\)</span> Take <span class="arithmatex">\(a_t \sim \pi_b(s_t)\)</span> // Sample action from behavior policy   <br>
5: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>   <br>
6: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_t + \gamma \max\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \right]\)</span>      <br>
7: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi(s_t) =
\begin{cases}
\arg\max\limits_a Q(s_t,a) &amp; \text{with probability } 1 - \epsilon \
\text{a random action} &amp; \text{with probability } \epsilon
\end{cases}\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>    <br>
9: end loop     </p>
<h2 id="reinforcement-4_model_free_control-value-function-approximation-vfa">Value Function Approximation (VFA)<a class="headerlink" href="#reinforcement-4_model_free_control-value-function-approximation-vfa" title="Permanent link">¶</a></h2>
<p>All methods discussed so far assume a tabular representation, where a separate entry for <span class="arithmatex">\(Q(s, a)\)</span> is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>
<h3 id="reinforcement-4_model_free_control-motivation-for-approximation">Motivation for Approximation<a class="headerlink" href="#reinforcement-4_model_free_control-motivation-for-approximation" title="Permanent link">¶</a></h3>
<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p>
<ol>
<li>Memory: Explicitly storing every <span class="arithmatex">\(V\)</span> or <span class="arithmatex">\(Q\)</span> value is impossible.</li>
<li>Computation: Computing or updating every value is too slow.</li>
<li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li>
</ol>
<p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w}) \approx Q(s, a)\)</span>. The goal shifts from filling a table to finding the parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that minimizes the error between the true value and the estimate.</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \mathbb{E}_{\pi} \left[ \left( Q^{\pi}(s, a) - \hat{Q}(s, a; \mathbf{w}) \right)^2 \right]
\]</div>
<p>The parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function <span class="arithmatex">\(J(\mathbf{w})\)</span>.</p>
<h3 id="reinforcement-4_model_free_control-model-free-control-with-vfa-policy-evaluation">Model-Free Control with VFA Policy Evaluation<a class="headerlink" href="#reinforcement-4_model_free_control-model-free-control-with-vfa-policy-evaluation" title="Permanent link">¶</a></h3>
<p>When using function approximation, we substitute the old <span class="arithmatex">\(Q(s, a)\)</span> in the update rules (MC, SARSA, Q-Learning) with the function approximator <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span>.</p>
<ul>
<li>
<p>MC VFA for Policy Evaluation: </p>
<p>The return <span class="arithmatex">\(G_t\)</span> is used as the target in an SGD update: <span class="arithmatex">\(\Delta \mathbf{w} \propto \alpha (G_t - \hat{Q}(s_t, a_t; \mathbf{w})) \nabla_{\mathbf{w}} \hat{Q}(s_t, a_t; \mathbf{w})\)</span>.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, set <span class="arithmatex">\(k = 1\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,L_k})\)</span> given <span class="arithmatex">\(\pi\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1, \dots, L_k\)</span> do     <br>
5: <span class="arithmatex">\(\quad\quad\)</span> if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then     <br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(G_t(s,a) = \sum_{j=t}^{L_k} r_{k,j}\)</span>    <br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 \left[ G_t(s,a) - \hat{Q}(s_t,a_t;\mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})\)</span> // Compute Gradient   <br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span> Update weights: <span class="arithmatex">\(\Delta \mathbf{w}\)</span>      <br>
9: <span class="arithmatex">\(\quad\quad\)</span> end if<br>
10: <span class="arithmatex">\(\quad\)</span> end for 
11: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(k = k + 1\)</span>       <br>
12: end loop    </p>
</li>
<li>
<p>SARSA with VFA: The TD target is <span class="arithmatex">\(r + \gamma \hat{Q}(s', a'; \mathbf{w})\)</span>, leveraging the current function approximation.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(s\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Given <span class="arithmatex">\(s\)</span>, sample <span class="arithmatex">\(a \sim \pi(s)\)</span>, observe <span class="arithmatex">\(r(s,a)\)</span>, and <span class="arithmatex">\(s' \sim p(s'|s,a)\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 [r + \gamma \hat{V}(s';\mathbf{w}) - \hat{V}(s;\mathbf{w})] \nabla_{\mathbf{w}} \hat{V}(s;\mathbf{w})\)</span>     <br>
5: <span class="arithmatex">\(\quad\)</span> Update weights <span class="arithmatex">\(\Delta \mathbf{w}\)</span>         <br>
6: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(s'\)</span> is not a terminal state then   <br>
7: <span class="arithmatex">\(\quad\quad\)</span> Set <span class="arithmatex">\(s = s'\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> else       <br>
9: <span class="arithmatex">\(\quad\quad\)</span> Restart episode, sample initial state <span class="arithmatex">\(s\)</span>     <br>
10: <span class="arithmatex">\(\quad\)</span> end if    <br>
11: end loop      <br>
* Q-Learning with VFA: The TD target is <span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s', a'; \mathbf{w})\)</span>.</p>
</li>
</ul>
<h3 id="reinforcement-4_model_free_control-control-using-vfa">Control using VFA<a class="headerlink" href="#reinforcement-4_model_free_control-control-using-vfa" title="Permanent link">¶</a></h3>
<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular <span class="arithmatex">\(Q(s,a)\)</span> with a function approximator <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>, we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>.</p>
<p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each <span class="arithmatex">\((s,a)\)</span>, the agent learns a parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function <span class="arithmatex">\(Q^*(s,a)\)</span>.</p>
<p>The learning problem becomes:</p>
<div class="arithmatex">\[
\min_{\mathbf{w}} \; J(\mathbf{w}) = \mathbb{E} \left[ \left( Q^*(s,a) - \hat{Q}(s,a;\mathbf{w}) \right)^2 \right]
\]</div>
<p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p>
<div class="arithmatex">\[
\Delta \mathbf{w} \propto \left( \text{target} - \hat{Q}(s_t,a_t;\mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})
\]</div>
<p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Target for updating <span class="arithmatex">\(\mathbf{w}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Monte Carlo</td>
<td><span class="arithmatex">\(G_t\)</span></td>
</tr>
<tr>
<td>SARSA</td>
<td><span class="arithmatex">\(r + \gamma \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
<tr>
<td>Q-Learning</td>
<td><span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
</tbody>
</table>
<p>These methods now operate in the same way as before, except instead of updating a single <span class="arithmatex">\(Q(s,a)\)</span> entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>
<h3 id="reinforcement-4_model_free_control-challenges-the-deadly-triad">Challenges: The Deadly Triad<a class="headerlink" href="#reinforcement-4_model_free_control-challenges-the-deadly-triad" title="Permanent link">¶</a></h3>
<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p>
<div class="arithmatex">\[
\text{Function Approximation} \;+\; \text{Bootstrapping} \;+\; \text{Off-policy Learning}
\]</div>
<p>This combination is known as the Deadly Triad .</p>
<ul>
<li>Function Approximation : Generalizes across states but may introduce bias.</li>
<li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li>
<li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li>
</ul>
<p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p>
<p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate <span class="arithmatex">\(Q(s,a)\)</span>.</p>
<h3 id="reinforcement-4_model_free_control-deep-q-networks-dqn">Deep Q-Networks (DQN)<a class="headerlink" href="#reinforcement-4_model_free_control-deep-q-networks-dqn" title="Permanent link">¶</a></h3>
<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p>
<p>DQN stabilizes the non-linear learning process using two critical techniques:</p>
<ol>
<li>
<p>Experience Replay (ER): Transitions <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> are stored in a replay buffer (<span class="arithmatex">\(\mathcal{D}\)</span>). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from <span class="arithmatex">\(\mathcal{D}\)</span> for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p>
</li>
<li>
<p>Fixed Q-Targets: The Q-Learning update requires a target value <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w})\)</span>. To prevent the estimate <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> from chasing its own rapidly changing target, the parameters <span class="arithmatex">\(\mathbf{w}^{-}\)</span> used to compute the target are fixed for a period of time, then synchronized with the current parameters <span class="arithmatex">\(\mathbf{w}\)</span>. This provides a stable target <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w}^{-})\)</span>.</p>
</li>
</ol>
<p>Deep Q-Network (DQN) Algorithm:</p>
<p>1: Input <span class="arithmatex">\(C\)</span>, <span class="arithmatex">\(\alpha\)</span>, <span class="arithmatex">\(D = {}\)</span>, Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(\mathbf{w}^- = \mathbf{w}\)</span>, <span class="arithmatex">\(t = 0\)</span>       <br>
2: Get initial state <span class="arithmatex">\(s_0\)</span>            <br>
3: loop       <br>
4: <span class="arithmatex">\(\quad\)</span> Sample action <span class="arithmatex">\(a_t\)</span> using <span class="arithmatex">\(\epsilon\)</span>-greedy policy w.r.t. current <span class="arithmatex">\(\hat{Q}(s_t, a; \mathbf{w})\)</span>    <br>
5: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span>      <br>
6: <span class="arithmatex">\(\quad\)</span> Store transition <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> in replay buffer <span class="arithmatex">\(D\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Sample a random minibatch of tuples <span class="arithmatex">\((s_i, a_i, r_i, s'i)\)</span> from <span class="arithmatex">\(D\)</span>    <br>
8: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(j\)</span> in minibatch do    <br>
9: <span class="arithmatex">\(\quad\quad\)</span> if episode terminates at step <span class="arithmatex">\(i+1\)</span> then      <br>
10: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i\)</span>     <br>
11: <span class="arithmatex">\(\quad\quad\)</span> else     <br>
12: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i + \gamma \max\limits{a'} \hat{Q}(s'i, a'; \mathbf{w}^-)\)</span>     <br>
13: <span class="arithmatex">\(\quad\quad\)</span> end if   <br>
14: <span class="arithmatex">\(\quad\quad\)</span> Update <span class="arithmatex">\(\mathbf{w}\)</span> using gradient descent:  <br>
<span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\Delta \mathbf{w} = \alpha \left( y_i - \hat{Q}(s_i, a_i; \mathbf{w}) \right) \nabla{\mathbf{w}} \hat{Q}(s_i, a_i; \mathbf{w})\)</span>    <br>
15: <span class="arithmatex">\(\quad\)</span> end for       <br>
16: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>   <br>
17: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(t \mod C == 0\)</span> then   <br>
18: <span class="arithmatex">\(\quad\quad\)</span> <span class="arithmatex">\(\mathbf{w}^- \leftarrow \mathbf{w}\)</span>     <br>
19: <span class="arithmatex">\(\quad\)</span> end if    <br>
20: end loop        </p>
<h2 id="reinforcement-4_model_free_control-model-free-control-mental-map">Model Free Control Mental Map<a class="headerlink" href="#reinforcement-4_model_free_control-model-free-control-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                     Model-Free Control
    Goal: Learn the Optimal Policy π* without knowing P or R
                               │
                               ▼
           Key Concept: Action-Value Function Q(s,a)
       ┌─────────────────────────────────────────────┐
       │Qπ(s,a) = Expected return by taking action a │
       │in state s and following policy π thereafter │
       └─────────────────────────────────────────────┘
                               │
                      No model → Learn Q directly
                               │
                               ▼
                   Generalized Policy Iteration
       ┌───────────────────────────┬───────────────────────────┐
       │   Policy Evaluation       │     Policy Improvement    │
       │   Learn Qπ(s,a)           │   π ← greedy w.r.t Q      │
       └───────────────────────────┴───────────────────────────┘
                               │
                               ▼
                Challenge: Exploration vs. Exploitation
       ┌──────────────────────────────────────────────────────┐
       │Greedy policy → Exploits but stops exploring          │
       │ε-greedy policy → Balances exploration &amp; exploitation │
       │GLIE condition: ε → 0 and ∞ exploration               │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                Model-Free Control Families (Tabular)
       ┌────────────────────────────┬────────────────────────────┐
       │   Monte Carlo Control      │      Temporal Difference   │
       │   (Episode-based)          │      (Step-based)          │
       └────────────────────────────┴────────────────────────────┘
                               │
          ┌────────────────────┴───────────────────┐
          ▼                                        ▼
 Monte Carlo Control:                       TD Control:
 Estimates Q from full returns          Estimates Q usingbootstrapped targets
 Uses ε-greedy policy                   Works online, faster, low variance
 Episodic only                          Works for episodic &amp; continuing
          │                                        │
    ┌─────┴─────────────┐             ┌────────────┴───────────────┐
    │ GLIE MC Control   │             │ On-Policy TD: SARSA        │
    └───────────────────┘             │ Off-Policy TD: Q-Learning  │
                                      └────────────────────────────┘
                                                    |
                                                    ▼
┌──────────────────────────────────────────────────────────────────────────────┐
| On-Policy TD — SARSA                     |  Off-Policy TD — Q-Learning       |
| Learns Qπ for the policy being followed  |  Learns Q* while following π_b    |
| Update uses next action from π           |  Update uses max action (greedy)  |
| Update Target:                           |  Update Target:                   |
|  r + γ Q(s',a')                          |  r + γ maxₐ Q(s',a)               |
└────────────────────────────┴─────────────────────────────────────────────────┘



      Value Function Approximation (Large/Continuous spaces)
       ┌──────────────────────────────────────────────────────┐
       │ Replace Q(s,a) with Q̂(s,a;w) using function approx   │
       │ Generalization across states                         │
       │ Gradient-based updates (SGD)                         │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
            Deep Q-Learning (DQN) — Stable VFA Control
       ┌──────────────────────────────────────────────────────┐
       │ Experience Replay — decorrelate samples             │
       │ Target Networks — stabilize bootstrapped targets    │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                      Final Outcome of Model-Free Control
       ┌───────────────────────────────────────────────────────┐
       │ Learn π* directly from experience without model       │
       │ Learn Q*(s,a) through MC, SARSA, or Q-Learning        │
       │ Scale to large spaces using function approximation    │
       │ DQN enables deep RL in complex environments           │
       └───────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-5_policy_gradient" heading-number="2.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-policy-gradient-methods">Chapter 5: Policy Gradient Methods<a class="headerlink" href="#reinforcement-5_policy_gradient-chapter-5-policy-gradient-methods" title="Permanent link">¶</a></h1>
<p>In previous chapters, we derived policies indirectly from value functions using greedy or ε-greedy strategies. However, value-based RL has several challenges:</p>
<ul>
<li>Does not naturally support stochastic policies  </li>
<li>Struggles in continuous action spaces  </li>
<li>Optimizing through value functions is often indirect and unstable</li>
</ul>
<p>Policy Gradient Methods directly optimize the policy itself:</p>
<div class="arithmatex">\[
\pi_\theta(a|s) = P(a \mid s; \theta)
\]</div>
<p>Our goal becomes:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta V(\theta)
\]</div>
<p>That is, learn policy parameters θ that maximize expected return.</p>
<blockquote>
<p>Value-based methods struggle in these cases because they do not directly learn the policy. Instead, they estimate action values <span class="arithmatex">\(Q(s,a)\)</span> and derive a policy using greedy or ε-greedy strategies. This makes the policy indirect and unstable. Small changes in <span class="arithmatex">\(Q(s,a)\)</span> can suddenly change the best action, making learning discontinuous and erratic — especially with function approximation like neural networks. Furthermore, value-based methods do not naturally support stochastic or continuous action spaces, since computing <span class="arithmatex">\(\arg\max_a Q(s,a)\)</span> is infeasible when actions are continuous or infinite. Policy-based methods solve this problem by directly modeling and learning the policy, such as using a softmax distribution for discrete actions or Gaussian distributions for continuous actions.</p>
</blockquote>
<h2 id="reinforcement-5_policy_gradient-value-based-vs-policy-based-rl">Value-Based vs Policy-Based RL<a class="headerlink" href="#reinforcement-5_policy_gradient-value-based-vs-policy-based-rl" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Approach</th>
<th>What is Learned?</th>
<th>Policy Type</th>
<th>Works in Continuous Actions?</th>
</tr>
</thead>
<tbody>
<tr>
<td>Value-Based</td>
<td><span class="arithmatex">\(V(s)\)</span> or <span class="arithmatex">\(Q(s,a)\)</span></td>
<td>Indirect (ε-greedy, greedy)</td>
<td>No</td>
</tr>
<tr>
<td>Policy-Based</td>
<td><span class="arithmatex">\(\pi_\theta(a/s)\)</span></td>
<td>Direct, stochastic</td>
<td>Yes</td>
</tr>
<tr>
<td>Actor-Critic</td>
<td>Both</td>
<td>Direct &amp; learned</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-5_policy_gradient-policy-optimization-objective">Policy Optimization Objective<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-optimization-objective" title="Permanent link">¶</a></h2>
<p>In policy-based reinforcement learning, the policy itself is directly parameterized as <span class="arithmatex">\(\pi_\theta(a|s)\)</span>, and our goal is to find the parameters <span class="arithmatex">\(\theta\)</span> that produce the best possible behavior. For episodic tasks starting at initial state <span class="arithmatex">\(s_0\)</span>, the quality of a policy is measured by its expected return:</p>
<div class="arithmatex">\[
V(\theta) = V_{\pi_\theta}(s_0) = \mathbb{E}_{\pi_\theta}[G_0]
\]</div>
<p>Therefore, policy optimization can be formulated as an optimization problem, where the goal is to find the policy parameters <span class="arithmatex">\(\theta\)</span> that maximize the expected return:</p>
<div class="arithmatex">\[
\theta^* = \arg\max_\theta V(s_0, \theta)
\]</div>
<p>This optimization does not necessarily require gradients.  We can also use gradient-free (derivative-free) optimization methods such as:</p>
<ul>
<li>Hill Climbing – Iteratively adjusts parameters in small random directions and keeps changes that improve performance.</li>
<li>Simplex / Amoeba / Nelder-Mead – Uses a geometric shape (simplex) to explore the parameter space and moves it towards higher-performing regions.</li>
<li>Genetic Algorithms – Evolves a population of candidate policies using selection, crossover, and mutation, inspired by natural evolution.</li>
<li>Cross-Entropy Method (CEM) – Samples multiple policy candidates, selects the top performers, and updates the sampling distribution towards them.</li>
<li>Covariance Matrix Adaptation (CMA) – Adapts both the mean and covariance of a Gaussian distribution to efficiently search complex, high-dimensional policy spaces.</li>
</ul>
<p>Gradient-free policy optimization methods are often excellent and simple baselines to try.  They are highly flexible, can work with any policy parameterization (including non-differentiable ones), and are easy to parallelize, as policies can be evaluated independently across multiple environments. However, these methods are typically less sample efficient because they treat each policy evaluation as a black box and ignore the temporal structure of trajectories. They do not make use of gradients, value functions, or bootstrapping.</p>
<p>To improve efficiency, we can use gradient-based optimization techniques, which exploit  the structure of the return function and update parameters using local information. Common gradient-based optimizers include:</p>
<ul>
<li>Gradient Descent</li>
<li>Conjugate Gradient</li>
<li>Quasi-Newton Methods (e.g., BFGS, L-BFGS)</li>
</ul>
<p>These methods are generally more sample efficient, especially in large or continuous state-action spaces.</p>
<h2 id="reinforcement-5_policy_gradient-policy-gradient">Policy Gradient<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-gradient" title="Permanent link">¶</a></h2>
<p>The goal is to find parameters <span class="arithmatex">\(\theta\)</span> that maximize the expected return. Policy gradient algorithms search for a local maximum of <span class="arithmatex">\(V(\theta)\)</span> by performing gradient ascent:</p>
<div class="arithmatex">\[
\Delta \theta = \alpha \nabla_\theta V(s_0, \theta)
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is the step-size (learning rate) and <span class="arithmatex">\(\nabla_\theta V(s_0, \theta)\)</span> is the policy gradient.</p>
<p>Assuming an episodic MDP with discount factor <span class="arithmatex">\(\gamma = 1\)</span>, the value of a parameterized policy <span class="arithmatex">\(\pi_\theta\)</span> starting from state <span class="arithmatex">\(s_0\)</span> is</p>
<div class="arithmatex">\[
V(s_0,\theta) 
= \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} r(s_t, a_t); \; s_0, \pi_\theta \right],
\]</div>
<p>where the expectation is taken over the states and actions visited when following <span class="arithmatex">\(\pi_\theta\)</span>. This policy value can be re-expressed in multiple ways.  </p>
<ul>
<li>
<p>First, in terms of the action-value function:
    <script type="math/tex; mode=display">
    V(s_0,\theta) = \sum_{a} \pi_\theta(a \mid s_0) \, Q(s_0,a;\theta).
    </script>
</p>
</li>
<li>
<p>Second, in terms of full trajectories. Let a state–action trajectory be:</p>
<p><span class="arithmatex">\(\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\dots,s_{T-1},a_{T-1},r_T),\)</span></p>
<p>and define</p>
<p>
<script type="math/tex; mode=display">
R(\tau) = \sum_{t=0}^{T} r_{s_t,a_t}</script>
as the sum of rewards of trajectory <span class="arithmatex">\(\tau\)</span>.  </p>
<p>Let <span class="arithmatex">\(P(\tau;\theta)\)</span> denote the probability of trajectory <span class="arithmatex">\(\tau\)</span> when starting in <span class="arithmatex">\(s_0\)</span> and following policy <span class="arithmatex">\(\pi_\theta\)</span>. Then:</p>
<div class="arithmatex">\[
V(s_0,\theta) = \sum_{\tau} P(\tau;\theta) \, R(\tau).\]</div>
</li>
</ul>
<p>In this trajectory notation, our optimization objective becomes</p>
<div class="arithmatex">\[
\theta^* 
= \arg\max_{\theta} V(s_0,\theta) 
= \arg\max_{\theta} \sum_{\tau} P(\tau;\theta) \, R(\tau).
\]</div>
<p>Taking gradient yields:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) = 
\sum_{\tau} P(\tau|\theta)R(\tau) 
\nabla_\theta \log P(\tau|\theta)
\]</div>
<p>Using sampled trajectories:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m}\sum_{i=1}^{m} 
R(\tau^{(i)})
\nabla_\theta \log P(\tau^{(i)}|\theta)
\]</div>
<p>Trajectory probability:</p>
<div class="arithmatex">\[
P(\tau|\theta) =
P(s_0)
\prod_{t=0}^{T-1}
\pi_\theta(a_t|s_t)\cdot P(s_{t+1}|s_t,a_t)
\]</div>
<p>Since dynamics are independent of <span class="arithmatex">\(\theta\)</span>:</p>
<div class="arithmatex">\[
\nabla_\theta \log P(\tau|\theta) =
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t|s_t)
\]</div>
<p>Thus:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m}\sum_{i=1}^{m}
R(\tau^{(i)})
\sum_{t=0}^{T-1} 
\nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
\]</div>
<p>The term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span> is called the score function. 
It is the gradient of the log of a parameterized probability distribution and measures how sensitive the policy’s action probability is to changes in the parameters <span class="arithmatex">\(\theta\)</span>.  It plays a central role in policy gradient methods because it allows us to estimate gradients without knowing the environment dynamics, using only samples from the policy.</p>
<h4 id="reinforcement-5_policy_gradient-softmax-policy-discrete-action-spaces">Softmax Policy (Discrete Action Spaces)<a class="headerlink" href="#reinforcement-5_policy_gradient-softmax-policy-discrete-action-spaces" title="Permanent link">¶</a></h4>
<p>In discrete action spaces, a common parameterization of the policy is the softmax policy, which assigns probabilities based on exponentiated weighted features. Each action is represented using feature vector <span class="arithmatex">\(\phi(s,a)\)</span>, and the policy is defined as:</p>
<div class="arithmatex">\[
\pi_\theta(a|s) = 
\frac{e^{\phi(s,a)^T \theta}}
     {\sum_{a'} e^{\phi(s,a')^T \theta}}
\]</div>
<p>The corresponding score function is:</p>
<div class="arithmatex">\[
\nabla_\theta \log \pi_\theta(a|s)
= \phi(s,a) - \mathbb{E}_{a' \sim \pi_\theta}[\phi(s,a')]
\]</div>
<p>This means the gradient increases the probability of the selected action's features and decreases the probability of competing actions based on their expected feature values.</p>
<h4 id="reinforcement-5_policy_gradient-gaussian-policy-continuous-action-spaces">Gaussian Policy (Continuous Action Spaces)<a class="headerlink" href="#reinforcement-5_policy_gradient-gaussian-policy-continuous-action-spaces" title="Permanent link">¶</a></h4>
<p>For continuous action spaces, the Gaussian policy is a natural choice.  The policy outputs actions by sampling from a normal distribution:</p>
<div class="arithmatex">\[
a \sim \mathcal{N}(\mu(s), \sigma^2)
\]</div>
<p>The mean is a linear function of state features:</p>
<div class="arithmatex">\[
\mu(s) = \phi(s)^T \theta
\]</div>
<p>If we assume a fixed variance <span class="arithmatex">\(\sigma^2\)</span>, the score function becomes:</p>
<div class="arithmatex">\[
\nabla_\theta \log \pi_\theta(a|s)
= \frac{(a - \mu(s))}{\sigma^2} \, \phi(s)
\]</div>
<p>This tells us that the gradient increases the likelihood of actions that are close to the mean <span class="arithmatex">\(\mu(s)\)</span> and reduces the probability of actions that deviate from it.</p>
<p>Deep neural networks (and other differentiable models) can also be used<br>
to represent <span class="arithmatex">\(\pi_\theta(a|s)\)</span>, allowing score functions to be computed automatically using backpropagation.</p>
<blockquote>
<p>Intution: 
Think of a sample trajectory <span class="arithmatex">\(\tau\)</span> as something we tried — a sequence of states, actions, and rewards collected during an episode. The return <span class="arithmatex">\(R(\tau)\)</span> tells us how good that sample was (higher return means better behavior).
The gradient term <span class="arithmatex">\(\nabla_\theta \log P(\tau|\theta)\)</span> tells us how to adjust the policy parameters <span class="arithmatex">\(\theta\)</span> to make the trajectory more or less likely.
So, when we multiply them:
<script type="math/tex; mode=display">
R(\tau) \, \nabla_\theta \log P(\tau \mid \theta)</script>
we are effectively saying:
If a trajectory was good, update the policy to make it more likely to occur again.<br>
If it was bad, update the policy to make it less likely.
This simple idea is the core of policy gradient methods.</p>
</blockquote>
<h2 id="reinforcement-5_policy_gradient-reinforce-algorithm-monte-carlo-policy-gradient">REINFORCE Algorithm (Monte Carlo Policy Gradient)<a class="headerlink" href="#reinforcement-5_policy_gradient-reinforce-algorithm-monte-carlo-policy-gradient" title="Permanent link">¶</a></h2>
<p>Update rule:</p>
<div class="arithmatex">\[
\Delta \theta = \alpha \nabla_\theta \log \pi_\theta(a_t|s_t)\, R_t
\]</div>
<p>Algorithm:</p>
<p>1: Initialize policy parameters <span class="arithmatex">\(\theta\)</span><br>
2: loop (for each episode)<br>
3: <span class="arithmatex">\(\quad\)</span> Generate a trajectory <span class="arithmatex">\(\tau = (s_0, a_0, r_1, \dots, s_T)\)</span> using <span class="arithmatex">\(\pi_\theta\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> for each time step <span class="arithmatex">\(t\)</span> in <span class="arithmatex">\(\tau\)</span><br>
5: <span class="arithmatex">\(\quad\quad\)</span> Compute return: <span class="arithmatex">\(\qquad R_t = \sum_{k=t}^{T-1} \gamma^{\,k-t} r_{k+1}\)</span><br>
6: <span class="arithmatex">\(\quad\quad\)</span> Compute policy gradient term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span><br>
7: <span class="arithmatex">\(\quad\quad\)</span> Update policy parameters: <span class="arithmatex">\(\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t)\, R_t\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> end for<br>
9: end loop  </p>
<h2 id="reinforcement-5_policy_gradient-policy-gradient-methods-mental-map">Policy Gradient Methods — Mental Map<a class="headerlink" href="#reinforcement-5_policy_gradient-policy-gradient-methods-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                     Policy Gradient Methods
    Goal: Learn the optimal policy π* directly (no Q or V tables)
                               │
                               ▼
         Key Concept: Parameterized Policy πθ(a|s)
       ┌─────────────────────────────────────────────┐
       │ Policy is a function with parameters θ      │
       │ πθ(a|s) gives probability of taking action a│
       │ Optimization targets J(θ)=Expected Return   │
       └─────────────────────────────────────────────┘
                               │
                    Direct Policy Optimization
                               │
                               ▼
                 Optimization Objective (J(θ))
       ┌─────────────────────────────────────────────┐
       │ θ* = argmaxθ V(θ) = argmaxθ Eπθ[G₀]         │
       │ Search in parameter space for best policy   │
       └─────────────────────────────────────────────┘
                               │
                               ▼
              Two Families of Policy Optimization
       ┌────────────────────────────┬────────────────────────────┐
       │  Gradient-Free Methods     │   Gradient-Based Methods   │
       └────────────────────────────┴────────────────────────────┘
                │                                      │
                │                                      ▼
                │                          Policy Gradient Methods
                │                                      │
                ▼                                      ▼
   No gradient needed                     Uses ∇θ log πθ(a|s) * Return
   – Hill Climbing                        – REINFORCE
   – CEM, CMA                             – Actor-Critic
   – Genetic Algorithms                   – Advantage Methods
   │                                      │
   └──── Flexible &amp; parallelizable        └──── Sample efficient
                               │
                               ▼
                   Policy Gradient Core Idea
       ┌───────────────────────────────────────────────┐
       │ Increase probability of good actions          │
       │ Decrease probability of poor actions          │
       │ Gradient term: ∇θ log πθ(a|s)(score function) │
       └───────────────────────────────────────────────┘
                               │
                               ▼
                     REINFORCE Algorithm (MC)
       ┌───────────────────────────────────────────────┐
       │ Sample full episodes (Monte Carlo)            │
       │ Compute return Gt at each time step           │
       │ Update: θ ← θ + α ∇θ log πθ(a_t|s_t) * G_t    │
       └───────────────────────────────────────────────┘
                               │
                               ▼
                      Policy Parameterization
       ┌────────────────────────────┬────────────────────────────┐
       │ Softmax Policy (Discrete)  │ Gaussian Policy(Continuous)│
       │ πθ(a|s) = exp(...)         │ a ~ N(μ(s), σ²)            │
       │ ∇θ log πθ = φ - Eφ         │ ∇θ log πθ = (a-μ)/σ² * φ   │
       └────────────────────────────┴────────────────────────────┘
                               │
                               ▼
                         Final Outcome
       ┌─────────────────────────────────────────────────┐
       │ Learn π* directly (no need for Q or V tables)   │
       │ Works naturally with stochastic &amp; continuous    │
       │ Supports neural network policy parameterization │
       │ Foundation of modern deep RL (PPO, A3C, DDPG)   │
       └─────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-6_pg2" heading-number="2.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-policy-gradient-variance-reduction-and-actor-critic">Chapter 6: Policy Gradient Variance Reduction and Actor-Critic<a class="headerlink" href="#reinforcement-6_pg2-chapter-6-policy-gradient-variance-reduction-and-actor-critic" title="Permanent link">¶</a></h1>
<p>In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy <span class="arithmatex">\(\pi_\theta(a|s)\)</span> in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:</p>
<ul>
<li>
<p>High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return <span class="arithmatex">\(G_t\)</span> depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.</p>
</li>
<li>
<p>Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.</p>
</li>
<li>
<p>Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.</p>
</li>
<li>
<p>Parameter Space vs Policy Space Mismatch: A small change in policy parameters <span class="arithmatex">\(\theta\)</span> does not always translate to a small change in the policy’s behavior. For example, in a two-action policy with probability <span class="arithmatex">\(\pi_\theta(a=1)=\sigma(\theta)\)</span> (sigmoid), a slight shift in <span class="arithmatex">\(\theta\)</span> can swing the action probabilities significantly if <span class="arithmatex">\(\theta\)</span> is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.</p>
</li>
</ul>
<p>The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor–critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.</p>
<h2 id="reinforcement-6_pg2-policy-gradient-theorem-and-reinforce">Policy Gradient Theorem and REINFORCE<a class="headerlink" href="#reinforcement-6_pg2-policy-gradient-theorem-and-reinforce" title="Permanent link">¶</a></h2>
<p>n the previous chapter, we derived an expression for the gradient of the policy objective:</p>
<div class="arithmatex">\[
\nabla_\theta V(\theta) \approx
\frac{1}{m} \sum_{i=1}^{m}
R(\tau^{(i)}) 
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)}),
\]</div>
<p>where each trajectory <span class="arithmatex">\(\tau^{(i)}\)</span> is generated by the current policy <span class="arithmatex">\(\pi_\theta\)</span>.</p>
<p>This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:</p>
<ul>
<li>The return <span class="arithmatex">\(R(\tau)\)</span> depends on the entire trajectory.</li>
<li>Different trajectories can have very different returns.</li>
<li>Updates become noisy, unstable, and slow to converge.</li>
</ul>
<p>In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.</p>
<p>Goal:<br>
Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.</p>
<h3 id="reinforcement-6_pg2-reducing-variance-with-baselines">Reducing Variance with Baselines<a class="headerlink" href="#reinforcement-6_pg2-reducing-variance-with-baselines" title="Permanent link">¶</a></h3>
<p>One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action’s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return.
Mathematically, we modify the gradient as follows:</p>
<div class="arithmatex">\[
\nabla_\theta \mathbb{E}_\tau [R] \;=\;
\mathbb{E}_\tau \left[
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\;
\left(\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\right)
\right].
\]</div>
<p>where <span class="arithmatex">\(b(s_t)\)</span> is an arbitrary baseline that depends on the state <span class="arithmatex">\(s_t\)</span> (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, <span class="arithmatex">\(b(s_t)\)</span> represents a reference level for the return at state <span class="arithmatex">\(s_t\)</span>; the term <span class="arithmatex">\((G_t - b(s_t))\)</span> is asking: did the action at <span class="arithmatex">\(s_t\)</span> do better or worse than this baseline expectation?</p>
<p>A particularly good choice for the baseline is the value function under the current policy, <span class="arithmatex">\(b(s_t) \approx V_{\pi}(s_t)\)</span>. This is the expected return from state <span class="arithmatex">\(s_t\)</span> if we continue following the current policy. Using <span class="arithmatex">\(V_{\pi}(s_t)\)</span> as <span class="arithmatex">\(b(s_t)\)</span> minimizes variance because it subtracts out the expected part of <span class="arithmatex">\(G_t\)</span>, leaving only the unexpected advantage of the action <span class="arithmatex">\(a_t\)</span>.
Using a value function baseline leads to defining the advantage function:
<script type="math/tex; mode=display"> A(s_t, a_t) \;=\; Q(s_t, a_t) - V(s_t), </script>
where <span class="arithmatex">\(Q(s_t,a_t)\)</span> is the expected return for taking action <span class="arithmatex">\(a_t\)</span> in <span class="arithmatex">\(s_t\)</span> and following the policy thereafter, and <span class="arithmatex">\(V(s_t)\)</span> is the expected return from <span class="arithmatex">\(s_t\)</span> on average. The advantage <span class="arithmatex">\(A(s_t,a_t)\)</span> tells us how much better or worse the chosen action was compared to the policy’s typical action at that state. If <span class="arithmatex">\(A(s_t,a_t)\)</span> is positive, the action did better than expected; if negative, it did worse than expected. Using <span class="arithmatex">\(A(s_t,a_t)\)</span> in the gradient update focuses learning on the deviations from usual outcomes.</p>
<p>Benefits of Using a Baseline (Advantage):
1.  Variance Reduction: Subtracting <span class="arithmatex">\(V(s_t)\)</span> removes the predictable part of the return, reducing the variability of the term <span class="arithmatex">\((G_t - b(s_t))\)</span>. The policy update then depends on advantage, which typically has lower variance than raw returns.
2.  Focused Learning: The update ignores outcomes that are “as expected” and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment.
3.  Unbiased Gradient: Because <span class="arithmatex">\(b(s_t)\)</span> does not depend on the action, the expected value of <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\, b(s_t)\)</span> is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance</p>
<p>Using a baseline in practice usually means we need to estimate the value function <span class="arithmatex">\(V_{\pi}(s)\)</span> for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress <span class="arithmatex">\(b(s)\)</span> toward the observed returns <span class="arithmatex">\(G_t\)</span> to improve the baseline estimate.</p>
<p>Algorithm: Policy Gradient with Baseline (Advantage Estimation)</p>
<p>1: Initialize policy parameter <span class="arithmatex">\(\theta\)</span>, baseline <span class="arithmatex">\(b(s)\)</span><br>
2: for iteration <span class="arithmatex">\(= 1, 2, \dots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect a set of trajectories by executing the current policy <span class="arithmatex">\(\pi_\theta\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> for each trajectory <span class="arithmatex">\(\tau^{(i)}\)</span> and each timestep <span class="arithmatex">\(t\)</span> do<br>
5: <span class="arithmatex">\(\quad\quad\)</span> Compute return:<br>
<span class="arithmatex">\(\quad\quad\quad G_t^{(i)} = \sum_{t'=t}^{T-1} r_{t'}^{(i)}\)</span><br>
6: <span class="arithmatex">\(\quad\quad\)</span> Compute advantage estimate:<br>
<span class="arithmatex">\(\quad\quad\quad \hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> end for<br>
8: <span class="arithmatex">\(\quad\)</span> Re-fit baseline by minimizing:<br>
<span class="arithmatex">\(\quad\quad \sum_i \sum_t \big(b(s_t^{(i)}) - G_t^{(i)}\big)^2\)</span><br>
9: <span class="arithmatex">\(\quad\)</span> Update policy parameters using gradient estimate:<br>
<span class="arithmatex">\(\quad\quad \theta \leftarrow \theta + \alpha \sum_{i,t} \nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\, \hat{A}_t^{(i)}\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> (Plug into SGD or Adam optimizer)<br>
11: end for  </p>
<p>This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect (<span class="arithmatex">\(b(s)=V_\pi(s)\)</span>), then <span class="arithmatex">\(\hat{A}t = G_t - V\pi(s_t)\)</span> is an estimate of the advantage <span class="arithmatex">\(A(s_t,a_t)\)</span>. Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.</p>
<h2 id="reinforcement-6_pg2-actorcritic-methods">Actor–Critic Methods<a class="headerlink" href="#reinforcement-6_pg2-actorcritic-methods" title="Permanent link">¶</a></h2>
<p>Using a learned baseline brings us to the idea of actor–critic algorithms. In the policy gradient with baseline above, the policy is the "actor" and the value function baseline is a "critic" that evaluates the actor’s decisions. Actor–critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.</p>
<ul>
<li>Actor: the policy <span class="arithmatex">\(\pi_\theta(a|s)\)</span> that selects actions and is updated by gradient ascent.</li>
<li>Critic: a value function <span class="arithmatex">\(V_w(s)\)</span> (with parameters <span class="arithmatex">\(w\)</span>) that estimates the return from state <span class="arithmatex">\(s\)</span> under the current policy. The critic provides the baseline or advantage estimates used in the actor’s update.</li>
</ul>
<p>Instead of waiting for full episode returns <span class="arithmatex">\(G_t\)</span>, an actor–critic uses the critic’s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic’s current estimate of advantage <span class="arithmatex">\(A(s_t,a_t)\)</span>, and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.</p>
<p>Actor update: The policy (actor) update is similar to before, but using the critic’s advantage estimate <span class="arithmatex">\(A_t\)</span> at time <span class="arithmatex">\(t\)</span>:
<script type="math/tex; mode=display"> \theta \;\leftarrow\; \theta + \alpha\, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\; A_t. </script>
Here <span class="arithmatex">\(A_t \approx Q(s_t,a_t) - V(s_t)\)</span> is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive <span class="arithmatex">\(A_t\)</span>) and away from actions that seem worse than expected.</p>
<p>Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error <span class="arithmatex">\(\delta_t\)</span> to update <span class="arithmatex">\(w\)</span>:
<script type="math/tex; mode=display"> \delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t), </script>
which measures the discrepancy between the predicted value at <span class="arithmatex">\(s_t\)</span> and the reward plus discounted value of the next state. The critic’s parameters <span class="arithmatex">\(w\)</span> are updated by a gradient step proportional to <span class="arithmatex">\(\delta_t \nabla_w V_w(s_t)\)</span> (this is essentially a TD(0) update). In practice:
<script type="math/tex; mode=display"> w \;\leftarrow\; w + \beta\, \delta_t\, \nabla_w V_w(s_t), </script>
with <span class="arithmatex">\(\beta\)</span> a critic learning rate. This update pushes the critic’s value estimate <span class="arithmatex">\(V_w(s_t)\)</span> toward the observed reward plus the estimated value of <span class="arithmatex">\(s_{t+1}\)</span>. The TD error <span class="arithmatex">\(\delta_t\)</span> is also used as an advantage estimate for the actor: notice <span class="arithmatex">\(\delta_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)\)</span> serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.</p>
<p>Why Actor–Critic? Actor–critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor–critic algorithms tend to:
- Learn faster (lower variance updates thanks to the critic’s guidance)
- Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends)
- Naturally handle continuing (non-episodic) tasks via the critic’s ongoing value estimates
- Still allow stochastic policies and continuous actions (since the actor is explicit)</p>
<p>However, actor–critics introduce bias through the critic’s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor–critic; later algorithms address this).</p>
<p>Advantage Estimation: In an actor–critic, one often uses n-step returns or more generally <span class="arithmatex">\(\lambda\)</span>-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return <span class="arithmatex">\(R^{(3)}t = r_t + \gamma r{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3})\)</span>, and define <span class="arithmatex">\(\hat{A}_t = R^{(3)}_t - V(s_t)\)</span>. Smaller <span class="arithmatex">\(n\)</span> gives lower variance but more bias; larger <span class="arithmatex">\(n\)</span> gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.</p>
<p>In summary, the actor–critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy’s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.</p>
<h2 id="reinforcement-6_pg2-limitations-of-vanilla-policy-gradient-and-trust-region-motivation">Limitations of Vanilla Policy Gradient and Trust-Region Motivation<a class="headerlink" href="#reinforcement-6_pg2-limitations-of-vanilla-policy-gradient-and-trust-region-motivation" title="Permanent link">¶</a></h2>
<p>Despite using baselines and even actor–critic methods, vanilla policy gradient algorithms (including basic actor–critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:</p>
<ul>
<li>
<p>On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy <span class="arithmatex">\(\pi_{\text{old}}\)</span> to update the current policy <span class="arithmatex">\(\pi_{\text{new}}\)</span> introduces bias, because the gradient formula assumes data comes from <span class="arithmatex">\(\pi_{\text{new}}\)</span>. In short, standard policy gradients waste a lot of data, making them sample-inefficient.</p>
</li>
<li>
<p>Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios <span class="arithmatex">\(r_t = \frac{\pi_{\text{new}}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}\)</span>. This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if <span class="arithmatex">\(\pi_{\text{new}}\)</span> differs significantly from <span class="arithmatex">\(\pi_{\text{old}}\)</span>. A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.</p>
</li>
<li>
<p>Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile – even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not “too large” in terms of its impact on the policy’s behavior.</p>
</li>
<li>
<p>Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in <span class="arithmatex">\(\theta\)</span>-space. A small change in parameters can lead to a disproportionate change in the policy’s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes ≠ small policy changes, it’s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.</p>
</li>
</ul>
<p>These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update – in other words, to stay within a “trust region” around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback–Leibler divergence as a distance between <span class="arithmatex">\(\pi_{\text{new}}\)</span> and <span class="arithmatex">\(\pi_{\text{old}}\)</span>), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor–critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.</p></body></html></section><section class="print-page" id="reinforcement-7_gae" heading-number="2.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo">Chapter 7: Advances in Policy Optimization – GAE, TRPO, and PPO<a class="headerlink" href="#reinforcement-7_gae-chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo" title="Permanent link">¶</a></h1>
<p>In the previous chapter, we improved the foundation of policy gradients by reducing variance (using baselines) and introducing actor–critic methods. We also noted that unrestricted policy updates can be unstable and sample-inefficient. In this chapter, we present modern advances in policy optimization that build on those ideas to achieve much better performance in practice. We focus on two main developments: Generalized Advantage Estimation (GAE), which refines how we estimate advantages to balance bias and variance, and trust-region methods (specifically Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)) that ensure updates do not destabilize the policy. These techniques enable more sample-efficient, stable learning by reusing data safely and preventing large detrimental policy shifts.</p>
<h2 id="reinforcement-7_gae-generalized-advantage-estimation-gae">Generalized Advantage Estimation (GAE)<a class="headerlink" href="#reinforcement-7_gae-generalized-advantage-estimation-gae" title="Permanent link">¶</a></h2>
<p>Accurate and low-variance advantage estimates are crucial for effective policy gradient updates. Recall that the policy gradient update uses the term <span class="arithmatex">\(\nabla_\theta \log \pi_\theta(a_t|s_t)\,A(s_t,a_t)\)</span> – if <span class="arithmatex">\(A(s_t,a_t)\)</span> is noisy or biased, it can severely affect learning. Advantage can be estimated via:
- Monte Carlo returns: <span class="arithmatex">\(A_t = G_t - V(s_t)\)</span> using the full return <span class="arithmatex">\(G_t\)</span> (summing all future rewards until episode end). This is an unbiased estimator of the true advantage, but it has very high variance because it includes all random future outcomes.</p>
<ul>
<li>
<p>One-step TD returns: <span class="arithmatex">\(A_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)\)</span>, using the critic’s bootstrapped estimate of the future. This one-step advantage (equivalently the TD error <span class="arithmatex">\(\delta_t\)</span>) has much lower variance (it relies on the learned value for the next state) but is biased by function approximation and by truncating the return after one step.</p>
</li>
<li>
<p>n-Step returns:We can also use intermediate approaches, for example a 2-step return <span class="arithmatex">\(R^{(2)}t = r_t + \gamma r{t+1} + \gamma^2 V(s_{t+2})\)</span> giving an advantage <span class="arithmatex">\(\hat{A}^{(2)}_t = R^{(2)}_t - V(s_t)\)</span>. In general, an n-step advantage estimator can be written as:</p>
<div class="arithmatex">\[A^{t}(n) = \sum_{i=0}^{n-1} \gamma^{i} r_{t+i+1} + \gamma^{n} V(s_{t+n}) -V(s_{t})\]</div>
<p>which blends <span class="arithmatex">\(n\)</span> actual rewards with a bootstrap at time <span class="arithmatex">\(t+n\)</span>. Smaller <span class="arithmatex">\(n\)</span> (like 1) means more bias (due to heavy reliance on <span class="arithmatex">\(V\)</span>) but low variance; larger <span class="arithmatex">\(n\)</span> (approaching the episode length) reduces bias but increases variance.</p>
</li>
</ul>
<p>The pattern becomes clearer if we express these in terms of the TD error <span class="arithmatex">\(\delta_t\)</span> (the one-step advantage at <span class="arithmatex">\(t\)</span>):</p>
<p>
<script type="math/tex; mode=display"> \delta_t \;=\; r_t + \gamma V(s_{t+1}) - V(s_t). </script>
- For a 1-step return, <span class="arithmatex">\(\hat{A}^{(1)}_t = \delta_t\)</span>.
- For a 2-step return, <span class="arithmatex">\(\hat{A}^{(2)}t = \delta_t + \gamma\,\delta\)</span>.
- For an <span class="arithmatex">\(n\)</span>-step return, <span class="arithmatex">\(\hat{A}^{(n)}t = \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{n-1}\delta_{t+n-1}\)</span></p>
<p>Each additional term <span class="arithmatex">\(\gamma^i \delta_{t+i}\)</span> extends the return by one more step of real reward before bootstrapping, increasing bias a bit (since it assumes the later <span class="arithmatex">\(\delta\)</span> terms are based on an approximate <span class="arithmatex">\(V\)</span>) but capturing more actual reward outcomes (reducing variance less).</p>
<p>Generalized Advantage Estimation (GAE) takes this idea to its logical conclusion by forming a weighted sum of all n-step advantages, with exponentially decreasing weights. Instead of picking a fixed <span class="arithmatex">\(n\)</span>, GAE uses a parameter <span class="arithmatex">\(0 \le \lambda \le 1\)</span> to blend advantages of different lengths:</p>
<div class="arithmatex">\[\hat{A}^{\text{GAE}(\gamma,\lambda)}_t \;=\; (1-\lambda)\Big(\hat{A}^{(1)}_t + \lambda\,\hat{A}^{(2)}_t + \lambda^2\,\hat{A}^{(3)}_t + \cdots\Big)\]</div>
<p>This infinite series can be shown to simplify to a very convenient form:</p>
<div class="arithmatex">\[\hat{A}_t^{\text{GAE}(\gamma,\lambda)} = \sum_{i=0}^{\infty} (\gamma \lambda)^i\delta_{t+i}\]</div>
<p>which is an exponentially-weighted sum of the future TD errors. In practice, this is implemented with a simple recursion running backward through each trajectory (since it’s a sum of discounted TD errors).</p>
<p>Key intuition: <span class="arithmatex">\(\lambda\)</span> controls the bias–variance trade-off in advantage estimation:</p>
<ul>
<li>
<p><span class="arithmatex">\(\lambda = 0\)</span> uses only the one-step TD error: <span class="arithmatex">\(\hat{A}^{\text{GAE}(0)}_t = \delta_t\)</span>. This is the lowest-variance, highest-bias estimator (similar to TD(0) advantage)[21].</p>
</li>
<li>
<p><span class="arithmatex">\(\lambda = 1\)</span> uses an infinitely long sum of un-discounted TD errors, which in theory equals the full Monte Carlo return advantage (since all bootstrapping is deferred to the end). This is unbiased (in the limit of exact <span class="arithmatex">\(V\)</span>) but highest variance – essentially Monte Carlo estimation.</p>
</li>
<li>
<p>Intermediate <span class="arithmatex">\(0&lt;\lambda&lt;1\)</span> gives a mixture. A typical choice is <span class="arithmatex">\(\lambda = 0.95\)</span> in many applications, which provides a good balance (mostly long-horizon returns with a bit of bootstrapping to damp variance).</p>
</li>
</ul>
<p>GAE is not introducing a new kind of return; rather, it generalizes existing returns. It smoothly interpolates between TD and Monte Carlo methods. When <span class="arithmatex">\(\lambda\)</span> is low, GAE trusts the critic more (using more bootstrapped estimates); when <span class="arithmatex">\(\lambda\)</span> is high, GAE leans toward actual returns over many steps.
In modern actor–critic algorithms (including TRPO and PPO), GAE is used to compute the advantage for each state-action in a batch. A typical implementation for each iteration is:</p>
<ol>
<li>Collect trajectories using the current policy <span class="arithmatex">\(\pi_{\theta}\)</span> (e.g. run <span class="arithmatex">\(N\)</span> episodes or <span class="arithmatex">\(T\)</span> time steps of experience).</li>
<li>Compute state values <span class="arithmatex">\(V(s_t)\)</span> for each state visited (using the current value function estimate).</li>
<li>Compute TD residuals <span class="arithmatex">\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span> for each time step.</li>
<li>
<p>Apply GAE formula: going from <span class="arithmatex">\(t=T-1\)</span> down to <span class="arithmatex">\(0\)</span>, accumulate <span class="arithmatex">\(\hat{A}_t = \delta_t + \gamma \lambda, \hat{A}{t+1}\)</span>, with <span class="arithmatex">\(\hat{A}_{T} = 0\)</span>. This yields <span class="arithmatex">\(\hat{A}_t \approx \sum{i\ge0} (\gamma \lambda)^i \delta{t+i}\)</span>.</p>
</li>
<li>
<p>Use Advantages for Update: These <span class="arithmatex">\(\hat{A}_t\)</span> values serve as the advantage estimates in the policy gradient update. Simultaneously, you can compute proxy returns for the critic by adding <span class="arithmatex">\(\hat{A}_t\)</span> to the baseline <span class="arithmatex">\(V(s_t)\)</span> (i.e. <span class="arithmatex">\(\hat{R}_t = \hat{A}_t + V(s_t)\)</span>, an estimate of the actual return) and use those to update the value function parameters.</p>
</li>
</ol>
<p>The result of GAE is a much smoother, lower-variance advantage signal for the actor, without introducing too much bias. Empirically, this greatly stabilizes training: the policy doesn’t overreact to single high-return episodes, and it doesn’t ignore long-term outcomes either. GAE essentially bridges the gap between the high-variance Monte Carlo world of Chapter 5 and the low-variance TD world of Chapter 3–4, and it has become a standard component in virtually all modern policy optimization algorithms.</p>
<h2 id="reinforcement-7_gae-kl-divergence-constraints-and-surrogate-objectives">KL Divergence Constraints and Surrogate Objectives<a class="headerlink" href="#reinforcement-7_gae-kl-divergence-constraints-and-surrogate-objectives" title="Permanent link">¶</a></h2>
<p>We now turn to the question of stable policy updates. As discussed, a major issue with vanilla policy gradient is that a single update can accidentally push the policy into a disastrous region (because the gradient is computed at the current policy but we might step too far). To make updates safer, we want to constrain how much the policy changes at each step. A natural way to measure change between the old policy <span class="arithmatex">\(\pi_{\text{old}}\)</span> and a new policy <span class="arithmatex">\(\pi_{\text{new}}\)</span> is to use the Kullback–Leibler (KL) divergence. For example, we can require:</p>
<div class="arithmatex">\[\mathbb{E}_{s \sim d^{\pi_{\text{old}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\text{new}}(\cdot \mid s)\,\|\,\pi_{\text{old}}(\cdot \mid s)\bigr)
\right]
\le \delta\]</div>
<p>for some small <span class="arithmatex">\(\delta\)</span>. This means that on average over states (under the old policy’s state distribution <span class="arithmatex">\(d_{\pi_{\text{old}}}\)</span>), the new policy’s probability distribution is not too far from the old policy’s distribution. A small KL divergence ensures the policies behave similarly, limiting the “surprise” from one update.</p>
<p>But how do we optimize under such a constraint? We need an objective function that tells us whether <span class="arithmatex">\(\pi_{\text{new}}\)</span> is better than <span class="arithmatex">\(\pi_{\text{old}}\)</span>. Fortunately, theory provides a useful tool: a surrogate objective that approximates the change in performance if the policy change is small. One version, derived from the policy performance difference lemma and monotonic improvement theorem, is:</p>
<div class="arithmatex">\[L_{\pi_{\text{old}}}(\pi_{\text{new}})
=
\mathbb{E}_{s,a \sim \pi_{\text{old}}}
\left[
\frac{\pi_{\text{new}}(a \mid s)}{\pi_{\text{old}}(a \mid s)}
A_{\pi_{\text{old}}}(s,a)
\right]\]</div>
<p>This is an objective functional—it evaluates the new policy using samples from the old policy, weighting rewards by the importance ratio <span class="arithmatex">\(r(s,a) = \pi_{\text{new}}(a|s)/\pi_{\text{old}}(a|s)\)</span>. Intuitively, <span class="arithmatex">\(L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span> is asking: if the old policy visited state <span class="arithmatex">\(s\)</span> and took action <span class="arithmatex">\(a\)</span>, how good would that decision be under the new policy’s probabilities? Actions that the new policy wants to do more of (<span class="arithmatex">\(r &gt; 1\)</span>) will contribute their advantage (good or bad) proportionally more.</p>
<p>Critically, one can show that if <span class="arithmatex">\(\pi_{\text{new}}\)</span> is very close to <span class="arithmatex">\(\pi_{\text{old}}\)</span> (in KL terms), then improving this surrogate <span class="arithmatex">\(L\)</span> guarantees an improvement in the true return <span class="arithmatex">\(J(\pi)\)</span>. Specifically, there is a bound such that:</p>
<div class="arithmatex">\[J(\pi_{\text{new}})
\ge
J(\pi_{\text{old}})
+
L_{\pi_{\text{old}}}(\pi_{\text{new}})
-
C \,
\mathbb{E}_{s \sim d^{\pi_{\text{old}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\text{new}} \,\|\, \pi_{\text{old}}\bigr)[s]
\right]\]</div>
<p>for some constant <span class="arithmatex">\(C\)</span> related to horizon and policy support. When the KL divergence is small, the last term is second-order (negligible), so roughly we get <span class="arithmatex">\(J(\pi_{\text{new}}) \gtrapprox J(\pi_{\text{old}}) + L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span>. In other words, maximizing <span class="arithmatex">\(L\)</span> while keeping KL small ensures monotonic improvement: each update should not reduce true performance.</p>
<p>This insight leads directly to a constrained optimization formulation for safe policy updates:</p>
<ul>
<li>Objective: Maximize the surrogate <span class="arithmatex">\(L_{\pi_{\text{old}}}(\pi_{\text{new}})\)</span> (i.e. maximize expected advantage-weighted probability ratios).</li>
<li>Constraint: Limit the policy divergence via <span class="arithmatex">\(D_{\mathrm{KL}}(\pi_{\text{new}}\Vert \pi_{\text{old}}) \le \delta\)</span> (for some small <span class="arithmatex">\(\delta\)</span>).
Algorithms that implement this idea are called trust-region methods, because they optimize the policy within a trust region of the old policy. Next, we discuss two prominent algorithms: TRPO, which tackles the constrained problem directly (with some approximations), and PPO, which simplifies it into an easier unconstrained loss function.</li>
</ul>
<h2 id="reinforcement-7_gae-trust-region-policy-optimization">Trust Region Policy Optimization<a class="headerlink" href="#reinforcement-7_gae-trust-region-policy-optimization" title="Permanent link">¶</a></h2>
<p>Trust Region Policy Optimization (TRPO) is a seminal algorithm that explicitly embodies the constrained update approach. TRPO chooses a new policy by approximately solving:</p>
<div class="arithmatex">\[\max_{\theta_{\text{new}}} \; L_{\theta_{\text{old}}}(\theta_{\text{new}})
\quad \text{s.t.} \quad
\mathbb{E}_{s \sim d^{\pi_{\theta_{\text{old}}}}}
\left[
D_{\mathrm{KL}}\bigl(\pi_{\theta_{\text{new}}} \,\|\, \pi_{\theta_{\text{old}}}\bigr)
\right]
\le \delta\]</div>
<p>where <span class="arithmatex">\(L_{\theta_{\text{old}}}(\theta_{\text{new}})\)</span> is the surrogate objective defined above, and <span class="arithmatex">\(\delta\)</span> is a small trust-region threshold. In practice, solving this exactly is difficult due to the infinite-dimensional policy space. TRPO makes it tractable by using a few key ideas:</p>
<ul>
<li>
<p>Approximating the constraint via a quadratic expansion of the KL divergence (which yields a Fisher Information Matrix). This turns the problem into something like a second-order update (a natural gradient step). In fact, TRPO’s solution can be shown to correspond to a natural gradient ascent:</p>
<p><span class="arithmatex">\(\theta_{\text{new}} = \theta_{\text{old}} + \sqrt{\frac{2\delta}{g^T F^{-1} g}}\; F^{-1} g\)</span>$</p>
<p>where <span class="arithmatex">\(g = \nabla_\theta L\)</span> and <span class="arithmatex">\(F\)</span> is the Fisher matrix. This ensures the KL constraint is satisfied approximately, and is equivalent to scaling the gradient by <span class="arithmatex">\(F^{-1}\)</span>. In simpler terms, TRPO updates <span class="arithmatex">\(\theta\)</span> in a direction that accounts for the curvature of the policy space, so that the change in policy (KL) is proportional to the step size.</p>
</li>
<li>
<p>Using a line search to ensure the new policy actually improves <span class="arithmatex">\(J(\pi)\)</span>. TRPO will back off the step size if the updated policy violates the constraint or fails to achieve a performance improvement. This safeguard maintains the monotonic improvement guarantee in practice.</p>
</li>
</ul>
<p>A simplified outline of TRPO is:</p>
<ol>
<li>Collect trajectories with the current policy <span class="arithmatex">\(\pi_{\theta_{\text{old}}}\)</span>.</li>
<li>Estimate advantages <span class="arithmatex">\(\hat{A}_t\)</span> for each time step (using GAE or another method for high-quality advantage estimates).</li>
<li>Compute surrogate objective <span class="arithmatex">\(L(\theta) = \mathbb{E}[r_t(\theta), \hat{A}t]\)</span> where <span class="arithmatex">\(r_t(\theta) = \frac{\pi{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span>.</li>
<li>Approximate KL constraint: Compute the policy gradient <span class="arithmatex">\(\nabla_\theta L\)</span> and the Fisher matrix <span class="arithmatex">\(F\)</span> (via sample-based estimation of the Hessian of the KL divergence). Solve for the update direction <span class="arithmatex">\(p \approx F^{-1} \nabla_\theta L\)</span> (e.g. using conjugate gradient).</li>
<li>Line search: Scale and apply the update step <span class="arithmatex">\(\theta \leftarrow \theta + p\)</span> gradually, checking the KL and improvement. Stop when the KL constraint or improvement criterion is satisfied.</li>
</ol>
<p>TRPO’s updates are therefore conservative by design – they will only take as large a step as can be trusted not to degrade performance. TRPO was influential because it demonstrated much more stable and reliable training on complex continuous control tasks than vanilla policy gradient.</p>
<p>Strengths and Weaknesses of TRPO: TRPO offers a theoretical guarantee of non-destructive updates – under certain assumptions, each iteration is guaranteed to improve or at least not decrease performance. It uses a natural gradient approach that respects the geometry of policy space, which is more effective than an arbitrary gradient in parameter space. However, TRPO comes at a cost: it requires calculating second-order information (the Fisher matrix), and implementing the conjugate gradient solver and line search adds complexity. The algorithm can be slower per iteration and is more complex to code and tune. In practice, TRPO, while effective, proved somewhat cumbersome for large-scale problems due to these complexities.</p>
<h2 id="reinforcement-7_gae-proximal-policy-optimization">Proximal Policy Optimization<a class="headerlink" href="#reinforcement-7_gae-proximal-policy-optimization" title="Permanent link">¶</a></h2>
<p>Proximal Policy Optimization (PPO) was introduced as a simpler, more user-friendly variant of TRPO that achieves similar results with only first-order optimization. The core idea of PPO is to keep the spirit of trust-region updates (don’t move the policy too far in one go) but implement it via a relaxed objective that can be optimized with standard stochastic gradient descent.
There are two main variants of PPO:</p>
<h3 id="reinforcement-7_gae-kl-penalty-objective">KL-Penalty Objective:<a class="headerlink" href="#reinforcement-7_gae-kl-penalty-objective" title="Permanent link">¶</a></h3>
<p>One version of PPO adds the KL-divergence as a penalty to the objective rather than a hard constraint. The objective becomes:</p>
<div class="arithmatex">\[J_{\text{PPO-KL}}(\theta)
=
\mathbb{E}\!\left[ r_t(\theta)\, \hat{A}^t \right]
-
\beta \,\mathbb{E}\!\left[
D_{\mathrm{KL}}\!\left(\pi_{\theta} \,\|\, \pi_{\theta_{\text{old}}}\right)
\right]\]</div>
<p>where <span class="arithmatex">\(\beta\)</span> is a coefficient determining how strongly to penalize deviation from the old policy. If the KL divergence in an update becomes too large, <span class="arithmatex">\(\beta\)</span> can be adjusted (increased) to enforce smaller steps in subsequent updates. This approach maintains a soft notion of a trust region.</p>
<h4 id="reinforcement-7_gae-algorithm-ppo-with-kl-penalty">Algorithm (PPO with KL Penalty)<a class="headerlink" href="#reinforcement-7_gae-algorithm-ppo-with-kl-penalty" title="Permanent link">¶</a></h4>
<p>1: Input: initial policy parameters <span class="arithmatex">\(\theta_0\)</span>, initial KL penalty <span class="arithmatex">\(\beta_0\)</span>, target KL-divergence <span class="arithmatex">\(\delta\)</span><br>
2: for <span class="arithmatex">\(k = 0, 1, 2, \ldots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect set of partial trajectories <span class="arithmatex">\(\mathcal{D}_k\)</span> using policy <span class="arithmatex">\(\pi_k = \pi(\theta_k)\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Estimate advantages <span class="arithmatex">\(\hat{A}^t_k\)</span> using any advantage estimation algorithm<br>
5: <span class="arithmatex">\(\quad\)</span> Compute policy update by approximately solving<br>
<span class="arithmatex">\(\quad\quad\)</span> <span class="arithmatex">\(\theta_{k+1} = \arg\max_\theta \; L_{\theta_k}(\theta) - \beta_k \hat{D}_{KL}(\theta \,\|\, \theta_k)\)</span><br>
6: <span class="arithmatex">\(\quad\)</span> Implement this optimization with <span class="arithmatex">\(K\)</span> steps of minibatch SGD (e.g., Adam)<br>
7: <span class="arithmatex">\(\quad\)</span> Measure actual KL: <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k)\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \ge 1.5\delta\)</span> then<br>
9: <span class="arithmatex">\(\quad\quad\)</span> Increase penalty: <span class="arithmatex">\(\beta_{k+1} = 2\beta_k\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> else if <span class="arithmatex">\(\hat{D}_{KL}(\theta_{k+1}\|\theta_k) \le \delta/1.5\)</span> then<br>
11: <span class="arithmatex">\(\quad\quad\)</span> Decrease penalty: <span class="arithmatex">\(\beta_{k+1} = \beta_k/2\)</span><br>
12: <span class="arithmatex">\(\quad\)</span> end if<br>
13: end for  </p>
<h3 id="reinforcement-7_gae-clipped-surrogate-objective-ppo-clip">Clipped Surrogate Objective (PPO-Clip):<a class="headerlink" href="#reinforcement-7_gae-clipped-surrogate-objective-ppo-clip" title="Permanent link">¶</a></h3>
<p>The more popular variant of PPO uses a clipped surrogate objective to restrict policy updates:</p>
<div class="arithmatex">\[L^\text{CLIP}(\theta)
=
\mathbb{E}_{t}\!\left[
\min\!\Big(
r_t(\theta)\,\hat{A}^t,\;
\text{clip}\!\big(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,\hat{A}^t
\Big)
\right]\]</div>
<p>where <span class="arithmatex">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span> as before, and <span class="arithmatex">\(\epsilon\)</span> is a small hyperparameter (e.g. 0.1 or 0.2) that defines the clipping range. This objective says: if the new policy’s probability ratio <span class="arithmatex">\(r_t(\theta)\)</span> stays within <span class="arithmatex">\([1-\epsilon,\,1+\epsilon]\)</span>, we use the normal surrogate <span class="arithmatex">\(r_t \hat{A}_t\)</span>. But if <span class="arithmatex">\(r_t\)</span> tries to go outside this range (meaning the policy probability for an action has changed dramatically), we clip <span class="arithmatex">\(r_t\)</span> to either <span class="arithmatex">\(1+\epsilon\)</span> or <span class="arithmatex">\(1-\epsilon\)</span> before multiplying by <span class="arithmatex">\(\hat{A}_t\)</span>. Effectively, the advantage contribution is capped once the policy deviates too much from the old policy.</p>
<p>The clipped objective is not exactly the original constrained problem, but it serves a similar purpose: it removes the incentive for the optimizer to push <span class="arithmatex">\(r_t\)</span> outside of <span class="arithmatex">\([1-\epsilon,1+\epsilon]\)</span>. If increasing <span class="arithmatex">\(|\theta|\)</span> further doesn’t increase the objective (because the min() will select the clipped term), then overly large policy changes are discouraged.</p>
<p>Why Clipping Works: Clipping is a simple heuristic, but it has proven extremely effective:</p>
<ul>
<li>
<p>It enforces a soft trust region by preventing extreme updates for any single state-action probability. The policy can still change, but not so much that any one probability ratio blows up.</p>
</li>
<li>
<p>It avoids the complexity of solving a constrained optimization or computing second-order derivatives – we can just do standard SGD on <span class="arithmatex">\(L^{CLIP}(\theta)\)</span>.</p>
</li>
<li>
<p>It keeps importance sampling ratios near 1, which means the algorithm can safely perform multiple epochs of updates on the same batch of data without the estimates drifting too far. This directly improves sample efficiency (unlike vanilla policy gradient, PPO typically updates each batch for several epochs).</p>
</li>
</ul>
<h4 id="reinforcement-7_gae-ppo-clipped-algorithm">PPO (Clipped) Algorithm<a class="headerlink" href="#reinforcement-7_gae-ppo-clipped-algorithm" title="Permanent link">¶</a></h4>
<p>1: Input: initial policy parameters <span class="arithmatex">\(\theta_0\)</span>, clipping threshold <span class="arithmatex">\(\epsilon\)</span><br>
2: for <span class="arithmatex">\(k = 0, 1, 2, \ldots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Collect a set of partial trajectories <span class="arithmatex">\(\mathcal{D}_k\)</span> using policy <span class="arithmatex">\(\pi_k = \pi(\theta_k)\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Estimate advantages <span class="arithmatex">\(\hat{A}^{\,t}_k\)</span> using any advantage estimation algorithm (e.g., GAE)<br>
5: <span class="arithmatex">\(\quad\)</span> Define the clipped surrogate objective<br>
<span class="arithmatex">\(\quad\quad\)</span><br>
<script type="math/tex; mode=display">
\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)
= 
\mathbb{E}_{\tau \sim \pi_{\theta_k}}
\left[
\sum_{t=0}^{T}
\min\!\left(
r_t(\theta)\,\hat{A}^t_k,\;
\operatorname{clip}\!\left(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\right)\hat{A}^t_k
\right)
\right]
</script>
6: <span class="arithmatex">\(\quad\)</span> Update policy parameters with several epochs of minibatch SGD to approximately maximize <span class="arithmatex">\(\mathcal{L}^{\text{CLIP}}_{\theta_k}(\theta)\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> Set <span class="arithmatex">\(\theta_{k+1}\)</span> to the resulting parameters<br>
8: end for  </p>
<p>In practice, PPO with clipping has become one of the most widely used RL algorithms because it strikes a good balance between performance and simplicity. It is relatively easy to implement (compared to TRPO) and has been found to be robust across many tasks and hyperparameters. While it doesn’t guarantee monotonic improvement in theory, in practice it achieves stable training behavior very similar to TRPO.</p>
<p>In modern practice, PPO is the dominant choice for policy optimization in deep RL, due to its relative simplicity and strong performance across many environments. TRPO is still important conceptually (and sometimes used in scenarios where theoretical guarantees are desired), but PPO’s convenience usually wins out.</p>
<h2 id="reinforcement-7_gae-putting-it-together-sample-efficiency-stability-and-monotonic-improvement">Putting It Together: Sample Efficiency, Stability, and Monotonic Improvement<a class="headerlink" href="#reinforcement-7_gae-putting-it-together-sample-efficiency-stability-and-monotonic-improvement" title="Permanent link">¶</a></h2>
<p>The advances covered in this chapter are often used together in state-of-the-art algorithms:</p>
<ul>
<li>
<p>Generalized Advantage Estimation (GAE) provides high-quality advantage estimates that significantly reduce variance without too much bias. This means we can get away with smaller batch sizes or fewer episodes to get a good learning signal – improving sample efficiency.</p>
</li>
<li>
<p>Trust-region update rules (TRPO/PPO) ensure that each policy update is safe and stable – the policy doesn’t change erratically, preventing the kind of catastrophic drops in reward that naive policy gradients can suffer. By keeping policy changes small (via KL constraints or clipping), these methods enable multiple updates on the same batch of data (improving data efficiency) and maintain policy monotonicity, i.e. each update is expected to improve or at least not significantly degrade performance.</p>
</li>
<li>
<p>In practice, an algorithm like PPO with GAE is an actor–critic method that uses all these ideas: an actor policy updated with a clipped surrogate objective (making updates stable), a critic to approximate <span class="arithmatex">\(V(s)\)</span> (enabling advantage estimation), GAE to compute advantages (trading off bias/variance), and typically multiple gradient epochs per batch to squeeze more learning out of each sample. This combination has proven remarkably successful in domains from simulated control tasks to games.</p>
</li>
</ul>
<p>By building on the foundational policy gradient framework and addressing its shortcomings, GAE and trust-region approaches have made deep reinforcement learning much more practical and reliable. They illustrate how theoretical insights (performance bounds, policy geometry) and practical tricks (advantage normalization, clipping) come together to yield algorithms that can solve challenging RL problems while using reasonable amounts of training data and maintaining stability throughout learning. Each component – be it advantage estimation or constrained updates – plays a role in ensuring that learning is as efficient, stable, and monotonic as possible. Together, they represent the state-of-the-art toolkit for policy optimization in reinforcement learning.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Key Idea</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>REINFORCE</td>
<td>MC return-based policy gradient</td>
<td>Simple, unbiased</td>
<td>Very high variance</td>
</tr>
<tr>
<td>Actor–Critic</td>
<td>TD baseline value function</td>
<td>More sample-efficient</td>
<td>Requires critic</td>
</tr>
<tr>
<td>Advantage Actor–Critic</td>
<td>Uses <span class="arithmatex">\(A(s,a)\)</span> for updates</td>
<td>Best bias–variance trade</td>
<td>Needs accurate value est.</td>
</tr>
<tr>
<td>TRPO</td>
<td>Trust-region with KL constraint</td>
<td>Strong theory, stable</td>
<td>Complex, second-order</td>
</tr>
<tr>
<td>PPO</td>
<td>Clipped/penalized surrogate objective</td>
<td>Simple, stable, popular</td>
<td>Heuristic, tuning needed</td>
</tr>
</tbody>
</table>
<h2 id="reinforcement-7_gae-mental-map">Mental Map<a class="headerlink" href="#reinforcement-7_gae-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                  Advanced Policy Gradient Methods
     Goal: Fix limitations of vanilla PG (variance, stability, KL control)
                               │
                               ▼
             Core Challenges in Policy Gradient Methods
       ┌────────────────────────────────────────────────────────┐
       │ High variance (MC returns)                             │
       │ Poor sample efficiency (on-policy only)                │
       │ Sensitive to step size → catastrophic policy collapse  │
       │ Small θ change ≠ small policy change                   │
       │ Reusing old data is unstable                           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Variance Reduction (Baselines)
       ┌────────────────────────────────────────────────────────┐
       │ Introduce baseline b(s) → subtract expectation         │
       │ Keeps estimator unbiased                               │
       │ Good choice: b(s)= V(s) → yields Advantage A(s,a)      │
       │ Update based on: how much action outperformed expected │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                       Advantage Function A(s,a)
       ┌────────────────────────────────────────────────────────┐
       │ A(s,a) = Q(s,a) – V(s)                                 │
       │ Measures how much BETTER the action was vs average     │
       │ Positive → increase πθ(a|s); Negative → decrease it    │
       │ Major variance reduction – foundation of Actor–Critic  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                         Actor–Critic Framework
       ┌────────────────────────────────────────────────────────┐
       │ Actor: policy πθ(a|s)                                  │
       │ Critic: value function V(s;w) estimates baseline       │
       │ TD error δt reduces variance (bootstrapping)           │
       │ Faster, more sample-efficient than REINFORCE           │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                     Target Estimation for the Critic
       ┌────────────────────────────┬────────────────────────────┐
       │ Monte Carlo (∞-step)       │  TD (1-step)               │
       │ + Unbiased                 │  + Low variance            │
       │ – High variance            │  – Biased                  │
       ├────────────────────────────┴────────────────────────────┤
       │ n-Step Returns: Blend of TD and MC                      │
       │ Control bias–variance by choosing n                     │
       │ Larger n → MC-like; smaller n → TD-like                 │
       └─────────────────────────────────────────────────────────┘
                               │
                               ▼
             Fundamental Problems with Vanilla Policy Gradient
       ┌────────────────────────────────────────────────────────┐
       │ Uses each batch for ONE gradient step (on-policy)      │
       │ Step size is unstable → huge performance collapse      │
       │ Small changes in θ → large unintended policy changes   │
       │ Need mechanism to limit POLICY CHANGE, not θ change    │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
            Safe Policy Improvement Theory → TRPO &amp; PPO
       ┌────────────────────────────────────────────────────────┐
       │ Policy Performance Difference Lemma                    │
       │   J(π') − J(π) = Eπ' [Aπ(s,a)]                         │
       │ KL Divergence as policy distance metric                │
       │   D_KL(π'||π) small → safe update                      │
       │ Monotonic Improvement Bound                            │
       │   Lower bound on J(π') using surrogate loss Lπ(π')     │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                   Surrogate Objective for Safe Updates
       ┌────────────────────────────────────────────────────────┐
       │ Lπ(π') = E[ (π'(a|s)/π(a|s)) * Aπ(s,a) ]               │
       │ Importance sampling + KL regularization                │
       │ Foundation of Trust-Region Policy Optimization (TRPO)  │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
              Proximal Policy Optimization (PPO) – Key Ideas
       ┌────────────────────────────┬────────────────────────────┐
       │ PPO-KL Penalty             │ PPO-Clipped Objective      │
       │ Adds β·KL to loss          │ Clips ratio r_t(θ) to      │
       │ Adjust β adaptively        │ [1−ε, 1+ε] to prevent      │
       │ Prevents large updates     │ destructive policy jumps   │
       └────────────────────────────┴────────────────────────────┘
                               │
                               ▼
                         PPO Algorithm Summary
       ┌────────────────────────────────────────────────────────┐
       │ 1. Collect trajectories from old policy                │
       │ 2. Estimate advantages Â_t (GAE, TD, etc.)            │
       │ 3. Optimize clipped surrogate for many epochs          │
       │ 4. Update parameters safely                            │
       └────────────────────────────────────────────────────────┘
                               │
                               ▼
                          Final Outcome (Chapter 6)
       ┌────────────────────────────────────────────────────────┐
       │ Stable and efficient policy optimization               │
       │ Reuse data safely across multiple updates              │
       │ Avoid catastrophic policy collapse                     │
       │ Foundation of modern deep RL algorithms                │
       │ (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    │
       └────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-8_imitation_learning" heading-number="2.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-8-imitation-learning">Chapter 8: Imitation Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-chapter-8-imitation-learning" title="Permanent link">¶</a></h1>
<p>In previous chapters, we focused on reinforcement learning with explicit reward signals guiding the agent's behavior. We assumed that a well-defined reward function <span class="arithmatex">\(R(s,a)\)</span> was provided as part of the MDP, and the agent’s goal was to learn a policy that maximizes cumulative reward. But what if specifying the reward is difficult or the agent cannot safely explore to learn from reward? Imitation Learning (IL) addresses these scenarios by leveraging expert demonstrations instead of explicit rewards.</p>
<p>Imitation Learning allows an agent to learn how to act by mimicking an expert’s behavior, rather than by maximizing a hand-crafted reward.</p>
<h2 id="reinforcement-8_imitation_learning-motivation-the-case-for-learning-from-demonstrations">Motivation: The Case for Learning from Demonstrations<a class="headerlink" href="#reinforcement-8_imitation_learning-motivation-the-case-for-learning-from-demonstrations" title="Permanent link">¶</a></h2>
<p>Designing a reward function that truly captures the desired behavior can be extremely challenging. A misspecified reward can lead to unintended behaviors (reward hacking) or require exhaustive tuning. Even with a good reward, some environments present sparse rewards (e.g. only a success/failure signal at the very end of an episode) – making pure trial-and-error learning inefficient. In other cases, unsafe exploration is a concern: letting an agent freely explore (as classic RL would) could be dangerous or costly (imagine a self-driving car learning by crashing to discover that crashing is bad).</p>
<p>However, in many of these settings expert behavior is available: we might have logs of human drivers driving safely, or demonstrations of a robot performing the task. Imitation Learning leverages this data. Instead of specifying what to do via a reward function, we show the agent how to do it via example trajectories. The agent's objective is then to imitate the expert as closely as possible.</p>
<p>This paradigm contrasts with reward-based RL in key ways:</p>
<ul>
<li>
<p>Reward-Based RL: The agent explores and learns by trial-and-error, guided by a numeric reward signal for feedback. It requires careful reward design and often extensive exploration.</p>
</li>
<li>
<p>Imitation Learning: The agent learns from demonstrations of the desired behavior, treating the expert’s actions as ground truth. No explicit reward is needed to train; learning is driven by matching the expert's behavior.</p>
</li>
</ul>
<p>By learning from an expert, IL can produce competent policies much faster and safer in these scenarios. It essentially sidesteps the credit assignment problem of RL (because the "right" action is directly provided by the expert) and avoids dangerous exploration. In domains like autonomous driving, robotics, or any task where a human can demonstrate the skill, IL offers a powerful shortcut to get an agent up to a reasonable performance.</p>
<h2 id="reinforcement-8_imitation_learning-imitation-learning-problem-setup">Imitation Learning Problem Setup<a class="headerlink" href="#reinforcement-8_imitation_learning-imitation-learning-problem-setup" title="Permanent link">¶</a></h2>
<p>Formally, we can describe the imitation learning scenario using the same environment structure as an MDP <span class="arithmatex">\((S, A, P, R, \gamma)\)</span> except that the reward function <span class="arithmatex">\(R\)</span> is unknown or not used. The agent still has a state space <span class="arithmatex">\(S\)</span>, an action space <span class="arithmatex">\(A\)</span>, and the environment transition dynamics <span class="arithmatex">\(P(s' \mid s, a)\)</span>. What we do have, instead of <span class="arithmatex">\(R\)</span>, is access to expert demonstrations. An expert (which could be a human or a pre-trained optimal agent) provides example trajectories:</p>
<div class="arithmatex">\[
\tau_E = (s_0, a_0, s_1, a_1, \dots , s_T)
\]</div>
<p>collected by following the expert’s policy <span class="arithmatex">\(\pi_E\)</span> in the environment. We may have a dataset <span class="arithmatex">\(D\)</span> of these expert trajectories (or simply a set of state-action pairs drawn from expert behavior). The key point is that in IL, the agent does not receive numeric rewards from the environment. Instead, success is measured by how well the agent’s behavior matches the expert’s behavior.</p>
<p>The goal of imitation learning can be stated as: find a policy <span class="arithmatex">\(\pi\)</span> for the agent that reproduces the expert's behavior (and ideally, achieves similar performance on the task). If the expert is optimal or highly skilled, we hope <span class="arithmatex">\(\pi\)</span> will achieve near-optimal results as well. This is an alternative path to finding a good policy without ever specifying a reward function explicitly or performing unguided exploration.</p>
<p>(If we imagine there was some true but unknown reward <span class="arithmatex">\(R\)</span> the expert is optimizing, then ideally <span class="arithmatex">\(\pi\)</span> should perform nearly as well as <span class="arithmatex">\(\pi_E\)</span> on that reward. IL attempts to reach that outcome via demonstrations rather than explicit reward feedback.)</p>
<h2 id="reinforcement-8_imitation_learning-3-behavioral-cloning-learning-by-supervised-imitation">3. Behavioral Cloning: Learning by Supervised Imitation<a class="headerlink" href="#reinforcement-8_imitation_learning-3-behavioral-cloning-learning-by-supervised-imitation" title="Permanent link">¶</a></h2>
<p>The most direct approach to imitation learning is Behavioral Cloning. Behavioral cloning treats imitation as a pure supervised learning problem: we train a policy to map states to the expert’s actions, using the expert demonstrations as labeled examples. In essence, the agent "clones" the expert's behavior by learning to predict the expert's action in any given state.</p>
<blockquote>
<p>BC: Learn state to action mappings using expert demonstrations.</p>
</blockquote>
<p>In practice, we parameterize a policy <span class="arithmatex">\(\pi_\theta(a\mid s)\)</span> (e.g. a neural network with parameters <span class="arithmatex">\(\theta\)</span>) and adjust <span class="arithmatex">\(\theta\)</span> so that <span class="arithmatex">\(\pi_\theta(\cdot\mid s)\)</span> is as close as possible to the expert’s action choice in state <span class="arithmatex">\(s\)</span>. We define a loss function on the dataset of state-action pairs. For example:</p>
<ul>
<li>Discrete actions: Use cross-entropy (negative log-likelihood) of the expert’s action.</li>
</ul>
<div class="arithmatex">\[L(\theta) = - \mathbb{E}_{(s,a)\sim D}\left[ \log \pi_{\theta}(a \mid s) \right]\]</div>
<ul>
<li>Continuous actions: Use mean squared error (regression loss).</li>
</ul>
<div class="arithmatex">\[L(\theta) = \mathbb{E}_{(s,a)\sim D} \left[ \left( \pi_\theta(s) - a \right)^2 \right]\]</div>
<p>Minimizing these losses drives the policy to imitate the expert decisions on the training set.</p>
<p>Training a behavioral cloning agent typically involves three steps:</p>
<ol>
<li>
<p>Collect demonstrations: Gather a dataset <span class="arithmatex">\(D = {(s_i, a_i)}\)</span> of expert state-action examples by observing the expert <span class="arithmatex">\(\pi_E\)</span> in the environment.</p>
</li>
<li>
<p>Supervised learning on <span class="arithmatex">\((s, a)\)</span> pairs: Choose a policy representation for <span class="arithmatex">\(\pi_\theta\)</span> and use the collected data to adjust <span class="arithmatex">\(\theta\)</span>. For each example <span class="arithmatex">\((s_i, a_i)\)</span>, we update <span class="arithmatex">\(\pi_\theta\)</span> to reduce the error between its prediction <span class="arithmatex">\(\pi_\theta(s_i)\)</span> and the expert’s action <span class="arithmatex">\(a_i\)</span>. (For instance, if actions are discrete, we increase the probability <span class="arithmatex">\(\pi_\theta(a_i \mid s_i)\)</span> for the expert’s action; if continuous, we move <span class="arithmatex">\(\pi_\theta(s_i)\)</span> closer to <span class="arithmatex">\(a_i\)</span> in value.)</p>
</li>
<li>
<p>Deployment: Once the policy is trained (approximating <span class="arithmatex">\(\pi_E\)</span>), we fix <span class="arithmatex">\(\theta\)</span>. The agent then acts autonomously: at each state <span class="arithmatex">\(s\)</span>, it outputs <span class="arithmatex">\(a = \pi_\theta(s)\)</span> as its action. Ideally, this learned policy will behave similarly to the expert in the environment.</p>
</li>
</ol>
<p>If the expert demonstrations are representative of the situations the agent will face, behavioral cloning can yield a policy that mimics the expert’s behavior effectively. BC has some clear advantages:</p>
<ul>
<li>
<p>Simplicity: It reduces policy learning to standard supervised learning, for which many stable algorithms and optimizations exist.</p>
</li>
<li>
<p>Offline training: The model can be trained entirely from pre-recorded expert data, without requiring interactive environment feedback. This makes it data-efficient in terms of environment interactions.</p>
</li>
<li>
<p>Safety: No random exploration is needed. The agent never tries highly suboptimal actions during training, since it always learns from demonstrated good behavior (critical in safety-sensitive domains).</p>
</li>
</ul>
<p>However, purely copying the expert also comes with important limitations.</p>
<h3 id="reinforcement-8_imitation_learning-covariate-shift-and-compounding-errors">Covariate Shift and Compounding Errors<a class="headerlink" href="#reinforcement-8_imitation_learning-covariate-shift-and-compounding-errors" title="Permanent link">¶</a></h3>
<p>The main problem with behavioral cloning is that the training distribution of states can differ from the test distribution when the agent actually runs. During training, <span class="arithmatex">\(\pi_\theta\)</span> is only exposed to states that the expert visited. But once the agent is deployed, if it ever deviates even slightly from the expert’s trajectory, it may enter states not seen in the training data. In those unfamiliar states, the policy’s predictions may be unreliable, leading to errors that cause it to drift further from expert-like behavior.</p>
<blockquote>
<p>A small mistake can snowball: once the agent strays from what the expert would do, it encounters novel situations where its learned policy might be very poor. One error leads to another, and the agent can cascade into failure because it was never taught how to recover.</p>
</blockquote>
<p>This phenomenon is known as covariate shift or distributional shift. The learner is trained on the state distribution induced by the expert policy <span class="arithmatex">\(\pi_E\)</span>, but it is testing on the state distribution induced by its own policy <span class="arithmatex">\(\pi_\theta\)</span>. Unless <span class="arithmatex">\(\pi_\theta\)</span> is perfect, these distributions will diverge over time, and the divergence can grow unchecked. In other words, the agent might handle situations similar to the expert's trajectories well, but if it finds itself in a situation the expert never encountered (often a result of a prior mistake), it has no guidance on what to do and can rapidly veer off course. This is often illustrated by the example of a self-driving car learned by BC: if it slightly misjudges a turn and drifts, it may end up in a part of the road it never saw during training, leading to more errors (compounding until possibly a crash).</p>
<p>Another limitation is that BC does not inherently guarantee optimality or improvement beyond the expert: the policy is only as good as the demonstration data. If the expert is suboptimal or the dataset doesn’t cover certain scenarios, the cloned policy will reflect those shortcomings and cannot improve by itself (since it has no feedback signal like reward to further refine its behavior). In reinforcement learning terms, BC has no notion of feedback for success or failure; it merely apes the expert, so it cannot discover better strategies or correct mistakes outside the expert's shadow.</p>
<p>Researchers have developed strategies to mitigate the covariate shift problem. One approach is Dataset Aggregation (DAgger), which is an iterative algorithm: after training an initial policy via BC, let the policy interact with the environment and observe where it makes mistakes or visits unseen states; then have the expert provide the correct actions for those states, add these state-action pairs to the training set, and retrain the policy. By repeating this process, the policy’s training distribution is gradually brought closer to the distribution it will encounter when it controls the agent. DAgger can significantly reduce compounding errors, but it requires ongoing access to an expert for feedback during training.</p>
<p>In summary, behavioral cloning is a powerful first step for imitation learning—it's straightforward and avoids many challenges of pure RL. But one must be mindful of its limitations: a blindly cloned policy can fail catastrophically when it encounters situations outside the expert’s experience. This motivates more sophisticated imitation learning methods that incorporate the dynamics of the environment and attempt to infer the intent behind expert actions, rather than just copying them. We turn to those next.</p>
<h2 id="reinforcement-8_imitation_learning-inverse-reinforcement-learning-learning-the-why">Inverse Reinforcement Learning: Learning the "Why"<a class="headerlink" href="#reinforcement-8_imitation_learning-inverse-reinforcement-learning-learning-the-why" title="Permanent link">¶</a></h2>
<p>Behavioral cloning directly learns what to do (mapping states to actions) but does not capture why those actions are desirable. Inverse Reinforcement Learning (IRL) instead asks: Given expert behavior, what underlying reward function <span class="arithmatex">\(R\)</span> could explain it? In other words, IRL attempts to reverse-engineer the expert's objectives from its observed behavior.</p>
<p>In IRL, we assume that the expert <span class="arithmatex">\(\pi_E\)</span> is (approximately) optimal for some unknown reward function <span class="arithmatex">\(R^*\)</span>. The goal is to infer a reward function <span class="arithmatex">\(\hat{R}\)</span> such that, if an agent were to optimize <span class="arithmatex">\(\hat{R}\)</span>, it would reproduce the expert’s behavior. Formally, we want <span class="arithmatex">\(\pi_E\)</span> to be the optimal policy under the learned reward:</p>
<div class="arithmatex">\[\pi_E = \arg\max_{\pi} \, V_R^{\pi}\]</div>
<p>where <span class="arithmatex">\(V^{\pi}_{\hat{R}}\)</span> is the expected return of policy <span class="arithmatex">\(\pi\)</span> under the reward function <span class="arithmatex">\(\hat{R}\)</span>. In words, the expert should have higher cumulative reward (according to <span class="arithmatex">\(\hat{R}\)</span>) than any other policy. If we can find such an <span class="arithmatex">\(\hat{R}\)</span>, we have explained the expert’s behavior in terms of incentives.</p>
<blockquote>
<p>Intuition: IRL flips the reinforcement learning problem on its head. Rather than starting with a reward and finding a policy, we start with a policy (the expert's) and try to find a reward that this policy optimizes. It's like observing an expert driver and deducing that they must be implicitly trading off goals like "reach the destination quickly" and "avoid collisions" because their driving balances speed and safety.</p>
</blockquote>
<p>One challenge is that IRL is inherently an under-defined (ill-posed) problem: many possible reward functions might make <span class="arithmatex">\(\pi_E\)</span> appear optimal. To resolve this ambiguity, IRL algorithms introduce additional criteria or regularization. For example, they might prefer the simplest reward function that explains the behavior, or in the case of maximum entropy IRL, prefer a reward that leads to the most random (maximally entropic) policy among those that match the expert's behavior – this avoids overly narrow explanations and spreads probability over possible behaviors unless forced by data.</p>
<p>Once a candidate reward function <span class="arithmatex">\(\hat{R}(s,a)\)</span> is learned through IRL, the process typically continues as follows: we plug <span class="arithmatex">\(\hat{R}\)</span> back into the environment and solve a forward RL problem (using any suitable algorithm from earlier chapters) to obtain a policy <span class="arithmatex">\(\pi_{\hat{R}}\)</span> that maximizes this recovered reward. Ideally, <span class="arithmatex">\(\pi_{\hat{R}}\)</span> will then behave similarly to the expert's policy <span class="arithmatex">\(\pi_E\)</span> (since <span class="arithmatex">\(\hat{R}\)</span> was chosen to explain <span class="arithmatex">\(\pi_E\)</span>). The end result is an agent that not only imitates the expert, but also has an explicit reward model of the task it is performing.</p>
<p>IRL is usually more complex and computationally expensive than behavioral cloning, because it often involves a nested loop: for each candidate reward function, the algorithm may need to perform an inner optimization (solving an MDP) to evaluate how well that reward explains the expert. However, IRL provides several potential benefits:</p>
<ul>
<li>
<p>It yields a reward function, which is a portable definition of the task. This inferred reward can then be reused: for example, to train new agents from scratch, to evaluate different policies, or to modify the task (by tweaking the reward) in a principled way.</p>
</li>
<li>
<p>It can generalize better to new situations. If the environment changes in dynamics or constraints, having <span class="arithmatex">\(\hat{R}\)</span> allows us to re-optimize and find a new optimal policy for the new conditions. A policy learned by pure BC might not adapt well beyond the situations it was shown, whereas a reward captures the goal and can be re-optimized.</p>
</li>
<li>
<p>It may allow the agent to exceed the demonstrator’s performance. Since IRL ultimately produces a reward function, an agent can continue to improve with further RL optimization. If the expert was suboptimal or noisy, a sufficiently good RL algorithm might find a policy that achieves an even higher reward (i.e. fine-tunes the behavior) while still aligning with the expert’s intent encoded in <span class="arithmatex">\(\hat{R}\)</span>.</p>
</li>
</ul>
<p>In summary, IRL shifts the imitation learning problem from policy regression to reward inference. It answers a fundamentally different question: instead of directly cloning actions, infer the hidden goals that the expert is pursuing. With <span class="arithmatex">\(\hat{R}\)</span> in hand, we then fall back on standard RL techniques (like those from Chapters 4–8) to derive a policy. IRL is especially appealing in scenarios where we suspect the expert’s behavior is optimizing some elegant underlying objective, and we want to uncover that objective for reuse or interpretation. The cost of IRL is the added complexity of the learning process, but the payoff is a deeper understanding of the task and potentially greater robustness and optimality of the learned policy.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-inverse-reinforcement-learning">Maximum Entropy Inverse Reinforcement Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-inverse-reinforcement-learning" title="Permanent link">¶</a></h3>
<h3 id="reinforcement-8_imitation_learning-principle-of-maximum-entropy">Principle of Maximum Entropy<a class="headerlink" href="#reinforcement-8_imitation_learning-principle-of-maximum-entropy" title="Permanent link">¶</a></h3>
<p>The entropy of a distribution <span class="arithmatex">\(p(s)\)</span> is defined as:</p>
<div class="arithmatex">\[H(p) = -\sum_{s} p(s)\log p(s)\]</div>
<p>The principle of maximum entropy states: The probability distribution that best represents our state of knowledge is the one with the largest entropy, given the constraints of precisely stated prior data. Consider all probability distributions consistent with the observed data. Select the one with maximum entropy—i.e., the least biased distribution that fits what we know while assuming nothing extra.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-applied-to-irl">Maximum Entropy Applied to IRL<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-applied-to-irl" title="Permanent link">¶</a></h3>
<p>We seek a distribution over trajectories <span class="arithmatex">\(P(\tau)\)</span> that:</p>
<ol>
<li>Has maximum entropy, and</li>
<li>Matches expert feature expectations.</li>
</ol>
<p>Formally, we maximize:</p>
<div class="arithmatex">\[\max_{P} -\sum_{\tau} P(\tau)\log P(\tau)\]</div>
<p>subject to:</p>
<div class="arithmatex">\[\sum_{\tau} P(\tau)\mu(\tau) = \frac{1}{|D|}\sum_{\tau_i \in D} \mu(\tau_i)\]</div>
<div class="arithmatex">\[\sum_{\tau} P(\tau) = 1\]</div>
<p>Here:</p>
<ul>
<li><span class="arithmatex">\(\mu(\tau)\)</span> represents feature counts for trajectory <span class="arithmatex">\(\tau\)</span></li>
<li><span class="arithmatex">\(D\)</span> is the expert demonstration set</li>
</ul>
<p>This says: among all possible distributions consistent with observed expert feature averages, choose the one with maximum uncertainty.</p>
<h3 id="reinforcement-8_imitation_learning-matching-rewards">Matching Rewards<a class="headerlink" href="#reinforcement-8_imitation_learning-matching-rewards" title="Permanent link">¶</a></h3>
<p>In linear reward IRL, we assume rewards take the form:</p>
<div class="arithmatex">\[r_\phi(\tau) = \phi^\top \mu(\tau)\]</div>
<p>We want a policy <span class="arithmatex">\(\pi\)</span> that induces a trajectory distribution <span class="arithmatex">\(P(\tau)\)</span> matching the expert’s expected reward under <span class="arithmatex">\(r_\phi\)</span>:</p>
<div class="arithmatex">\[\max_{P(\tau)} -\sum_{\tau}P(\tau)\log P(\tau)\]</div>
<p>subject to:</p>
<div class="arithmatex">\[\sum_{\tau} P(\tau)r_\phi(\tau) = \sum_{\tau} \hat{P}(\tau)r_\phi(\tau)\]</div>
<div class="arithmatex">\[\sum_{\tau}P(\tau)=1\]</div>
<p>This aligns the learner’s expected reward with the expert’s reward estimate.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-exponential-family-distributions">Maximum Entropy ⇒ Exponential Family Distributions<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-exponential-family-distributions" title="Permanent link">¶</a></h3>
<p>Using constrained optimization (Lagrangians), we obtain:</p>
<div class="arithmatex">\[\log P(\tau) = \lambda_1 r_\phi(\tau) - 1 - \lambda_0\]</div>
<p>Thus:</p>
<div class="arithmatex">\[P(\tau) \propto \exp(r_\phi(\tau))\]</div>
<p>This reveals a key result: The maximum entropy distribution consistent with constraints belongs to the exponential family.</p>
<p>That is,</p>
<div class="arithmatex">\[p(\tau|\phi) = \frac{1}{Z(\phi)}\exp(r_\phi(\tau))\]</div>
<p>where</p>
<div class="arithmatex">\[Z(\phi)=\sum_{\tau}\exp(r_\phi(\tau))\]</div>
<p>This means we can now learn <span class="arithmatex">\(\phi\)</span> by maximizing likelihood of observed expert data, because the trajectory distribution becomes a normalized exponential model.</p>
<h3 id="reinforcement-8_imitation_learning-maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution">Maximum Entropy Over <span class="arithmatex">\(\tau\)</span> Equals Maximum Likelihood of Observed Data Under Max Entropy (Exponential Family) Distribution<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution" title="Permanent link">¶</a></h3>
<p>Jaynes (1957) showed:
Maximizing entropy over trajectories = maximizing likelihood of data under the maximum-entropy distribution.</p>
<p>So we:</p>
<ol>
<li>Assume <span class="arithmatex">\(p(\tau|\phi)\)</span> has exponential form</li>
<li>Learn <span class="arithmatex">\(\phi\)</span> by maximizing:</li>
</ol>
<div class="arithmatex">\[\max_{\phi} \prod_{\tau \in D} p(\tau|\phi)\]</div>
<p>This allows IRL to treat expert demonstrations as data to be probabilistically explained.</p>
<h2 id="reinforcement-8_imitation_learning-maximum-entropy-inverse-rl-algorithm">Maximum Entropy Inverse RL Algorithm<a class="headerlink" href="#reinforcement-8_imitation_learning-maximum-entropy-inverse-rl-algorithm" title="Permanent link">¶</a></h2>
<p>Assuming known dynamics and linear rewards:</p>
<ol>
<li>Input: expert demonstrations <span class="arithmatex">\(\mathcal{D}\)</span></li>
<li>Initialize reward weights <span class="arithmatex">\(r_\phi\)</span></li>
<li>Compute optimal policy <span class="arithmatex">\(\pi(a|s)\)</span> given <span class="arithmatex">\(r_\phi\)</span> (via dynamic programming / value iteration)</li>
<li>Compute state visitation frequencies <span class="arithmatex">\(\rho(s|\phi,T)\)</span></li>
<li>
<p>Compute gradient on reward parameters:</p>
<p><span class="arithmatex">\(\nabla J(\phi) = \frac{1}{N}\sum_{\tau_i \in \mathcal{D}} \mu(\tau_i) - \sum_{s}\rho(s|\phi,T)\mu(s)\)</span></p>
</li>
<li>
<p>Update <span class="arithmatex">\(\phi\)</span> via gradient step</p>
</li>
<li>Repeat from Step 3</li>
</ol>
<blockquote>
<p>Maximum Entropy IRL assumes experts act stochastically but optimally.
Instead of selecting a single best policy, it finds a distribution over trajectories consistent with expert behavior.
The resulting trajectory probabilities follow:
<span class="arithmatex">\(<span class="arithmatex">\(P(\tau) \propto \exp(r_\phi(\tau))\)</span>\)</span></p>
<p>Learning becomes maximum likelihood estimation: find reward parameters <span class="arithmatex">\(\phi\)</span> that best explain expert demonstrations.</p>
</blockquote>
<h2 id="reinforcement-8_imitation_learning-apprenticeship-learning">Apprenticeship Learning<a class="headerlink" href="#reinforcement-8_imitation_learning-apprenticeship-learning" title="Permanent link">¶</a></h2>
<p>Apprenticeship Learning usually refers to the scenario where an agent learns to perform a task by iteratively improving its policy using expert demonstrations as a reference. In many contexts, this term is used when an IRL algorithm is combined with policy learning: the agent behaves as an apprentice to the expert, gradually mastering the task. The classic formulation by Abbeel and Ng (2004) introduced apprenticeship learning via IRL, which guarantees that the learner’s policy will perform nearly as well as the expert’s, given enough demonstration data.</p>
<p>One way to think of apprenticeship learning is as follows: rather than directly cloning actions, we try to match the feature expectations of the expert. Suppose we have some features <span class="arithmatex">\(\phi(s)\)</span> of states (or state-action pairs) that capture what we care about in the task (for example, in driving, features might include lane deviation, speed, collision count, etc.). The expert will have some expected cumulative feature values <script type="math/tex"> \mathbb{E}_{\pi_E}\left[\sum_t \phi(s_t)\right] </script>. Apprenticeship learning methods aim for the learner to achieve similar feature expectations.</p>
<p>A prototypical apprenticeship learning algorithm proceeds like this:</p>
<ol>
<li>
<p>Initialize a candidate policy (it could even start random).</p>
</li>
<li>
<p>Evaluate how this policy behaves in terms of features (run it in simulation to estimate <span class="arithmatex">\(\mathbb{E}_{\pi}\left[\sum_t \phi(s_t)\right]\)</span>).</p>
</li>
<li>
<p>Compare the policy’s behavior to the expert’s behavior. Identify the biggest discrepancy in feature expectations.</p>
</li>
<li>
<p>Adjust the reward (implicitly defined as a weighted sum of features) to penalize the discrepancy. In other words, find reward weights <span class="arithmatex">\(w\)</span> such that the expert’s advantage over the apprentice in those feature dimensions is highlighted.</p>
</li>
<li>
<p>Optimize a new policy for this updated reward function (solve the MDP with the new <span class="arithmatex">\(w\)</span> to get <span class="arithmatex">\(\pi_{\text{new}}\)</span> that maximizes <span class="arithmatex">\(w \cdot \phi\)</span>).</p>
</li>
<li>
<p>Set this <span class="arithmatex">\(\pi_{\text{new}}\)</span> as the apprentice’s policy and repeat the evaluation -&gt; comparison -&gt; reward adjustment cycle.</p>
</li>
</ol>
<p>Each iteration pushes the apprentice to close the gap on the feature that most distinguishes it from the expert. After a few iterations, this process yields a policy that matches the expert on all key feature dimensions within some tolerance. At that point, the apprentice is essentially as good as the expert with respect to any reward expressible as a combination of those features.</p>
<p>The term apprenticeship learning highlights that the agent is not just mimicking blindly but is engaged in a process of improvement guided by the expert’s example. Importantly, the focus is on achieving at least the expert’s level of performance. We don’t necessarily care about identifying the exact reward the expert had; we care that our apprentice’s policy is successful. In fact, in the algorithm above, the reward weights <span class="arithmatex">\(w\)</span> found in each iteration are intermediate tools – at the end, one can take the final policy and deploy it, without needing to stick to a single explicit reward interpretation.</p>
<p>In relation to IRL, apprenticeship learning can be seen as a practical approach to use IRL for control: IRL finds a reward that explains the expert, and then the agent learns a policy for that reward; if it’s not yet good enough, adjust and repeat. Modern developments in imitation learning often follow this spirit. For example, Generative Adversarial Imitation Learning (GAIL) is a more recent technique where the agent learns a policy by trying to fool a discriminator into thinking the agent’s trajectories are from the expert – conceptually, the discriminator’s judgment provides a sort of reward signal telling the agent how "expert-like" its behavior is. This can be viewed as a form of apprenticeship learning, since the agent is iteratively tweaking its policy to become indistinguishable from the expert.</p>
<p>In summary, apprenticeship learning is about learning by iteratively comparing to an expert and closing the gap. It often uses IRL under the hood, but its end goal is the policy (the apprentice’s skill), not necessarily the reward. It underscores a key point: in imitation learning, sometimes we care more about performing as well as the expert (a direct goal), and sometimes we care about understanding the expert’s intentions (the indirect goal via IRL). Apprenticeship learning emphasizes the former.</p>
<h2 id="reinforcement-8_imitation_learning-imitation-learning-in-the-rl-landscape">Imitation Learning in the RL Landscape<a class="headerlink" href="#reinforcement-8_imitation_learning-imitation-learning-in-the-rl-landscape" title="Permanent link">¶</a></h2>
<p>Imitation learning fills an important niche in the overall reinforcement learning framework. It is especially useful when:</p>
<ol>
<li>
<p>Rewards are difficult to specify: If it's unclear how to craft a reward that captures all aspects of the desired behavior, providing demonstrations can bypass this. IL shines in complex tasks (e.g. high-level driving maneuvers, dexterous robot manipulation) where manually writing a reward function would be cumbersome or prone to error.</p>
</li>
<li>
<p>Rewards are sparse or delayed: When reward feedback is very rare or only given at the end of an episode, a pure RL agent might struggle to get enough signal to learn. An expert trajectory provides dense guidance at every time step (state-action pairs), effectively providing a shaped signal through imitation. This can jump-start learning in tasks that are otherwise too sparse for RL to crack (Chapter 4 discussed how sparse rewards make value estimation difficult – IL sidesteps that by using expert knowledge).</p>
</li>
<li>
<p>Exploration is risky or expensive: In real-world environments like robotics, autonomous driving, or healthcare, exploring with random or untrained policies can be dangerous or costly. IL allows learning a policy without the agent ever taking unguided actions in the real environment; it learns from safe, successful behaviors demonstrated by the expert. This makes it an attractive approach when safety is a hard constraint.</p>
</li>
</ol>
<p>It’s important to note that IL is not necessarily a replacement for reward-based RL, but rather a complement to it. A common practical approach is to bootstrap an agent with imitation learning and then fine-tune it with reinforcement learning. For example, one might first use behavioral cloning to teach a robot arm the basics of a task from human demonstrations, getting it into a reasonable regime of behavior; then, if a reward function is available (even a sparse one for success), use RL to further improve the policy, possibly surpassing the human expert's performance or adapting to slight changes in the task. The initial IL phase provides a good policy prior (saving time and avoiding dangerous exploration), and the subsequent RL phase lets the agent optimize and explore around that policy to refine skills.</p>
<p>On the flip side, imitation learning does require expert data. If obtaining demonstrations is hard (or if no expert exists for a brand-new task), IL might not be applicable. Moreover, if the expert demonstrations are of varying quality or contain noise, the agent will faithfully learn those imperfections unless additional measures (like filtering data or combining with RL optimization) are taken. In contrast, a pure RL approach, given a well-defined reward and enough exploration, can in principle discover superior strategies that no demonstrator provided. Thus, in practice, there is a trade-off: IL can dramatically speed up learning and improve safety given an expert, whereas RL remains the go-to when we only have a reward signal and the freedom to explore.</p>
<p>Imitation learning has become a critical part of the toolbox for solving real-world sequential decision problems. It enables success in domains that might be intractable for pure reinforcement learning by providing an external source of guidance. By learning directly from expert behavior – through methods like behavioral cloning (learning the policy directly) or inverse reinforcement learning (learning the underlying reward and then the policy) – an agent can shortcut the trial-and-error process. Of course, IL introduces its own challenges (distribution shift, reliance on demonstration coverage, potential suboptimality of the expert), but these can often be managed with algorithmic innovations (DAgger, combining IL with RL, etc.). In summary, imitation learning serves as a powerful paradigm for training agents in cases where designing rewards or allowing extensive exploration is impractical, and it often works hand-in-hand with traditional RL to achieve the best results in complex environments.</p>
<h3 id="reinforcement-8_imitation_learning-mental-map">Mental map<a class="headerlink" href="#reinforcement-8_imitation_learning-mental-map" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>                    Imitation Learning (IL)
      Goal: Learn behavior from expert demonstrations
                     instead of explicit rewards
                                │
                                ▼
             Why Imitation Learning? (Motivation)
 ┌───────────────────────────────────────────────────────────┐
 │ Hard to design rewards → reward hacking, tuning           │
 │ Sparse rewards → inefficient trial &amp; error                │
 │ Unsafe exploration (robots, driving, healthcare)          │
 │ Expert data available → demonstrations as guidance        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                   IL vs Reward-Based RL
 ┌─────────────────────────────┬──────────────────────────────┐
 │ Reward-Based RL             │ Imitation Learning           │
 │ + Explores actively         │ + Learns from expert         │
 │ + Needs reward design       │ + No explicit reward         │
 │ – Unsafe / inefficient      │ – Depends on demo quality   │
 └─────────────────────────────┴──────────────────────────────┘
                                │
                                ▼
                         IL Problem Setup
 ┌───────────────────────────────────────────────────────────┐
 │ MDP without reward function                                │
 │ Access to expert trajectories τE (s,a pairs)               │
 │ Goal → Learn policy π that mimics πE                       │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Core Method 1: Behavioral Cloning (BC)
 ┌───────────────────────────────────────────────────────────┐
 │ Treat imitation as supervised learning                    │
 │ Train πθ(s) → aE using dataset D                          │
 │ Discrete: cross-entropy loss                              │
 │ Continuous: mean squared error                            │
 │ Advantages: simple, offline, safe                         │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Key BC Problem: Covariate / Distribution Shift
 ┌───────────────────────────────────────────────────────────┐
 │ Trained only on expert states                             │
 │ When deployed, policy errors lead to unseen states        │
 │ → Poor decisions → more drift → compounding failure       │
 │ BC cannot recover or improve beyond expert                │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                    Fixing BC: DAgger (Idea)
 ┌───────────────────────────────────────────────────────────┐
 │ Let policy act, collect mistakes                          │
 │ Ask expert for correct action                             │
 │ Add to dataset and retrain                                │
 │ → brings training data closer to deployment distribution  │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
       Core Method 2: Inverse Reinforcement Learning (IRL)
 ┌───────────────────────────────────────────────────────────┐
 │ Learn the “why” behind actions → infer hidden reward R*   │
 │ Expert assumed optimal                                     │
 │ Solve inverse problem: πE ≈ optimal for R*                 │
 │ After reward recovered → run normal RL to learn policy     │
 │ Benefits: generalization, interpretability, improve expert │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                Core Method 3: Apprenticeship Learning
 ┌───────────────────────────────────────────────────────────┐
 │ Iteratively improve policy via comparing to expert        │
 │ Match feature expectations φ(s)                           │
 │ Reweights reward → optimize → evaluate → repeat           │
 │ Goal: perform at least as well as expert                  │
 │ Often implemented via IRL (e.g., GAIL conceptually)       │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Role of IL within broader RL landscape
 ┌───────────────────────────────────────────────────────────┐
 │ When IL is useful:                                        │
 │ - Reward hard to design                                   │
 │ - Unsafe or costly to explore                             │
 │ - Sparse reward tasks                                     │
 │ IL + RL hybrid: BC warm-start → RL fine-tune beyond expert│
 │ Limitations: need expert, demos may be suboptimal         │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Final Takeaway (Chapter Summary)
 ┌───────────────────────────────────────────────────────────┐
 │ IL bypasses reward engineering &amp; risky exploration         │
 │ BC learns “what,” IRL learns “why,” apprenticeship learns  │
 │ “how to get as good as expert.”                           │
 │ IL often combined with RL for best performance.           │
 └───────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-9_rlhf" heading-number="2.9"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment">Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment<a class="headerlink" href="#reinforcement-9_rlhf-chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment" title="Permanent link">¶</a></h1>
<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for “good” behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment – ensuring an AI’s behavior aligns with human preferences and values – by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute “score” to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p>
<p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>
<h2 id="reinforcement-9_rlhf-bradleyterry-preference-modeling-in-rlhf">Bradley–Terry Preference Modeling in RLHF<a class="headerlink" href="#reinforcement-9_rlhf-bradleyterry-preference-modeling-in-rlhf" title="Permanent link">¶</a></h2>
<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley–Terry model, a probabilistic model for noisy comparisons. </p>
<p>Consider a <span class="arithmatex">\(K\)</span>-armed bandit with actions <span class="arithmatex">\(b_1, b_2, \dots, b_K\)</span>, and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action <span class="arithmatex">\(b_i\)</span> over <span class="arithmatex">\(b_j\)</span> is modeled as:</p>
<div class="arithmatex">\[
P(b_i \succ b_j)
=
\frac{\exp(r(b_i))}{\exp(r(b_i)) + \exp(r(b_j))}
=
p_{ij}
\]</div>
<p>where <span class="arithmatex">\(r(b)\)</span> is an unobserved scalar reward associated with action <span class="arithmatex">\(b\)</span>. Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p>
<p>Assume we collect a dataset <span class="arithmatex">\(\mathcal{D}\)</span> of <span class="arithmatex">\(N\)</span> comparisons of the form <span class="arithmatex">\((b_i, b_j, \mu)\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mu(1) = 1\)</span> if the human marked <span class="arithmatex">\(b_i \succ b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0.5\)</span> if the human marked <span class="arithmatex">\(b_i = b_j\)</span></li>
<li><span class="arithmatex">\(\mu(1) = 0\)</span> if the human marked <span class="arithmatex">\(b_j \succ b_i\)</span></li>
</ul>
<p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p>
<div class="arithmatex">\[
\mathcal{L}
=
-
\sum_{(b_i,b_j,\mu)\in\mathcal{D}}
\left[
\mu(1)\log P(b_i \succ b_j)
+
\mu(2)\log P(b_j \succ b_i)
\right]
\]</div>
<p>Optimizing this loss adjusts the reward function <span class="arithmatex">\(r(\cdot)\)</span> so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p>
<p>Once the reward model is trained using the Bradley–Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley–Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>
<h2 id="reinforcement-9_rlhf-the-rlhf-training-pipeline">The RLHF Training Pipeline<a class="headerlink" href="#reinforcement-9_rlhf-the-rlhf-training-pipeline" title="Permanent link">¶</a></h2>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../reinforcement/images/llm_rlhf.png" data-desc-position="bottom"><img alt="Alt text" src="../reinforcement/images/llm_rlhf.png"></a></p>
<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p>
<ol>
<li>
<p>Supervised Fine-Tuning (SFT) – Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p>
</li>
<li>
<p>Reward Model Training from Human Preferences – Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function <span class="arithmatex">\(r_\phi(x,y)\)</span> (parameterized by <span class="arithmatex">\(\phi\)</span>) that predicts which response is more preferable for a given input x using Bradley–Terry model.</p>
</li>
<li>
<p>Reinforcement Learning Fine-Tuning – In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy <span class="arithmatex">\(\pi_\theta(y|x)\)</span> (with parameters <span class="arithmatex">\(\theta\)</span>) is updated to maximize the expected reward <span class="arithmatex">\(r_\phi(x,y)\)</span> of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model’s score, the policy might exploit flaws in <span class="arithmatex">\(r_\phi\)</span> (a form of “reward hacking”) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback–Leibler (KL) penalty that keeps the new policy <span class="arithmatex">\(\pi_\theta\)</span> close to the reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span> (often the SFT model). In summary, the RL objective can be written as:</p>
<div class="arithmatex">\[\max_{\pi_\theta} (
\underbrace{
\mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi_\theta(y \mid x)}
}_{\text{Sample from policy}}
\left[
\underbrace{
r_\phi(x,y)
}_{\text{Want high reward}}
\right]
-
\underbrace{
\beta \, \mathbb{D}_{\mathrm{KL}}
\left[
\pi_\theta(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x)
\right]
}_{\text{Keep KL to original model small}})
\]</div>
<p>where <span class="arithmatex">\(\beta&gt;0\)</span> controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if <span class="arithmatex">\(\pi_\theta\)</span> deviates too much from the original model’s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p>
</li>
</ol>
<p>Through this pipeline – SFT, reward modeling, and RL fine-tuning – we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like <span class="arithmatex">\(\beta\)</span> and PPO clip thresholds). The process can be unstable; for instance, if <span class="arithmatex">\(\beta\)</span> is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if <span class="arithmatex">\(\beta\)</span> is too high, the policy might hardly improve at all. Researchers have described RLHF as a “complex and often unstable procedure” that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>
<h2 id="reinforcement-9_rlhf-direct-preference-optimization-rlhf-without-rl">Direct Preference Optimization: RLHF without RL?<a class="headerlink" href="#reinforcement-9_rlhf-direct-preference-optimization-rlhf-without-rl" title="Permanent link">¶</a></h2>
<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p>
<p>Recall that in the RLHF setting, our goal is to find a policy <span class="arithmatex">\(\pi^*(y|x)\)</span> that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of <span class="arithmatex">\(J(\pi)\)</span> occurs when <span class="arithmatex">\(\pi\)</span> is proportional to the reference policy times an exponential of the reward:</p>
<div class="arithmatex">\[\pi^*(y \mid x) \propto \pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{1}{\beta}\, r_\phi(x, y)\right)\]</div>
<p>This equation gives a closed-form solution for the optimal policy in terms of the reward function <span class="arithmatex">\(r_\phi\)</span>. It makes sense: actions <span class="arithmatex">\(y\)</span> that have higher human-derived reward should be taken with higher probability, but we temper this by <span class="arithmatex">\(\beta\)</span> and weight by the reference probabilities <span class="arithmatex">\(\pi_{\text{ref}}(y|x)\)</span> so that we don’t stray too far. If we were to normalize the right-hand side, we’d write:</p>
<div class="arithmatex">\[\pi^*(y \mid x)
=
\frac{
\pi_{\text{ref}}(y \mid x)\,
\exp\!\left(\frac{r_\phi(x,y)}{\beta}\right)
}{
\sum_{y'} \pi_{\text{ref}}(y' \mid x)\,
\exp\!\left(\frac{r_\phi(x,y')}{\beta}\right)
}\]</div>
<p>Here the denominator is a partition functionsumming over all possible responses <span class="arithmatex">\(y'\)</span> for input <span class="arithmatex">\(x\)</span>. This normalization involves a sum over the entire response space, which is astronomically large for language models – hence we cannot directly compute <span class="arithmatex">\(\pi^*(y|x)\)</span> in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p>
<p>DPO’s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses <span class="arithmatex">\(y_+\)</span> (preferred) and <span class="arithmatex">\(y_-\)</span> (dispreferred) for the same context <span class="arithmatex">\(x\)</span>, the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p>
<div class="arithmatex">\[\frac{\pi^*(y^+ \mid x)}{\pi^\ast(y^- \mid x)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^+)}{\beta}\right)}
{\pi_{\text{ref}}(y^- \mid x)\,
\exp\!\left(\frac{r_\phi(x, y^-)}{\beta}\right)}
=
\frac{\pi_{\text{ref}}(y^+ \mid x)}
{\pi_{\text{ref}}(y^- \mid x)}
\exp\!\left(
\frac{1}{\beta}
\big[
r_\phi(x, y^+) - r_\phi(x, y^-)
\big]
\right)\]</div>
<p>Taking the log of both sides, we get a neat relationship:</p>
<div class="arithmatex">\[\frac{1}{\beta}
\big( r_\phi(x, y^{+}) - r_\phi(x, y^{-}) \big)
=
\big[ \log \pi^\ast(y^{+} \mid x) - \log \pi^\ast(y^{-} \mid x) \big]
-
\big[ \log \pi_{\text{ref}}(y^{+} \mid x) - \log \pi_{\text{ref}}(y^{-} \mid x) \big]\]</div>
<p>The term in brackets on the right is the difference in log-probabilities that the optimal policy <span class="arithmatex">\(\pi^*\)</span> assigns to the two responses (which in turn would equal the difference in our learned policy’s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if <span class="arithmatex">\(y_+\)</span> is better than <span class="arithmatex">\(y_-\)</span> by some amount of reward, then the optimal policy should tilt its probabilities in favor of <span class="arithmatex">\(y_+\)</span> by a corresponding factor.</p>
<p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts <span class="arithmatex">\(\pi_\theta\)</span> to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context <span class="arithmatex">\(x\)</span> and two candidate outputs <span class="arithmatex">\(y_+\)</span> (human-preferred) and <span class="arithmatex">\(y_-\)</span> (human-dispreferred), we want the model to assign a higher probability to <span class="arithmatex">\(y_+\)</span> than to <span class="arithmatex">\(y_-\)</span>, with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p>
<p>In practice, the DPO loss for a pair <span class="arithmatex">\((x, y_+, y_-)\)</span> is something like:</p>
<div class="arithmatex">\[\ell_{\text{DPO}}(\theta)
= - \log \sigma \!\left(
\beta\,
\big[ \log \pi_\theta(y^{+} \mid x) - \log \pi_\theta(y^{-} \mid x) \big]
\right)\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is the sigmoid function. This loss is low (i.e. good) when <span class="arithmatex">\(\log \pi_\theta(y_+|x) \gg \log \pi_\theta(y_-|x)\)</span>, meaning the model assigns much higher probability to the preferred outcome – which is what we want. If the model hasn’t yet learned the preference, the loss will be higher, and gradient descent on this loss will push <span class="arithmatex">\(\pi_\theta\)</span> to increase the probability of <span class="arithmatex">\(y_+\)</span> and decrease that of <span class="arithmatex">\(y_-\)</span>. Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy’s logits: effectively, <span class="arithmatex">\(\log \pi_\theta(y|x)\)</span> plays the role of a reward score for how good <span class="arithmatex">\(y\)</span> is, up to the scaling factor <span class="arithmatex">\(1/\beta\)</span>. In fact, the DPO derivation can be seen as combining the preference loss on <span class="arithmatex">\(r_\phi\)</span> with the <span class="arithmatex">\(\pi^*\)</span> solution formula to produce a preference loss on <span class="arithmatex">\(\pi_\theta\)</span>. The original DPO paper calls this approach “your language model is secretly a reward model” – by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p>
<p>## Mental map</p>
<p><code>text
         Reinforcement Learning from Human Feedback (RLHF)
   Goal: Align model behavior with human preferences and values
          when explicit reward design is impractical
                                │
                                ▼
           Why Dense Rewards Are Hard for Language Models
 ┌───────────────────────────────────────────────────────────┐
 │ Open-ended tasks (dialogue, summarization, reasoning)     │
 │ No clear numeric notion of “good” behavior                │
 │ Hand-crafted dense rewards → miss nuance, reward hacking  │
 │ Metrics (BLEU, ROUGE, length) poorly reflect human values │
 │ Human values are subjective, contextual, and fuzzy        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
            From Imitation Learning to Human Preferences
 ┌───────────────────────────────────────────────────────────┐
 │ Behavioral Cloning (IL): imitate demonstrations           │
 │ + Simple, safe, no reward needed                          │
 │ – Cannot exceed expert, sensitive to distribution shift   │
 │ DAgger: fixes BC but requires step-by-step human labeling │
 │ Pairwise preferences = middle ground                      │
 │ → no dense rewards, no per-step supervision               │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
           Pairwise Preference Feedback (Key Idea)
 ┌───────────────────────────────────────────────────────────┐
 │ Humans compare two outputs and choose the better one      │
 │ Easier than assigning numeric rewards                     │
 │ More informative than raw demonstrations                  │
 │ Scales to complex, open-ended behaviors                   │
 │ Forms basis of reward learning in RLHF                    │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
        Bradley–Terry Model: Preferences → Reward Signal
 ┌───────────────────────────────────────────────────────────┐
 │ Model noisy human comparisons probabilistically           │
 │ P(b_i ≻ b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    │
 │ r(b): latent scalar reward                                │
 │ Fit r(·) by maximizing likelihood / cross-entropy         │
 │ Preferred outputs get higher reward scores                │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
             RLHF Training Pipeline (3 Stages)
 ┌───────────────────────────────────────────────────────────┐
 │ 1. Supervised Fine-Tuning (SFT)                           │
 │    – Behavioral cloning on human-written demos            │
 │    – Produces reference policy π_ref                      │
 │                                                           │
 │ 2. Reward Model Training                                  │
 │    – Human pairwise preferences                           │
 │    – Train r_φ(x,y) via Bradley–Terry loss                │
 │                                                           │
 │ 3. RL Fine-Tuning (PPO)                                   │
 │    – Maximize reward r_φ(x,y)                             │
 │    – KL penalty keeps π_θ close to π_ref                  │
 │    – Prevents reward hacking &amp; language drift             │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              RLHF Objective (KL-Regularized RL)
 ┌───────────────────────────────────────────────────────────┐
 │ Maximize:                                                 │
 │   E[r_φ(x,y)] − β · KL(π_θ || π_ref)                      │
 │ β controls tradeoff:                                      │
 │   Low β → reward hacking / mode collapse                  │
 │   High β → little improvement over SFT                    │
 │ PPO provides stable policy updates                        │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
          Limitations of Standard RLHF (PPO-based)
 ┌───────────────────────────────────────────────────────────┐
 │ Requires training multiple models                         │
 │ Many hyperparameters (β, PPO clip, value loss, etc.)      │
 │ Sampling-based RL can be unstable                         │
 │ Expensive and complex pipeline                            │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
      Direct Preference Optimization (DPO): RLHF without RL
 ┌───────────────────────────────────────────────────────────┐
 │ Solve RLHF objective in closed form                       │
 │ Optimal policy:                                           │
 │   π*(y|x) ∝ π_ref(y|x) · exp(r_φ(x,y)/β)                  |
 │ Use probability ratios → normalization cancels            │
 │ Train π_θ directly on preference pairs                    │
 │ Loss: sigmoid on log-prob difference                      │
 │ “Your LM is secretly a reward model”                      │
 └───────────────────────────────────────────────────────────┘
                                │
                                ▼
              DPO vs PPO-based RLHF
 ┌─────────────────────────────┬─────────────────────────────┐
 │ RLHF (PPO)                  │ DPO                         │
 │ + Explicit RL optimization  │ + Pure supervised learning  │
 │ – Complex &amp; unstable        │ – Assumes KL-optimal form   │
 │ – Many hyperparameters      │ + Simple, stable, efficient │
 │                             │ + No separate reward model  │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
              Final Takeaway (Chapter Summary)
 ┌───────────────────────────────────────────────────────────┐
 │ Dense rewards are hard for language tasks                 │
 │ Pairwise preferences provide natural human feedback       │
 │ RLHF learns rewards from preferences + optimizes policy   │
 │ DPO simplifies RLHF by removing explicit RL               │
 │ Together, they extend imitation learning toward           │
 │ scalable value alignment for modern language models       │
 └───────────────────────────────────────────────────────────┘</code></p></body></html></section><section class="print-page" id="reinforcement-10_offline_rl" heading-number="2.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-10-batch-offline-rl-policy-evaluation-optimization">Chapter 10: Batch / Offline RL Policy Evaluation &amp; Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-chapter-10-batch-offline-rl-policy-evaluation-optimization" title="Permanent link">¶</a></h1>
<p>Learning from the Past</p>
<ul>
<li>Learning from Past Human Demonstrations: Imitation Learning</li>
<li>Learning from Past Human Preferences: RLHF and DPO</li>
<li>Learning from Past Decisions and Actions: Offline RL</li>
</ul>
<h2 id="reinforcement-10_offline_rl-offline-reinforcement-learning-a-different-approach">Offline Reinforcement Learning: A Different Approach<a class="headerlink" href="#reinforcement-10_offline_rl-offline-reinforcement-learning-a-different-approach" title="Permanent link">¶</a></h2>
<p>Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.</p>
<p>In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.</p>
<blockquote>
<p>Why Can’t We Just Use Q-Learning?</p>
<ul>
<li>Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values</li>
<li>But deadly triad of bootstrapping, function approximation and off
policy, and can fail</li>
</ul>
</blockquote>
<h2 id="reinforcement-10_offline_rl-batch-policy-evaluation-estimating-the-performance-of-a-policy">Batch Policy Evaluation: Estimating the Performance of a Policy<a class="headerlink" href="#reinforcement-10_offline_rl-batch-policy-evaluation-estimating-the-performance-of-a-policy" title="Permanent link">¶</a></h2>
<p>Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.</p>
<ol>
<li>
<p>Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.</p>
<p>Specifically, it learns two main components from data: a reward function <span class="arithmatex">\(\hat{r}(s,a)\)</span> and transition dynamics <span class="arithmatex">\(\hat{P}(s' \mid s,a)\)</span>. These are learned via supervised learning on the offline dataset <span class="arithmatex">\(D\)</span> of transitions collected by some behavior policy <span class="arithmatex">\(\pi_b\)</span>. For example, <span class="arithmatex">\(\hat{r}(s,a)\)</span> can be trained by regression to predict the observed reward given state <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>, and <span class="arithmatex">\(\hat{P}(s' \mid s,a)\)</span> can be fit to predict the next-state <span class="arithmatex">\(s'\)</span> (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in <span class="arithmatex">\(D\)</span> under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to “learn the environment’s rules". Once learned, this model <span class="arithmatex">\(\hat{\mathcal{M}} = (\hat{P}, \hat{r})\)</span> serves as a proxy for the real environment, which we can use for evaluating any policy <span class="arithmatex">\(\pi\)</span> without further real experience.</p>
<p>It’s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics <span class="arithmatex">\(\hat{P}\)</span> will be reliable only in regions covered by <span class="arithmatex">\(D\)</span>; if <span class="arithmatex">\(\pi\)</span> later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).</p>
<h4 id="reinforcement-10_offline_rl-algorithmic-outline-offline-policy-evaluation-via-model">Algorithmic Outline - Offline Policy Evaluation via Model:<a class="headerlink" href="#reinforcement-10_offline_rl-algorithmic-outline-offline-policy-evaluation-via-model" title="Permanent link">¶</a></h4>
<ol>
<li>
<p>Input: offline dataset <span class="arithmatex">\(D\)</span> of transitions (from behavior <span class="arithmatex">\(\pi_b\)</span>), a policy <span class="arithmatex">\(\pi\)</span> to evaluate, discount <span class="arithmatex">\(\gamma\)</span>.</p>
</li>
<li>
<p>Model Learning: Fit <span class="arithmatex">\(\hat{P}(s'|s,a)\)</span> and <span class="arithmatex">\(\hat{r}(s,a)\)</span> using <span class="arithmatex">\(D\)</span> (e.g. maximum likelihood estimation for dynamics, regression for rewards).</p>
</li>
<li>
<p>Policy Evaluation: Initialize <span class="arithmatex">\(V(s)=0\)</span> for all states (or some initial guess).</p>
</li>
<li>
<p>Loop (Bellman backups using <span class="arithmatex">\(\hat{P},\hat{r}\)</span>): For each state <span class="arithmatex">\(s\)</span> in the state space (or a representative set of states):</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(\hat{R}^\pi(s) = \sum_a \pi(a|s)\hat{r}(s,a)\)</span>.</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(V_{\text{new}}(s) = \hat{R}^\pi(s) + \gamma \sum_{s'} \hat{P}^\pi(s'\mid s),V(s')\)</span>.</p>
</li>
<li>
<p>Update <span class="arithmatex">\(V \leftarrow V_{\text{new}}\)</span> and repeat until convergence (the changes in <span class="arithmatex">\(V\)</span> are below a threshold).</p>
</li>
<li>
<p>Output: <span class="arithmatex">\(V(s)\)</span> for states of interest (e.g. the estimated value of <span class="arithmatex">\(\pi\)</span> under the initial state distribution <span class="arithmatex">\(S_0\)</span> can be obtained by <span class="arithmatex">\(\mathbb{E}_{s_0\sim S_0}[V(s_0)]\)</span>).</p>
</li>
</ol>
</li>
<li>
<p>Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.</p>
<h3 id="reinforcement-10_offline_rl-algorithm-3-fitted-q-evaluation-fqe-pi-c">Algorithm 3 Fitted Q Evaluation: FQE <span class="arithmatex">\((\pi, c)\)</span><a class="headerlink" href="#reinforcement-10_offline_rl-algorithm-3-fitted-q-evaluation-fqe-pi-c" title="Permanent link">¶</a></h3>
<p>Input: Dataset <span class="arithmatex">\(\mathcal{D} = \{(x_i, a_i, x'_i, c_i)\}_{i=1}^n \sim \pi_D\)</span>. Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy <span class="arithmatex">\(\pi\)</span> by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state <span class="arithmatex">\(s_i\)</span>, taking action <span class="arithmatex">\(a_i\)</span>, and then following policy <span class="arithmatex">\(\pi\)</span> thereafter.At each iteration, we construct a Bellman target:</p>
<div class="arithmatex">\[\tilde{Q}^\pi(s_i, a_i)
=
c_i + \gamma V_\theta^\pi(s_{i+1})
\]</div>
<p>where</p>
<div class="arithmatex">\[
V_\theta^\pi(s_{i+1}) = Q_\theta^\pi(s_{i+1}, \pi(s_{i+1})).
\]</div>
<p>The Q-function is parameterized by <span class="arithmatex">\(\theta\)</span> (e.g., a neural network), and is learned by solving a supervised regression problem:</p>
<div class="arithmatex">\[\arg\min_\theta
\sum_i
\Big(
Q_\theta^\pi(s_i, a_i)
-
\tilde{Q}^\pi(s_i, a_i)
\Big)^2\]</div>
<p>This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy <span class="arithmatex">\(\pi\)</span> under the data distribution induced by <span class="arithmatex">\(\pi_D\)</span>.</p>
<p>Function class <span class="arithmatex">\(F\)</span> (Let's assume we use a DNN for F). Policy </p>
<p><span class="arithmatex">\(\pi\)</span> to be evaluated.
1: Initialize <span class="arithmatex">\(Q_0 \in F\)</span> randomly<br>
2: for <span class="arithmatex">\(k = 1, 2, \dots, K\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> Compute target <span class="arithmatex">\(y_i = c_i + \gamma Q_{k-1}(x'_i, \pi(x'_i)) \quad \forall i\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Build training set <span class="arithmatex">\(\tilde{\mathcal{D}}_k = \{(x_i, a_i), y_i\}_{i=1}^n\)</span><br>
5: <span class="arithmatex">\(\quad\)</span> Solve a supervised learning problem:<br>
<script type="math/tex; mode=display">
Q_k = \arg\min_{f \in F} \frac{1}{n} \sum_{i=1}^n \big(f(x_i, a_i) - y_i\big)^2
</script>
<br>
6: end for<br>
Output: <span class="arithmatex">\(\hat{C}^\pi(x) = Q_K(x, \pi(x)) \quad \forall x\)</span></p>
<blockquote>
<h2 id="reinforcement-10_offline_rl-what-is-different-vs-dqn">What is different vs DQN?<a class="headerlink" href="#reinforcement-10_offline_rl-what-is-different-vs-dqn" title="Permanent link">¶</a></h2>
<p>DQN learns an optimal policy by interacting with the environment, while FQE evaluates a <em>fixed policy</em> using a <em>fixed offline dataset</em>.</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>FQE (Fitted Q Evaluation)</th>
<th>DQN (Deep Q-Network)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Goal</td>
<td>Policy evaluation</td>
<td>Policy optimization / control</td>
</tr>
<tr>
<td>Policy</td>
<td>Fixed target policy <span class="arithmatex">\(\pi\)</span></td>
<td>Implicitly learned via <span class="arithmatex">\(\max_a Q(s,a)\)</span></td>
</tr>
<tr>
<td>Data</td>
<td>Offline, fixed dataset <span class="arithmatex">\(\mathcal{D}\)</span></td>
<td>Online, collected during training</td>
</tr>
<tr>
<td>Bellman target</td>
<td><span class="arithmatex">\(c + \gamma Q(s', \pi(s'))\)</span></td>
<td><span class="arithmatex">\(r + \gamma \max_a Q(s', a)\)</span></td>
</tr>
<tr>
<td>Action at next state</td>
<td>From given policy <span class="arithmatex">\(\pi\)</span></td>
<td>Greedy over Q-values</td>
</tr>
<tr>
<td>Exploration</td>
<td>None</td>
<td>Required (e.g. <span class="arithmatex">\(\epsilon\)</span>-greedy)</td>
</tr>
<tr>
<td>Dataset changes?</td>
<td>❌ No</td>
<td>✅ Yes</td>
</tr>
<tr>
<td>Off-policy instability</td>
<td>Low</td>
<td>High</td>
</tr>
<tr>
<td>Convergence guarantees</td>
<td>Yes (tabular / linear)</td>
<td>No (with function approximation)</td>
</tr>
</tbody>
</table>
<h3 id="reinforcement-10_offline_rl-1-no-maximization-bias-in-fqe">1. No maximization bias in FQE<a class="headerlink" href="#reinforcement-10_offline_rl-1-no-maximization-bias-in-fqe" title="Permanent link">¶</a></h3>
<ul>
<li>DQN suffers from overestimation bias</li>
<li>FQE does pure regression, no bootstrapped max</li>
</ul>
<h3 id="reinforcement-10_offline_rl-2-stability">2. Stability<a class="headerlink" href="#reinforcement-10_offline_rl-2-stability" title="Permanent link">¶</a></h3>
<ul>
<li>FQE ≈ supervised learning  </li>
<li>DQN ≈ bootstrapped + non-stationary targets</li>
</ul>
<h3 id="reinforcement-10_offline_rl-3-offline-vs-online">3. Offline vs Online<a class="headerlink" href="#reinforcement-10_offline_rl-3-offline-vs-online" title="Permanent link">¶</a></h3>
<ul>
<li>FQE cannot improve the policy  </li>
<li>DQN must interact with environment</li>
</ul>
</blockquote>
</li>
<li>
<p>Importance Sampling:
    This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy’s value – assuming coverage (i.e. the target policy doesn’t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.</p>
</li>
</ol>
<h2 id="reinforcement-10_offline_rl-offline-policy-learning-optimization">Offline Policy Learning / Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-offline-policy-learning-optimization" title="Permanent link">¶</a></h2>
<p>Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.</p>
<ol>
<li>
<p>Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.</p>
</li>
<li>
<p>Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.</p>
</li>
<li>
<p>Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.</p>
</li>
</ol>
<h2 id="reinforcement-10_offline_rl-challenges-in-offline-policy-optimization">Challenges in Offline Policy Optimization<a class="headerlink" href="#reinforcement-10_offline_rl-challenges-in-offline-policy-optimization" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.</p>
</li>
<li>
<p>Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.</p>
</li>
<li>
<p>Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.</p>
</li>
</ol>
<p>Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.</p>
<h2 id="reinforcement-10_offline_rl-mental-map">Mental Map<a class="headerlink" href="#reinforcement-10_offline_rl-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                 Offline / Batch Reinforcement Learning
        Goal: Learn and evaluate policies from fixed historical data
           when exploration is unsafe, expensive, or impossible
                                │
                                ▼
              Why Online RL Is Not Always Feasible
 ┌─────────────────────────────────────────────────────────────┐
 │ Exploration can be dangerous (healthcare, driving, robotics)│
 │ Data already exists from past decisions                     │
 │ Real systems cannot reset or freely experiment              │
 │ We must learn without interacting with the environment      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Offline RL vs Standard RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Standard RL:                                                │
 │  – Collect data with current policy                         │
 │  – Explore → improve → repeat                               │
 │                                                             │
 │ Offline RL:                                                 │
 │  – Fixed dataset D from behavior policy π_b                 │
 │  – No new interaction allowed                               │
 │  – Must generalize only from observed data                  │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Why “Just Use Q-Learning” Fails Offline
 ┌─────────────────────────────────────────────────────────────┐
 │ Q-learning is off-policy — but not offline-safe             │
 │ Deadly triad:                                               │
 │   • Bootstrapping                                           │
 │   • Function approximation                                  │
 │   • Off-policy learning                                     │
 │ Leads to divergence &amp; overestimation                        │
 │ Especially severe with distribution mismatch                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Offline RL Decomposed into Two Core Problems
 ┌───────────────────────────────┬─────────────────────────────┐
 │ 1. Policy Evaluation (OPE)    │ 2. Policy Optimization      │
 │    “How good is this policy?” │    “How can we improve it?” │
 │    Without running it         │    Without new data         │
 └───────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
            Batch / Offline Policy Evaluation (OPE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate V^π or J(π) using only dataset D                   │
 │ Three major approaches:                                     │
 │  1. Model-based evaluation                                  │
 │  2. Model-free evaluation (FQE)                             │
 │  3. Importance Sampling                                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        1. Model-Based Offline Policy Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Learn a model from data:                                    │
 │   • Reward model: r̂(s,a)                                    │
 │   • Transition model: P̂(s'|s,a)                             │
 │ Treat batch data as supervised learning                     │
 │ Then simulate policy π inside learned model                 │
 │ Use Bellman backups on (P̂, r̂)                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Model-Based OPE: Key Limitation
 ┌─────────────────────────────────────────────────────────────┐
 │ Model is only reliable where data exists                    │
 │ Policy visiting unseen states/actions → extrapolation error │
 │ Model hallucination → highly biased value estimates         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Evaluate a *fixed policy* π                                 │
 │ Learn Q^π(s,a) from offline data via regression             │
 │ Bellman target:                                             │
 │   y = c + γ Q(s', π(s'))                                    │
 │ Pure supervised learning loop                               │
 │ Stable compared to Q-learning / DQN                         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             FQE vs DQN (Key Insight)
 ┌─────────────────────────────┬─────────────────────────────┐
 │ DQN                         │ FQE                         │
 │ Learns optimal policy       │ Evaluates fixed policy      │
 │ Uses max over actions       │ Uses given π(s')            │
 │ Online data collection      │ Fully offline               │
 │ Overestimation bias         │ No max → more stable        │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
        3. Importance Sampling (IS) Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat OPE as statistical estimation                         │
 │ Reweight trajectories by π / π_b                            │
 │ Unbiased if coverage holds                                  │
 │ Severe variance for long horizons or policy mismatch        │
 │ Variants:                                                   │
 │   • Per-decision IS                                         │
 │   • Weighted IS                                             │
 │   • Doubly robust estimators                                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
            Offline Policy Optimization
 ┌─────────────────────────────────────────────────────────────┐
 │ Goal: improve policy using only dataset D                   │
 │ Model-free: Fitted Q Iteration (FQI)                        │
 │ Model-based: planning inside learned model                  │
 │ Core challenge: distribution shift                          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        The Central Problem: Distribution Mismatch
 ┌─────────────────────────────────────────────────────────────┐
 │ Learned policy chooses actions unseen in data               │
 │ Q-values extrapolate → overly optimistic                    │
 │ Performance collapses at deployment                         │
 │ Offline RL ≠ just off-policy RL                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Conservative / Pessimistic Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Assume unknown actions are risky                            │
 │ Penalize state-action pairs with low data support           │
 │ Prefer policies close to behavior policy                    │
 │ Examples (conceptually):                                    │
 │   • Conservative Q-Learning (CQL)                           │
 │   • Regularization toward π_b                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Key Challenges in Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Coverage / overlap requirement                              │
 │ Model misspecification                                      │
 │ Value overestimation                                        │
 │ Bias–variance tradeoffs                                     │
 │ Safety vs optimality                                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Final Takeaway (Chapter Summary)
 ┌─────────────────────────────────────────────────────────────┐
 │ Offline RL learns entirely from past experience             │
 │ Policy evaluation is foundational before optimization       │
 │ Model-based, FQE, and IS provide OPE tools                  │
 │ Main risk: distribution shift &amp; extrapolation               │
 │ Conservative methods trade performance for safety           │
 │ Offline RL is essential for real-world decision systems     │
 └─────────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-11_fast_rl" heading-number="2.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-11-data-efficient-reinforcement-learning-bandit-foundations">Chapter 11: Data-Efficient Reinforcement Learning — Bandit Foundations<a class="headerlink" href="#reinforcement-11_fast_rl-chapter-11-data-efficient-reinforcement-learning-bandit-foundations" title="Permanent link">¶</a></h1>
<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>
<h2 id="reinforcement-11_fast_rl-the-multi-armed-bandit-model">The Multi-Armed Bandit Model<a class="headerlink" href="#reinforcement-11_fast_rl-the-multi-armed-bandit-model" title="Permanent link">¶</a></h2>
<p>A multi-armed bandit is defined as a tuple <span class="arithmatex">\((\mathcal{A}, \mathcal{R})\)</span>, where:</p>
<ul>
<li><span class="arithmatex">\(\mathcal{A} = \{a_1, \dots, a_m\}\)</span> is a known, finite set of actions (arms),</li>
<li><span class="arithmatex">\(R_a(r) = \mathbb{P}[r \mid a]\)</span> is an unknown probability distribution over rewards for each action.</li>
<li>there is no "state".</li>
</ul>
<p>At each timestep <span class="arithmatex">\(t\)</span>, the agent:</p>
<ol>
<li>Chooses an action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span>,</li>
<li>Receives a stochastic reward <span class="arithmatex">\(r_t \sim R_{a_t}\)</span>.</li>
</ol>
<p>Goal: Maximize cumulative reward:<br>
<script type="math/tex; mode=display">
\sum_{t=1}^{T} r_t
</script>
</p>
<p>This simple model embodies the core RL challenges—particularly exploration vs. exploitation—in an isolated setting.</p>
<h3 id="reinforcement-11_fast_rl-evaluating-algorithms-regret-framework">Evaluating Algorithms: Regret Framework<a class="headerlink" href="#reinforcement-11_fast_rl-evaluating-algorithms-regret-framework" title="Permanent link">¶</a></h3>
<p>Regret: </p>
<ul>
<li><span class="arithmatex">\(Q(a) = \mathbb{E}[r \mid a]\)</span> be the expected reward for action <span class="arithmatex">\(a\)</span>,</li>
<li><span class="arithmatex">\(a^* = \arg\max_{a \in \mathcal{A}} Q(a)\)</span>,</li>
<li>Optimal Value <span class="arithmatex">\(V^* = Q(a^*)\)</span></li>
</ul>
<p>Then regret is the opportunity loss for one step:
<script type="math/tex; mode=display">
\ell_t = \mathbb{E}[V^* - Q(a_t)]
</script>
</p>
<p>Total Regret is the total opportunity loss: Total regret over <span class="arithmatex">\(T\)</span> timesteps</p>
<p>
<script type="math/tex; mode=display">
L_T = \sum_{t=1}^T \ell_t = \sum_{a \in \mathcal{A}} \mathbb{E}[N_T(a)] \cdot \Delta_a
</script>
Where:</p>
<ul>
<li><span class="arithmatex">\(N_T(a)\)</span>: Number of times arm <span class="arithmatex">\(a\)</span> is selected by time <span class="arithmatex">\(T\)</span>,</li>
<li><span class="arithmatex">\(\Delta_a = V^* - Q(a)\)</span>: Suboptimality gap.</li>
</ul>
<blockquote>
<p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>
</blockquote>
<h2 id="reinforcement-11_fast_rl-baseline-approaches-and-their-regret">Baseline Approaches and Their Regret<a class="headerlink" href="#reinforcement-11_fast_rl-baseline-approaches-and-their-regret" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-11_fast_rl-greedy-algorithm">Greedy Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-greedy-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^t r_\tau \cdot \mathbb{1}(a_\tau = a)
\]</div>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)
\]</div>
<h4 id="reinforcement-11_fast_rl-key-insight">Key Insight:<a class="headerlink" href="#reinforcement-11_fast_rl-key-insight" title="Permanent link">¶</a></h4>
<ul>
<li>Exploits current estimates.</li>
<li>May lock onto suboptimal arms due to early bad luck.</li>
<li>Linear regret in expectation.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-example">Example:<a class="headerlink" href="#reinforcement-11_fast_rl-example" title="Permanent link">¶</a></h3>
<p>If <span class="arithmatex">\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\)</span>, and the first sample of <span class="arithmatex">\(a_1\)</span> yields 0, the greedy agent may ignore it indefinitely.</p>
<h3 id="reinforcement-11_fast_rl-varepsilon-greedy-algorithm"><span class="arithmatex">\(\varepsilon\)</span>-Greedy Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-varepsilon-greedy-algorithm" title="Permanent link">¶</a></h3>
<p>At each timestep:</p>
<ul>
<li>With probability <span class="arithmatex">\(1 - \varepsilon\)</span>: exploit (<span class="arithmatex">\(\arg\max \hat{Q}_t(a)\)</span>),</li>
<li>With probability <span class="arithmatex">\(\varepsilon\)</span>: explore uniformly at random.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-performance">Performance:<a class="headerlink" href="#reinforcement-11_fast_rl-performance" title="Permanent link">¶</a></h4>
<ul>
<li>Guarantees exploration.</li>
<li>Linear regret unless <span class="arithmatex">\(\varepsilon\)</span> decays over time.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-decaying-varepsilon-greedy">Decaying <span class="arithmatex">\(\varepsilon\)</span>-Greedy<a class="headerlink" href="#reinforcement-11_fast_rl-decaying-varepsilon-greedy" title="Permanent link">¶</a></h3>
<p>Allows <span class="arithmatex">\(\varepsilon_t \to 0\)</span> as <span class="arithmatex">\(t \to \infty\)</span>, enabling convergence.</p>
<h2 id="reinforcement-11_fast_rl-optimism-in-the-face-of-uncertainty">Optimism in the Face of Uncertainty<a class="headerlink" href="#reinforcement-11_fast_rl-optimism-in-the-face-of-uncertainty" title="Permanent link">¶</a></h2>
<p>Prefer actions with uncertain but potentially high value:</p>
<p>Why? Two possible outcomes:</p>
<ol>
<li>
<p>Getting a high reward:    If the arm really has a high mean reward.</p>
</li>
<li>
<p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p>
</li>
</ol>
<p>Algorithm: </p>
<ul>
<li>
<p>Estimate an upper confidence bound <span class="arithmatex">\(U_t(a)\)</span> for each action value, such that   <span class="arithmatex">\(Q(a) \le U_t(a)\)</span> with high probability.</p>
</li>
<li>
<p>This depends on the number of times <span class="arithmatex">\(N_t(a)\)</span> action <span class="arithmatex">\(a\)</span> has been selected.</p>
</li>
<li>
<p>Select the action maximizing the Upper Confidence Bound (UCB):</p>
</li>
</ul>
<div class="arithmatex">\[a_t = \arg\max_{a \in \mathcal{A}} \left[ U_t(a) \right]\]</div>
<blockquote>
<p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards <span class="arithmatex">\(X_i \in [0,1]\)</span>,
<script type="math/tex; mode=display">
\mathbb{P}\!\left[ \mathbb{E}[X] > \bar{X}_n + u \right]
\;\le\; \exp(-2 n u^2).
</script>
</p>
<p>Setting the right-hand side equal to <span class="arithmatex">\(\delta\)</span> and solving for <span class="arithmatex">\(u\)</span>,
<script type="math/tex; mode=display">
u = \sqrt{\frac{\log(1/\delta)}{2n}}.
</script>
Here, <span class="arithmatex">\(\delta\)</span> is the failure probability, and the confidence interval
holds with probability at least <span class="arithmatex">\(1 - \delta\)</span>.
This means that, with probability at least <span class="arithmatex">\(1 - \delta\)</span>,
<script type="math/tex; mode=display">
\bar{X}_n - u \;\le\; \mathbb{E}[X] \;\le\; \bar{X}_n + u.
</script>
</p>
</blockquote>
<div class="arithmatex">\[
a_t = \arg\max_{a \in \mathcal{A}} \left[ \hat{Q}_t(a) + \text{UCB}_t(a) \right]
\]</div>
<h3 id="reinforcement-11_fast_rl-ucb1-algorithm">UCB1 Algorithm<a class="headerlink" href="#reinforcement-11_fast_rl-ucb1-algorithm" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\text{UCB}_t(a) = \hat{Q}_t(a) + \sqrt{\frac{2 \log \frac{1}{\delta} }{N_t(a)}}
\]</div>
<ul>
<li>where <span class="arithmatex">\(\hat{Q}_t(a)\)</span> is empirical average</li>
<li><span class="arithmatex">\(N_t(a)\)</span> is number of samples of <span class="arithmatex">\(a\)</span> after <span class="arithmatex">\(t\)</span> timesteps.</li>
<li>Provable sublinear regret.</li>
<li>Balances estimated value and exploration bonus.</li>
</ul>
<p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p>
<p>1: Initialize for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span>:  <span class="arithmatex">\(\quad N(a) \leftarrow 0,\;\; \hat{Q}(a) \leftarrow 0\)</span>
2: Warm start (sample each arm once):<br>
3: for each arm <span class="arithmatex">\(a \in \mathcal{A}\)</span> do<br>
4: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a\)</span>, observe reward <span class="arithmatex">\(r \in [0,1]\)</span><br>
5: <span class="arithmatex">\(\quad N(a) \leftarrow 1\)</span><br>
6: <span class="arithmatex">\(\quad \hat{Q}(a) \leftarrow r\)</span><br>
7: end for<br>
8: Set <span class="arithmatex">\(t \leftarrow |\mathcal{A}|\)</span></p>
<p>9: for <span class="arithmatex">\(t = |\mathcal{A}|+1, |\mathcal{A}|+2, \dots\)</span> do<br>
10: <span class="arithmatex">\(\quad\)</span> Compute UCB for each arm: <span class="arithmatex">\(\quad \mathrm{UCB}_t(a) = \hat{Q}(a) + \sqrt{\frac{2\log t}{N(a)}}\)</span></p>
<p>11: <span class="arithmatex">\(\quad\)</span> Select action:<span class="arithmatex">\(\quad a_t \leftarrow \arg\max_{a \in \mathcal{A}} \mathrm{UCB}_t(a)\)</span></p>
<p>12: <span class="arithmatex">\(\quad\)</span> Pull arm <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span></p>
<p>13: <span class="arithmatex">\(\quad\)</span> Update count: <span class="arithmatex">\(\quad N(a_t) \leftarrow N(a_t) + 1\)</span></p>
<p>14: <span class="arithmatex">\(\quad\)</span> Update empirical mean (incremental):<br>
<script type="math/tex; mode=display">
\quad \hat{Q}(a_t) \leftarrow \hat{Q}(a_t) + \frac{1}{N(a_t)}\Big(r_t - \hat{Q}(a_t)\Big)
</script>
</p>
<p>15: end for</p>
<h2 id="reinforcement-11_fast_rl-119-optimistic-initialization-in-greedy-bandit-algorithms">11.9 Optimistic Initialization in Greedy Bandit Algorithms<a class="headerlink" href="#reinforcement-11_fast_rl-119-optimistic-initialization-in-greedy-bandit-algorithms" title="Permanent link">¶</a></h2>
<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>
<h3 id="reinforcement-11_fast_rl-motivation">Motivation<a class="headerlink" href="#reinforcement-11_fast_rl-motivation" title="Permanent link">¶</a></h3>
<p>Greedy algorithms, by default, select actions with the highest estimated value:</p>
<div class="arithmatex">\[
a_t = \arg\max_a \hat{Q}_t(a)
\]</div>
<p>If these <span class="arithmatex">\(\hat{Q}_t(a)\)</span> estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>
<h3 id="reinforcement-11_fast_rl-algorithmic-details">Algorithmic Details<a class="headerlink" href="#reinforcement-11_fast_rl-algorithmic-details" title="Permanent link">¶</a></h3>
<p>We initialize:</p>
<ul>
<li><span class="arithmatex">\(\hat{Q}_0(a) = Q_{\text{init}}\)</span> for all <span class="arithmatex">\(a \in \mathcal{A}\)</span>, where <span class="arithmatex">\(Q_{\text{init}}\)</span> is set higher than any reasonable expected reward (e.g., <span class="arithmatex">\(Q_{\text{init}} = 1\)</span> if rewards are bounded in <span class="arithmatex">\([0, 1]\)</span>).</li>
<li><span class="arithmatex">\(N(a) = 1\)</span> to ensure initial update is well-defined.</li>
</ul>
<p>Then we update action values using an incremental Monte Carlo estimate:</p>
<div class="arithmatex">\[
\hat{Q}_{t}(a_t) = \hat{Q}_{t-1}(a_t) + \frac{1}{N_t(a_t)} \left( r_t - \hat{Q}_{t-1}(a_t) \right)
\]</div>
<p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p>
<ul>
<li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li>
<li>Simple to implement: No need for tuning <span class="arithmatex">\(\varepsilon\)</span> or computing uncertainty estimates.</li>
<li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-key-design-considerations">Key Design Considerations<a class="headerlink" href="#reinforcement-11_fast_rl-key-design-considerations" title="Permanent link">¶</a></h4>
<ul>
<li>How optimistic is optimistic enough?<br>
  If <span class="arithmatex">\(Q_{\text{init}}\)</span> is not much larger than the true values, the agent may not explore effectively.</li>
<li>What if <span class="arithmatex">\(Q_{\text{init}}\)</span> is too high?<br>
  Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li>
</ul>
<h4 id="reinforcement-11_fast_rl-function-approximation">Function Approximation<a class="headerlink" href="#reinforcement-11_fast_rl-function-approximation" title="Permanent link">¶</a></h4>
<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>
<h2 id="reinforcement-11_fast_rl-1110-theoretical-frameworks-regret-and-pac">11.10 Theoretical Frameworks: Regret and PAC<a class="headerlink" href="#reinforcement-11_fast_rl-1110-theoretical-frameworks-regret-and-pac" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-11_fast_rl-regret-based-evaluation">Regret-Based Evaluation<a class="headerlink" href="#reinforcement-11_fast_rl-regret-based-evaluation" title="Permanent link">¶</a></h3>
<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p>
<ul>
<li>Many small mistakes (frequent near-optimal actions),</li>
<li>A few large mistakes (infrequent but very suboptimal actions).</li>
</ul>
<p>Minimizing regret growth with <span class="arithmatex">\(T\)</span> is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>
<h3 id="reinforcement-11_fast_rl-probably-approximately-correct-pac-framework">Probably Approximately Correct (PAC) Framework<a class="headerlink" href="#reinforcement-11_fast_rl-probably-approximately-correct-pac-framework" title="Permanent link">¶</a></h3>
<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p>
<p>An algorithm is <span class="arithmatex">\((\varepsilon, \delta)\)</span>-PAC if, on each time step <span class="arithmatex">\(t\)</span>, it chooses an action <span class="arithmatex">\(a_t\)</span> such that:</p>
<div class="arithmatex">\[
Q(a_t) \ge Q(a^*) - \varepsilon \quad \text{with probability at least } 1 - \delta
\]</div>
<p>on all but a polynomial number of time steps (in <span class="arithmatex">\(|\mathcal{A}|\)</span>, <span class="arithmatex">\(1/\varepsilon\)</span>, <span class="arithmatex">\(1/\delta\)</span>, etc). This ensures:</p>
<ul>
<li>The agent almost always behaves nearly optimally,</li>
<li>With high probability, after a reasonable amount of time.</li>
</ul>
<p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>
<h2 id="reinforcement-11_fast_rl-comparing-exploration-strategies">Comparing Exploration Strategies<a class="headerlink" href="#reinforcement-11_fast_rl-comparing-exploration-strategies" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Regret Behavior</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Greedy</td>
<td>Linear</td>
<td>No exploration mechanism</td>
</tr>
<tr>
<td>Constant <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Linear</td>
<td>Fixed chance of exploring</td>
</tr>
<tr>
<td>Decaying <span class="arithmatex">\(\varepsilon\)</span>-greedy</td>
<td>Sublinear (if tuned)</td>
<td>Requires prior knowledge of reward gaps</td>
</tr>
<tr>
<td>Optimistic Initialization</td>
<td>Sublinear (if optimistic enough)</td>
<td>Simple, effective in tabular settings</td>
</tr>
</tbody>
</table>
<p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>
<h2 id="reinforcement-11_fast_rl-bayesian-bandits">Bayesian Bandits<a class="headerlink" href="#reinforcement-11_fast_rl-bayesian-bandits" title="Permanent link">¶</a></h2>
<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in <span class="arithmatex">\([0,1]\)</span>). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>
<h3 id="reinforcement-11_fast_rl-key-idea-maintain-beliefs-over-arm-reward-distributions">Key Idea: Maintain Beliefs Over Arm Reward Distributions<a class="headerlink" href="#reinforcement-11_fast_rl-key-idea-maintain-beliefs-over-arm-reward-distributions" title="Permanent link">¶</a></h3>
<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter <span class="arithmatex">\(\\phi_i\)</span> for arm <span class="arithmatex">\(i\)</span>. Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of <span class="arithmatex">\(\\phi_i\)</span>, representing our uncertainty.</p>
<h4 id="reinforcement-11_fast_rl-prior-and-posterior">Prior and Posterior<a class="headerlink" href="#reinforcement-11_fast_rl-prior-and-posterior" title="Permanent link">¶</a></h4>
<ul>
<li>Prior: Our initial belief about <span class="arithmatex">\(\\phi_i\)</span> is encoded in a probability distribution <span class="arithmatex">\(p(\\phi_i)\)</span>.</li>
<li>Data: After pulling arm <span class="arithmatex">\(i\)</span> and observing reward <span class="arithmatex">\(r_{i1}\)</span>, we update our belief.</li>
<li>Posterior: The new belief is computed using Bayes' rule:</li>
</ul>
<div class="arithmatex">\[p(\phi_i \mid r_{i1}) =
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
p(r_{i1})
}
=
\frac{
p(r_{i1} \mid \phi_i)\, p(\phi_i)
}{
\int p(r_{i1} \mid \phi_i)\, p(\phi_i)\, d\phi_i
}\]</div>
<p>This posterior becomes the new prior for future updates as more data arrives.</p>
<h3 id="reinforcement-11_fast_rl-practical-considerations">Practical Considerations<a class="headerlink" href="#reinforcement-11_fast_rl-practical-considerations" title="Permanent link">¶</a></h3>
<p>Computing the posterior <span class="arithmatex">\(p(\phi_i \mid D)\)</span> (where <span class="arithmatex">\(D\)</span> is the observed data for arm <span class="arithmatex">\(i\)</span>) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p>
<ul>
<li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li>
<li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li>
</ul>
<h3 id="reinforcement-11_fast_rl-why-use-bayesian-bandits">Why Use Bayesian Bandits?<a class="headerlink" href="#reinforcement-11_fast_rl-why-use-bayesian-bandits" title="Permanent link">¶</a></h3>
<ul>
<li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li>
<li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li>
<li>Captures uncertainty in a principled and statistically coherent manner.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-summary">Summary<a class="headerlink" href="#reinforcement-11_fast_rl-summary" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian bandits treat the reward-generating parameters <span class="arithmatex">\(\phi_i\)</span> as random variables.</li>
<li>We maintain a posterior belief <span class="arithmatex">\(p(\phi_i \mid D)\)</span> using Bayes' rule.</li>
<li>When conjugate priors are used, analytical updates are possible.</li>
<li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li>
</ul>
<h3 id="reinforcement-11_fast_rl-thompson-sampling">Thompson Sampling:<a class="headerlink" href="#reinforcement-11_fast_rl-thompson-sampling" title="Permanent link">¶</a></h3>
<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p>
<p>This approach is also known as <em>probability matching</em>: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn’t deterministically select the arm with the highest mean—it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p>
<p>Algorithm: Thompson Sampling:</p>
<p>1: Initialize prior over each arm <span class="arithmatex">\(a\)</span>, <span class="arithmatex">\(p(\mathcal{R}_a)\)</span><br>
2: for iteration <span class="arithmatex">\(= 1, 2, \dots\)</span> do<br>
3: <span class="arithmatex">\(\quad\)</span> For each arm <span class="arithmatex">\(a\)</span> sample a reward distribution <span class="arithmatex">\(\mathcal{R}_a\)</span> from posterior<br>
4: <span class="arithmatex">\(\quad\)</span> Compute action-value function <span class="arithmatex">\(Q(a) = \mathbb{E}[\mathcal{R}_a]\)</span><br>
5: <span class="arithmatex">\(\quad a_t \equiv \arg\max_{a \in \mathcal{A}} Q(a)\)</span><br>
6: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r\)</span><br>
7: <span class="arithmatex">\(\quad\)</span> Update posterior <span class="arithmatex">\(p(\mathcal{R}_a)\)</span> using Bayes Rule<br>
8: end for  </p>
<h3 id="reinforcement-11_fast_rl-contextual-bandits">Contextual Bandits<a class="headerlink" href="#reinforcement-11_fast_rl-contextual-bandits" title="Permanent link">¶</a></h3>
<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context <span class="arithmatex">\(x_t\)</span> drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p>
<p>In this setting, the goal is to learn a context-dependent policy <span class="arithmatex">\(\pi(a \mid x)\)</span> that maps the observed context <span class="arithmatex">\(x_t\)</span> to a suitable arm <span class="arithmatex">\(a_t\)</span>, maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p>
<p>Formally, the interaction at each time step <span class="arithmatex">\(t\)</span> is:</p>
<ol>
<li>Observe context <span class="arithmatex">\(x_t \in \mathcal{X}\)</span></li>
<li>Choose action <span class="arithmatex">\(a_t \in \mathcal{A}\)</span> based on policy <span class="arithmatex">\(\pi(a \mid x_t)\)</span></li>
<li>Receive reward <span class="arithmatex">\(r_t(a_t, x_t)\)</span></li>
</ol>
<p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p>
<div class="arithmatex">\[
\pi^*(x) = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a, x)]
\]</div>
<p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>
<h2 id="reinforcement-11_fast_rl-mental-map">Mental Map<a class="headerlink" href="#reinforcement-11_fast_rl-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                Bandits: Foundations of Data-Efficient RL
     Goal: Understand exploration-exploitation in simplest setting
           Learn to act with minimal data through principled tradeoffs
                                │
                                ▼
               What Are Multi-Armed Bandits (MAB)?
 ┌─────────────────────────────────────────────────────────────┐
 │ Single-state (stateless) decision problems                  │
 │ Fixed set of actions (arms)                                 │
 │ Unknown reward distribution per arm                         │
 │ Choose an action, receive reward, repeat                    │
 │ No transition dynamics — unlike full RL                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Core Objective: Maximize Reward
 ┌─────────────────────────────────────────────────────────────┐
 │ Maximize total reward = minimize regret                     │
 │ Regret = missed opportunity vs optimal action               │
 │ Total regret used to evaluate algorithm efficiency          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                  Basic Bandit Algorithms
 ┌─────────────────────────────────────────────────────────────┐
 │ Greedy: exploit current best estimates (linear regret)      │
 │ ε-Greedy: random exploration with fixed ε                   │
 │ Decaying ε-Greedy: reduces ε over time                      │
 │ Optimistic Initialization: set high initial Q̂ values        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Principle: Optimism in the Face of Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat unvisited arms as potentially good                    │
 │ Upper Confidence Bound (UCB) algorithms                     │
 │ Tradeoff: mean reward + exploration bonus                   │
 │ Guarantees sublinear regret                                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Algorithmic Realization: UCB1
 ┌─────────────────────────────────────────────────────────────┐
 │ UCB_t(a) = Q̂_t(a) + √(2 log t / N_t(a))                     │
 │ Encourages pulling uncertain arms early                     │
 │ Regret ≈ O(√(T log T))                                      │
 │ Theoretically grounded and simple to implement              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Theoretical Frameworks: Regret vs PAC
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap from always acting optimally         │
 │ PAC: guarantees near-optimal behavior with high probability │
 │ Regret cares about sum of mistakes; PAC focuses on steps    │
 │ Both evaluate quality and efficiency of learning            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                Bayesian Bandits and Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat arm rewards as random variables                       │
 │ Use prior + observed data → posterior via Bayes rule        │
 │ Conjugate priors simplify computation                       │
 │ Enable principled uncertainty reasoning                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Thompson Sampling (Bayesian)
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample reward distribution from posterior per arm           │
 │ Pull arm with highest sampled reward                        │
 │ Probabilistic optimism: match probability of being best     │
 │ Natural exploration and strong empirical performance        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Probability Matching Perspective
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ sample optimal arm w/ correct frequency │
 │ Avoids hard-coded uncertainty bonuses                       │
 │ Simpler and often better in practice                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                       Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Input context x_t at each timestep                          │
 │ Reward distribution depends on (action, context)            │
 │ Learn policy π(a | x): context-aware decision making        │
 │ Real-world applications: ads, medicine, personalization     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Summary: Bandits as Foundation for Efficient RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Bandits isolate the exploration-exploitation tradeoff       │
 │ Simpler than full RL, but deeply insightful                 │
 │ Concepts generalize to value estimation, uncertainty        │
 │ Key tools: regret, PAC bounds, posterior reasoning          │
 └─────────────────────────────────────────────────────────────┘
</code></pre></div></body></html></section><section class="print-page" id="reinforcement-12_fast_mdps" heading-number="2.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-12-fast-reinforcement-learning-in-mdps-and-generalization">Chapter 12: Fast Reinforcement Learning in MDPs and Generalization<a class="headerlink" href="#reinforcement-12_fast_mdps-chapter-12-fast-reinforcement-learning-in-mdps-and-generalization" title="Permanent link">¶</a></h1>
<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p>
<ul>
<li>Bandits: Single-step decision-making problems.</li>
<li>MDPs: Sequential decision-making with transition dynamics.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-evaluation-frameworks">Evaluation Frameworks<a class="headerlink" href="#reinforcement-12_fast_mdps-evaluation-frameworks" title="Permanent link">¶</a></h3>
<p>To assess learning efficiency, we use:</p>
<ul>
<li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li>
<li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li>
<li>PAC (Probably Approximately Correct): Number of steps when the policy is not <span class="arithmatex">\(\epsilon\)</span>-optimal is bounded with high probability.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-exploration-approaches">Exploration Approaches<a class="headerlink" href="#reinforcement-12_fast_mdps-exploration-approaches" title="Permanent link">¶</a></h3>
<ul>
<li>Optimism under uncertainty (e.g., UCB)</li>
<li>Probability matching (e.g., Thompson Sampling)</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-pac-framework-for-mdps">PAC Framework for MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-pac-framework-for-mdps" title="Permanent link">¶</a></h2>
<p>A reinforcement learning algorithm <span class="arithmatex">\(A\)</span> is PAC if with probability at least <span class="arithmatex">\(1 - \delta\)</span>, it selects an <span class="arithmatex">\(\epsilon\)</span>-optimal action on all but a bounded number of time steps <span class="arithmatex">\(N\)</span>, where:</p>
<div class="arithmatex">\[
N = \text{poly} \left( |S|, |A|, \frac{1}{1 - \gamma}, \frac{1}{\epsilon}, \frac{1}{\delta} \right)
\]</div>
<h2 id="reinforcement-12_fast_mdps-mbie-eb-model-based-interval-estimation-with-exploration-bonus">MBIE-EB: Model-Based Interval Estimation with Exploration Bonus<a class="headerlink" href="#reinforcement-12_fast_mdps-mbie-eb-model-based-interval-estimation-with-exploration-bonus" title="Permanent link">¶</a></h2>
<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function <span class="arithmatex">\(Q(s, a)\)</span>.</p>
<p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p>
<ul>
<li>
<p>It estimates <span class="arithmatex">\(\hat{R}(s, a)\)</span> and <span class="arithmatex">\(\hat{T}(s' \mid s, a)\)</span> from data using empirical counts.</p>
</li>
<li>
<p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p>
</li>
</ul>
<p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p>
<p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p>
<p>Algorithm:</p>
<p>1: Given <span class="arithmatex">\(\epsilon\)</span>, <span class="arithmatex">\(\delta\)</span>, <span class="arithmatex">\(m\)</span><br>
2: <span class="arithmatex">\(\beta = \dfrac{1}{1-\gamma}\sqrt{0.5 \ln \!\left(\dfrac{2|S||A|m}{\delta}\right)}\)</span><br>
3: <span class="arithmatex">\(n_{sas}(s,a,s') = 0\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A, s' \in S\)</span><br>
4: <span class="arithmatex">\(rc(s,a) = 0\)</span>, <span class="arithmatex">\(n_{sa}(s,a) = 0\)</span>, <span class="arithmatex">\(\hat{Q}(s,a) = \dfrac{1}{1-\gamma}\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A\)</span><br>
5: <span class="arithmatex">\(t = 0\)</span>, <span class="arithmatex">\(s_t = s_{\text{init}}\)</span><br>
6: loop<br>
7: <span class="arithmatex">\(\quad a_t = \arg\max_{a \in A} \hat{Q}(s_t, a)\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and state <span class="arithmatex">\(s_{t+1}\)</span><br>
9: <span class="arithmatex">\(\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\)</span>,<br>
<span class="arithmatex">\(\quad\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\)</span><br>
10: <span class="arithmatex">\(\quad rc(s_t,a_t) = \dfrac{rc(s_t,a_t)\big(n_{sa}(s_t,a_t)-1\big) + r_t}{n_{sa}(s_t,a_t)}\)</span><br>
11: <span class="arithmatex">\(\quad \hat{R}(s_t,a_t) = rc(s_t,a_t)\)</span> and<br>
<span class="arithmatex">\(\quad\quad \hat{T}(s' \mid s_t,a_t) = \dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\)</span>, <span class="arithmatex">\(\forall s' \in S\)</span><br>
12: <span class="arithmatex">\(\quad\)</span> while not converged do<br>
13: <span class="arithmatex">\(\quad\quad \hat{Q}(s,a) = \hat{R}(s,a) + \gamma \sum_{s'} \hat{T}(s' \mid s,a)\max_{a'} \hat{Q}(s',a') + \dfrac{\beta}{\sqrt{n_{sa}(s,a)}}\)</span>,<br>
<span class="arithmatex">\(\quad\quad\quad \forall s \in S, a \in A\)</span><br>
14: <span class="arithmatex">\(\quad\)</span> end while<br>
15: end loop</p>
<h2 id="reinforcement-12_fast_mdps-bayesian-model-based-reinforcement-learning">Bayesian Model-Based Reinforcement Learning<a class="headerlink" href="#reinforcement-12_fast_mdps-bayesian-model-based-reinforcement-learning" title="Permanent link">¶</a></h2>
<p>Bayesian RL methods maintain a posterior over MDP models <span class="arithmatex">\((P, R)\)</span> and sample plausible environments from the posterior to plan and act.</p>
<p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p>
<p>Formally, the Thompson sampling policy is:</p>
<div class="arithmatex">\[
\pi(s, a \mid h_t) = \mathbb{P}\left(Q(s, a) \ge Q(s, a'),\; \forall a' \ne a \;\middle|\; h_t \right)
= \mathbb{E}_{\mathcal{P}, \mathcal{R} \mid h_t} \left[ \mathbb{1}\left(a = \arg\max_{a \in \mathcal{A}} Q(s, a)\right) \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(h_t\)</span> is the history up to time <span class="arithmatex">\(t\)</span> (including all observed transitions and rewards),
- <span class="arithmatex">\(\mathcal{P}, \mathcal{R}\)</span> are the transition and reward functions respectively,
- The expectation is taken over the posterior belief on the MDP <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span>.</p>
<h3 id="reinforcement-12_fast_mdps-thompson-sampling-algorithm-in-mdps">Thompson Sampling Algorithm in MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-thompson-sampling-algorithm-in-mdps" title="Permanent link">¶</a></h3>
<ol>
<li>Maintain a posterior <span class="arithmatex">\(p(\mathcal{P}, \mathcal{R} \mid h_t)\)</span> over the transition and reward models based on all observed data.</li>
<li>Sample a model <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span> from the posterior distribution.</li>
<li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function <span class="arithmatex">\(Q^*(s, a)\)</span>.</li>
<li>Select the action according to the optimal action in the sampled model:
   <script type="math/tex; mode=display">
   a_t = \arg\max_{a \in \mathcal{A}} Q^*(s_t, a)
   </script>
</li>
</ol>
<h3 id="reinforcement-12_fast_mdps-algorithm-thompson-sampling-for-mdps">Algorithm: Thompson Sampling for MDPs<a class="headerlink" href="#reinforcement-12_fast_mdps-algorithm-thompson-sampling-for-mdps" title="Permanent link">¶</a></h3>
<p>1: Initialize prior over dynamics and reward models for each <span class="arithmatex">\((s, a)\)</span>:  <span class="arithmatex">\(\quad p(\mathcal{T}(s' \mid s, a)), \quad p(\mathcal{R}(s, a))\)</span><br>
2: Initialize initial state <span class="arithmatex">\(s_0\)</span><br>
3: for <span class="arithmatex">\(k = 1\)</span> to <span class="arithmatex">\(K\)</span> episodes do<br>
4: <span class="arithmatex">\(\quad\)</span> Sample an MDP <span class="arithmatex">\(\mathcal{M}\)</span>:<br>
5: <span class="arithmatex">\(\quad\quad\)</span> for each <span class="arithmatex">\((s, a)\)</span> pair do<br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample transition model <span class="arithmatex">\(\mathcal{T}(s' \mid s, a)\)</span> from posterior<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample reward model <span class="arithmatex">\(\mathcal{R}(s, a)\)</span> from posterior<br>
8: <span class="arithmatex">\(\quad\quad\)</span> end for<br>
9: <span class="arithmatex">\(\quad\)</span> Compute optimal value function <span class="arithmatex">\(Q_{\mathcal{M}}^*\)</span> for sampled MDP <span class="arithmatex">\(\mathcal{M}\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1\)</span> to <span class="arithmatex">\(H\)</span> do<br>
11: <span class="arithmatex">\(\quad\quad a_t = \arg\max_{a \in \mathcal{A}} Q_{\mathcal{M}}^*(s_t, a)\)</span><br>
12: <span class="arithmatex">\(\quad\quad\)</span> Take action <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span><br>
13: <span class="arithmatex">\(\quad\)</span> end for<br>
14: <span class="arithmatex">\(\quad\)</span> Update posteriors: <span class="arithmatex">\(\quad\quad p(\mathcal{R}_{s_t, a_t} \mid r_t), \quad p(\mathcal{T}(s' \mid s_t, a_t) \mid s_{t+1})\)</span> using Bayes Rule<br>
15: end for</p>
<h2 id="reinforcement-12_fast_mdps-key-characteristics">Key Characteristics<a class="headerlink" href="#reinforcement-12_fast_mdps-key-characteristics" title="Permanent link">¶</a></h2>
<ul>
<li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li>
<li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li>
<li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li>
</ul>
<blockquote>
<p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>
</blockquote>
<h2 id="reinforcement-12_fast_mdps-generalization-in-contextual-bandits">Generalization in Contextual Bandits<a class="headerlink" href="#reinforcement-12_fast_mdps-generalization-in-contextual-bandits" title="Permanent link">¶</a></h2>
<p>Contextual bandits generalize standard bandits by associating a context or state <span class="arithmatex">\(s\)</span> with each decision:</p>
<ul>
<li>Reward depends on both context and action: <span class="arithmatex">\(r \sim P[r | s,a]\)</span></li>
<li>Often model reward as linear: <span class="arithmatex">\(r = \theta^\top \phi(s,a) + \epsilon\)</span>, with <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span></li>
</ul>
<h3 id="reinforcement-12_fast_mdps-benefits-of-generalization">Benefits of Generalization<a class="headerlink" href="#reinforcement-12_fast_mdps-benefits-of-generalization" title="Permanent link">¶</a></h3>
<ul>
<li>Allows learning across states/actions</li>
<li>Enables sample-efficient exploration in large state/action spaces</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-strategic-exploration-in-deep-rl">Strategic Exploration in Deep RL<a class="headerlink" href="#reinforcement-12_fast_mdps-strategic-exploration-in-deep-rl" title="Permanent link">¶</a></h2>
<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>
<h3 id="reinforcement-12_fast_mdps-optimistic-q-learning-with-function-approximation">Optimistic Q-Learning with Function Approximation<a class="headerlink" href="#reinforcement-12_fast_mdps-optimistic-q-learning-with-function-approximation" title="Permanent link">¶</a></h3>
<p>Modified Q-learning update:</p>
<div class="arithmatex">\[
\Delta w = \alpha \left( r + r_{\text{bonus}}(s,a) + \gamma \max_{a'} Q(s', a'; w) - Q(s,a;w) \right) \nabla_w Q(s,a;w)
\]</div>
<p>Bonus <span class="arithmatex">\(r_{\text{bonus}}\)</span> reflects novelty or epistemic uncertainty.</p>
<h3 id="reinforcement-12_fast_mdps-count-based-and-density-based-exploration">Count-Based and Density-Based Exploration<a class="headerlink" href="#reinforcement-12_fast_mdps-count-based-and-density-based-exploration" title="Permanent link">¶</a></h3>
<ul>
<li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li>
<li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li>
<li>Tang et al. (2017) use hashing-based counts.</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-thompson-sampling-for-deep-rl">Thompson Sampling for Deep RL<a class="headerlink" href="#reinforcement-12_fast_mdps-thompson-sampling-for-deep-rl" title="Permanent link">¶</a></h2>
<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>
<h3 id="reinforcement-12_fast_mdps-bootstrapped-dqn">Bootstrapped DQN<a class="headerlink" href="#reinforcement-12_fast_mdps-bootstrapped-dqn" title="Permanent link">¶</a></h3>
<ul>
<li>Train multiple Q-networks on bootstrapped datasets.</li>
<li>Select one head randomly at each episode for exploration.</li>
</ul>
<h3 id="reinforcement-12_fast_mdps-bayesian-deep-q-networks">Bayesian Deep Q-Networks<a class="headerlink" href="#reinforcement-12_fast_mdps-bayesian-deep-q-networks" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian linear regression on final layer</li>
<li>Posterior used to sample Q-values, enabling optimism</li>
<li>Outperforms naive bootstrapped DQNs in some settings</li>
</ul>
<h2 id="reinforcement-12_fast_mdps-mental-map">Mental Map<a class="headerlink" href="#reinforcement-12_fast_mdps-mental-map" title="Permanent link">¶</a></h2>
<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization
  Goal: Learn near-optimal policies in MDPs with limited data
    Extend bandit exploration ideas to sequential decision making
                            │
                            ▼
             Why MDPs Are Harder Than Bandits
</code></pre>
<p>┌─────────────────────────────────────────────────────────────┐
 │ MDPs involve sequential decisions with transitions           │
 │ Agent must explore over states and transitions              │
 │ Exploration affects future knowledge &amp; rewards              │
 │ Sample inefficiency is a major practical bottleneck         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Evaluation Frameworks for RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap vs optimal policy over time          │
 │ PAC (Probably Approximately Correct):                       │
 │   Guarantees ε-optimality with high probability             │
 │ Bayesian Regret: expected regret under prior over MDPs      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              PAC Learning in MDPs: Formal Guarantee
 ┌─────────────────────────────────────────────────────────────┐
 │ Algorithm is PAC if all but N steps are ε-optimal           │
 │ N = poly(|S|, |A|, 1/(1-γ), 1/ε, 1/δ)                        │
 │ Ensures high-probability performance bounds                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Optimism: MBIE-EB Algorithm (Model-Based)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate reward + transitions from data                     │
 │ Add bonus to Q-values: encourages actions with high uncertainty │
 │ Optimistic model induces exploration                        │
 │ Dynamic programming over Q̂ + bonus → exploration policy     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Algorithmic Principle: Optimism Under Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Add uncertainty-driven bonus to reward or Q-value           │
 │ Drives exploration to unknown regions                       │
 │ Simple but effective in tabular MDPs                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian RL and Posterior Sampling
 ┌─────────────────────────────────────────────────────────────┐
 │ Maintain belief (posterior) over MDP model (P, R)           │
 │ Sample MDP from posterior → plan optimally in sampled MDP   │
 │ Leads to probability matching via Thompson Sampling         │
 │ Posterior concentrates with data → convergence to optimal   │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Algorithm: Thompson Sampling in Model-Based RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample dynamics + rewards from posterior                    │
 │ Solve sampled MDP for optimal Q<em>                            │
 │ Act according to Q</em> in sample MDP                           │
 │ Update posterior using Bayes rule after each step           │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Exploration via Posterior Variance (Bayes)
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ Probability Matching                    │
 │ Probabilistically favors optimal but uncertain policies     │
 │ Elegant &amp; adaptive exploration                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Generalization via Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Rewards depend on both context and action                   │
 │ Learn generalizable function: Q(s,a) or π(a|s)              │
 │ Enables learning across states / actions                    │
 │ Use linear models or embeddings: φ(s,a)                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
         Exploration + Generalization in Deep RL Settings
 ┌─────────────────────────────────────────────────────────────┐
 │ Optimistic Q-learning: add r_bonus(s,a) in TD target        │
 │ r_bonus from novelty, density models, or uncertainty        │
 │ Count-based, hashing, or learned density bonuses            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian Deep RL: Posterior Approximation
 ┌─────────────────────────────────────────────────────────────┐
 │ Bootstrapped DQN: ensemble of Q-networks for exploration    │
 │ Bayesian DQN: sample from approximate Q-posteriors          │
 │ Enables implicit Thompson-like behavior                     │
 │ Scales to high-dimensional state/action spaces              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                         Chapter Summary
 ┌─────────────────────────────────────────────────────────────┐
 │ Strategic exploration = key to fast learning in MDPs        │
 │ Optimism (MBIE-EB) and Bayesian methods (Thompson)          │
 │ PAC and Bayesian regret are key evaluation tools            │
 │ Generalization (via features or deep nets) enables scaling  │
 │ Thompson Sampling and bootstrapped approximations bridge gap│
 │ Between tabular and high-dimensional RL                     │
 └─────────────────────────────────────────────────────────────┘
````</p></body></html></section><section class="print-page" id="reinforcement-13_montecarlo" heading-number="2.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-13-monte-carlo-tree-search">Chapter 13: Monte Carlo Tree Search<a class="headerlink" href="#reinforcement-13_montecarlo-chapter-13-monte-carlo-tree-search" title="Permanent link">¶</a></h1>
<p>Monte Carlo Tree Search (MCTS) is a powerful planning algorithm that uses simulation-based search to select actions in complex decision-making problems. It is especially effective in large or unknown environments where exact planning is infeasible. MCTS balances exploration and exploitation through sampling and is the backbone of major AI breakthroughs like AlphaGo and AlphaZero.</p>
<h2 id="reinforcement-13_montecarlo-131-motivation">13.1 Motivation<a class="headerlink" href="#reinforcement-13_montecarlo-131-motivation" title="Permanent link">¶</a></h2>
<p>In classical reinforcement learning (RL), agents often compute policies over the <em>entire</em> state space. MCTS takes a different approach: it performs local search from the current state, using simulated episodes to estimate action values and make near-optimal decisions <em>on the fly</em>.</p>
<p>This method is particularly useful in:</p>
<ul>
<li>Large state/action spaces</li>
<li>Games with high branching factor (e.g., Go, Chess)</li>
<li>Black-box or simulator-only environments</li>
</ul>
<h2 id="reinforcement-13_montecarlo-132-monte-carlo-search">13.2 Monte Carlo Search<a class="headerlink" href="#reinforcement-13_montecarlo-132-monte-carlo-search" title="Permanent link">¶</a></h2>
<p>A simple Monte Carlo search uses a model <span class="arithmatex">\(\mathcal{M}\)</span> (dynamics and resward model) and a rollout policy <span class="arithmatex">\(\pi\)</span> to simulate <span class="arithmatex">\(K\)</span> trajectories for each action <span class="arithmatex">\(a\)</span> from the current state <span class="arithmatex">\(s_t\)</span>:</p>
<ol>
<li>Simulate episodes <span class="arithmatex">\(\{s_t, a, r_{t+1}^{(k)}, \ldots, s_T^{(k)}\}\)</span> from <span class="arithmatex">\(\mathcal{M}, \pi\)</span>.</li>
<li>Estimate <span class="arithmatex">\(Q(s_t, a)\)</span> via sample average:</li>
</ol>
<div class="arithmatex">\[
Q(s_t, a) = \frac{1}{K} \sum_{k=1}^K G_t^{(k)} \rightarrow q^\pi(s_t, a)
\]</div>
<ol>
<li>Select the best action:</li>
</ol>
<div class="arithmatex">\[
a_t = \arg\max_a Q(s_t, a)
\]</div>
<p>This performs one-step policy improvement, but does not build deeper search trees.</p>
<h2 id="reinforcement-13_montecarlo-133-expectimax-search">13.3 Expectimax Search<a class="headerlink" href="#reinforcement-13_montecarlo-133-expectimax-search" title="Permanent link">¶</a></h2>
<p>To go beyond single-step rollouts, expectimax trees compute <span class="arithmatex">\(Q^*(s, a)\)</span> recursively using the model:</p>
<ul>
<li>Each node expands by looking ahead using the transition model.</li>
<li>Combines maximization (over actions) and expectation (over next states).</li>
<li>Forward search avoids solving the entire MDP and focuses only on the subtree starting at <span class="arithmatex">\(s_t\)</span>.</li>
</ul>
<p>However, the number of nodes grows exponentially with horizon <span class="arithmatex">\(H\)</span>: <span class="arithmatex">\(O(|S||A|)^H\)</span>.</p>
<h2 id="reinforcement-13_montecarlo-134-monte-carlo-tree-search-mcts">13.4 Monte Carlo Tree Search (MCTS)<a class="headerlink" href="#reinforcement-13_montecarlo-134-monte-carlo-tree-search-mcts" title="Permanent link">¶</a></h2>
<p>MCTS improves on expectimax by sampling rather than fully expanding the tree:</p>
<ol>
<li>Build a tree rooted at current state <span class="arithmatex">\(s_t\)</span>.</li>
<li>Perform <span class="arithmatex">\(K\)</span> simulations to expand and update parts of the tree.</li>
<li>Estimate <span class="arithmatex">\(Q(s, a)\)</span> using sampled returns.</li>
<li>Select the best action at the root:</li>
</ol>
<div class="arithmatex">\[
a_t = \arg\max_a Q(s_t, a)
\]</div>
<h2 id="reinforcement-13_montecarlo-135-upper-confidence-tree-uct">13.5 Upper Confidence Tree (UCT)<a class="headerlink" href="#reinforcement-13_montecarlo-135-upper-confidence-tree-uct" title="Permanent link">¶</a></h2>
<p>A key challenge in MCTS is deciding which action to simulate at each tree node. UCT addresses this by treating each decision as a multi-armed bandit problem and using an Upper Confidence Bound:</p>
<div class="arithmatex">\[
Q(s, a, i) = \underbrace{\frac{1}{N(i, a)} \sum_{k=1}^{N(i,a)} G_k(i,a)}_{\text{Mean Return}} + \underbrace{c \sqrt{\frac{\log N(i)}{N(i, a)}}}_{\text{Exploration Bonus}}
\]</div>
<ul>
<li><span class="arithmatex">\(N(i, a)\)</span>: number of times action <span class="arithmatex">\(a\)</span> taken at node <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(N(i)\)</span>: total visits to node <span class="arithmatex">\(i\)</span></li>
<li><span class="arithmatex">\(c\)</span>: exploration constant</li>
<li><span class="arithmatex">\(G_k(i, a)\)</span>: return from simulation <span class="arithmatex">\(k\)</span> for <span class="arithmatex">\((i, a)\)</span></li>
</ul>
<p>Action selection:</p>
<div class="arithmatex">\[
a_k^i = \arg\max_a Q(s, a, i)
\]</div>
<p>This balances exploitation of known good actions and exploration of uncertain ones.</p>
<h2 id="reinforcement-13_montecarlo-136-advantages-of-mcts">13.6 Advantages of MCTS<a class="headerlink" href="#reinforcement-13_montecarlo-136-advantages-of-mcts" title="Permanent link">¶</a></h2>
<ul>
<li>Anytime: Can stop search at any time and use the best estimates so far.</li>
<li>Model-based or black-box: Only needs sample access to the environment.</li>
<li>Best-first: Focuses computation on promising actions.</li>
<li>Scalable: Avoids full enumeration of action/state spaces.</li>
<li>Parallelizable: Independent simulations can be run in parallel.</li>
</ul>
<h2 id="reinforcement-13_montecarlo-137-alphazero-and-deep-mcts">13.7 AlphaZero and Deep MCTS<a class="headerlink" href="#reinforcement-13_montecarlo-137-alphazero-and-deep-mcts" title="Permanent link">¶</a></h2>
<p>AlphaZero revolutionized game-playing AI by combining deep learning with MCTS. Key ideas:</p>
<h3 id="reinforcement-13_montecarlo-policy-and-value-networks">Policy and Value Networks<a class="headerlink" href="#reinforcement-13_montecarlo-policy-and-value-networks" title="Permanent link">¶</a></h3>
<p>A neural network <span class="arithmatex">\(f_\theta(s)\)</span> outputs:</p>
<ul>
<li><span class="arithmatex">\(P\)</span>: action probabilities</li>
<li><span class="arithmatex">\(V\)</span>: value estimate</li>
</ul>
<div class="arithmatex">\[
(p, v) = f_\theta(s)
\]</div>
<h3 id="reinforcement-13_montecarlo-alphazero-mcts-steps">AlphaZero MCTS Steps<a class="headerlink" href="#reinforcement-13_montecarlo-alphazero-mcts-steps" title="Permanent link">¶</a></h3>
<ol>
<li>Select: Traverse tree using <span class="arithmatex">\(Q + U\)</span> to choose child nodes.</li>
<li>Expand: Add a new node, initialized with <span class="arithmatex">\(P\)</span> from <span class="arithmatex">\(f_\theta\)</span>.</li>
<li>Evaluate: Use <span class="arithmatex">\(v\)</span> from the network as the value of the leaf.</li>
<li>Backup: Propagate value estimates up the tree.</li>
<li>Repeat: Perform many rollouts to refine the tree.</li>
</ol>
<h3 id="reinforcement-13_montecarlo-root-action-selection">Root Action Selection<a class="headerlink" href="#reinforcement-13_montecarlo-root-action-selection" title="Permanent link">¶</a></h3>
<p>At the root, use visit counts <span class="arithmatex">\(N(s,a)\)</span> to compute the improved policy:</p>
<div class="arithmatex">\[
\pi(s, a) \propto N(s, a)^{1/\tau}
\]</div>
<p>where <span class="arithmatex">\(\tau\)</span> controls exploration vs exploitation.</p>
<h2 id="reinforcement-13_montecarlo-138-self-play-and-training">13.8 Self-Play and Training<a class="headerlink" href="#reinforcement-13_montecarlo-138-self-play-and-training" title="Permanent link">¶</a></h2>
<p>AlphaZero uses self-play to generate training data:</p>
<ol>
<li>Play full games using MCTS.</li>
<li>Record <span class="arithmatex">\((s, \pi, z)\)</span> tuples where:</li>
<li><span class="arithmatex">\(s\)</span>: game state</li>
<li><span class="arithmatex">\(\pi\)</span>: improved policy from MCTS</li>
<li><span class="arithmatex">\(z\)</span>: final game outcome</li>
<li>Train <span class="arithmatex">\(f_\theta\)</span> to minimize combined loss:</li>
</ol>
<div class="arithmatex">\[
\mathcal{L} = (z - v)^2 - \pi^\top \log p + \lambda \|\theta\|^2
\]</div>
<p>This allows continual improvement without human supervision.</p>
<h2 id="reinforcement-13_montecarlo-139-evaluation-and-impact">13.9 Evaluation and Impact<a class="headerlink" href="#reinforcement-13_montecarlo-139-evaluation-and-impact" title="Permanent link">¶</a></h2>
<ul>
<li>MCTS dramatically improves performance over raw policy/value networks.</li>
<li>Essential to surpassing human performance in Go, Chess, and Shogi.</li>
<li>Eliminates the need for human expert data.</li>
</ul>
<p>Insights:</p>
<ul>
<li>UCT enables principled tree search with exploration.</li>
<li>Neural nets guide and accelerate MCTS.</li>
<li>MCTS can be used in any environment where lookahead is possible.</li>
</ul>
<h2 id="reinforcement-13_montecarlo-1310-summary">13.10 Summary<a class="headerlink" href="#reinforcement-13_montecarlo-1310-summary" title="Permanent link">¶</a></h2>
<ul>
<li>MCTS uses simulation-based planning with a growing search tree.</li>
<li>UCT adds upper confidence bounds to balance exploration/exploitation.</li>
<li>AlphaZero combines MCTS with deep learning for superhuman performance.</li>
<li>Self-play enables autonomous training without labeled data.</li>
</ul>
<p>MCTS represents a powerful bridge between planning and learning, enabling agents to make strong decisions under uncertainty in complex domains.</p></body></html></section><section class="print-page" id="reinforcement-14_final" heading-number="2.14"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-14-c">Chapter 14: c<a class="headerlink" href="#reinforcement-14_final-chapter-14-c" title="Permanent link">¶</a></h1>
<p>In this final chapter, we recap the journey of reinforcement learning (RL) from its foundational ideas in multi-armed bandits through to the cutting-edge of deep RL. Along the way we will revisit key algorithmic concepts – including Upper Confidence Bounds (UCB), Thompson Sampling, Model-Based Interval Estimation with Exploration Bonus (MBIE-EB), and Monte Carlo Tree Search (MCTS) – and highlight how different approaches to exploration (optimism vs. probability matching) have shaped the field. We will also emphasize the theoretical foundations of RL (regret minimization, PAC guarantees, Bayesian methods) and illustrate how these principles connect to real-world successes like AlphaTensor and ChatGPT. Throughout, the aim is to provide a high-level summary and synthesis, reinforcing the insights gained across previous chapters.</p>
<h2 id="reinforcement-14_final-recap-from-bandits-to-deep-reinforcement-learning">Recap: From Bandits to Deep Reinforcement Learning<a class="headerlink" href="#reinforcement-14_final-recap-from-bandits-to-deep-reinforcement-learning" title="Permanent link">¶</a></h2>
<p>Reinforcement learning can be defined as learning through experience (data) to make good decisions under uncertainty. In an RL problem, an agent interacts with an environment, observes states <span class="arithmatex">\(s\)</span>, takes actions <span class="arithmatex">\(a\)</span>, and receives rewards <span class="arithmatex">\(r\)</span>, with the goal of learning a policy <span class="arithmatex">\(\pi(a|s)\)</span> that maximizes future expected reward. Several core features distinguish RL from other learning paradigms:</p>
<ul>
<li>
<p>Optimization of Long-Term Reward: The agent seeks to maximize cumulative reward, accounting for delayed consequences of actions.</p>
</li>
<li>
<p>Trial-and-Error Learning: The agent learns by exploring different actions and observing outcomes, balancing exploration vs. exploitation.</p>
</li>
<li>
<p>Generalization: The agent must generalize from limited experience to new situations (often via function approximation in large state spaces).</p>
</li>
<li>
<p>Data Distribution Shift: Unlike supervised learning, the agent’s own actions affect the data it collects and the states it visits, creating a feedback loop in the learning process.</p>
</li>
</ul>
<p>We began our journey with multi-armed bandits, the simplest RL setting. In a bandit problem there is a single state (no state transitions); each action (arm) yields a reward drawn from an unknown distribution, and the goal is to maximize reward over repeated plays. A bandit is essentially a stateless decision problem – the next situation does not depend on the previous action. This contrasts with the general Markov Decision Process (MDP) setting, where each action can change the state and influence future rewards and decisions. Bandits capture the essence of exploration-exploitation without the complication of state transitions, making them a perfect starting point.</p>
<p>From bandits we progressed to MDPs and multi-step RL problems, which introduce state dynamics and temporal credit assignment. We studied model-free methods (like Q-learning and policy gradient) and model-based methods (like planning with known models or learned models), as well as combinations thereof. As tasks grew more complex, we incorporated function approximation (e.g. using deep neural networks) to handle large or continuous state spaces. This led us into the realm of deep reinforcement learning, where algorithms like DQN and policy optimization methods (PPO, etc.) leverage deep networks as powerful function approximators. While function approximation enables scaling to complex domains, it also introduced new challenges such as stability of learning (e.g. off-policy learning instability, need for techniques like experience replay, target networks, or trust region methods). In parallel, we discussed how off-policy learning and exploration in large domains remain critical challenges, and saw approaches to address these (from clipped policy optimization (PPO) for stability, to imitation learning like DAGGER to incorporate expert knowledge, to pessimistic value adjustments for safer offline learning).</p>
<p>Throughout this journey, a unifying theme has been the exploration-exploitation dilemma and the development of algorithms to efficiently learn optimal strategies. In the following sections, we summarize some key algorithmic ideas for exploration and discuss how they exemplify different strategies to address this core challenge.</p>
<h2 id="reinforcement-14_final-key-algorithmic-ideas-in-exploration-and-planning">Key Algorithmic Ideas in Exploration and Planning<a class="headerlink" href="#reinforcement-14_final-key-algorithmic-ideas-in-exploration-and-planning" title="Permanent link">¶</a></h2>
<h3 id="reinforcement-14_final-optimistic-exploration-upper-confidence-bounds-ucb">Optimistic Exploration: Upper Confidence Bounds (UCB)<a class="headerlink" href="#reinforcement-14_final-optimistic-exploration-upper-confidence-bounds-ucb" title="Permanent link">¶</a></h3>
<p>A foundational idea for efficient exploration is optimism in the face of uncertainty. The principle is simple: assume the best about untried actions so that the agent is driven to explore them. The Upper Confidence Bound (UCB) algorithm is a classic realization of this idea for multi-armed bandits. UCB maintains an estimate <span class="arithmatex">\(\hat{Q}_t(a)\)</span> for the mean reward of each arm <span class="arithmatex">\(a\)</span> and an uncertainty interval (confidence bound) around that estimate. At each time <span class="arithmatex">\(t\)</span>, it selects the action maximizing an upper-confidence estimate of the reward:</p>
<div class="arithmatex">\[
a_t = \arg\max_{a \in A} \left[ \hat{Q}_t(a) + c \frac{\ln t}{N_t(a)} \right],
\]</div>
<p>where <span class="arithmatex">\(N_t(a)\)</span> is the number of times action <span class="arithmatex">\(a\)</span> has been taken up to time <span class="arithmatex">\(t\)</span>, and <span class="arithmatex">\(c\)</span> is a constant (e.g. <span class="arithmatex">\(c=\sqrt{2}\)</span> for the UCB1 algorithm).</p>
<p>This selection rule balances exploitation (the <span class="arithmatex">\(\hat{Q}_t(a)\)</span> term) with exploration (the bonus term that is large for rarely-selected actions). Intuitively, UCB explores actions with high potential payoffs or high uncertainty. This approach yields strong theoretical guarantees: for instance, UCB1 achieves sublinear regret on the order of <span class="arithmatex">\(O(\ln T)\)</span> for bandits, meaning the gap between the accumulated reward of UCB and that of an oracle choosing the best arm at each play grows only logarithmically with time. Optimistic algorithms like UCB are attractive because they are simple and provide worst-case performance guarantees (they will eventually try everything enough to near-certainty). Variants of UCB and optimism-driven exploration have been extended beyond bandits, for example to MDPs via exploration bonus terms.</p>
<h3 id="reinforcement-14_final-probability-matching-thompson-sampling">Probability Matching: Thompson Sampling<a class="headerlink" href="#reinforcement-14_final-probability-matching-thompson-sampling" title="Permanent link">¶</a></h3>
<p>An alternative approach to exploration comes from a Bayesian perspective. Instead of confidence bounds, the agent maintains a posterior distribution over the reward parameters of each action and samples an action according to the probability it is optimal. This strategy is known as Thompson Sampling (or probability matching). In the multi-armed bandit setting, Thompson Sampling can be implemented by assuming a prior for each arm’s mean reward, updating it with observed rewards, and then at each step sampling a value <span class="arithmatex">\(\tilde{\theta}_a\)</span> from the posterior of each arm’s mean. The agent then plays the arm with the highest sampled value. By randomly exploring according to its uncertainty, Thompson Sampling naturally balances exploration and exploitation in a Bayesian-optimal way for certain problems.</p>
<p>For example, if rewards are Bernoulli and a Beta prior is used for each arm’s success probability, Thompson Sampling draws a sample from each arm’s Beta posterior and picks the arm with the largest sample. This probability matching tends to allocate more trials to arms that are likely to be best, yet still occasionally tries others proportional to uncertainty. Empirically, Thompson Sampling often performs exceptionally well, sometimes even outperforming UCB in practice, and it has a Bayesian regret that is optimal in certain settings. The caveat is that analyzing Thompson Sampling’s worst-case performance is more complex; however, theoretical advances have shown Thompson Sampling achieves <span class="arithmatex">\(O(\ln T)\)</span> regret for many bandit problems as well. A key appeal of Thompson Sampling is its flexibility – it can be applied to complex problems if one can sample from a posterior (or an approximate posterior) of the model’s parameters. In modern RL, variants of Thompson Sampling inspire approaches like Bootstrapped DQN (which maintains an ensemble of value networks to generate randomized Q-value estimates for exploration).</p>
<h3 id="reinforcement-14_final-pac-mdp-algorithms-and-exploration-bonuses-mbie-eb">PAC-MDP Algorithms and Exploration Bonuses (MBIE-EB)<a class="headerlink" href="#reinforcement-14_final-pac-mdp-algorithms-and-exploration-bonuses-mbie-eb" title="Permanent link">¶</a></h3>
<p>In full reinforcement learning problems (MDPs), the exploration challenge becomes more intricate due to state transitions. PAC-MDP algorithms provide a framework for efficient exploration with theoretical guarantees. PAC stands for “Probably Approximately Correct,” meaning these algorithms guarantee that with high probability (<span class="arithmatex">\(1-\delta\)</span>) the agent will behave near-optimally (within <span class="arithmatex">\(\varepsilon\)</span> of the optimal return) after a certain number of time steps that is polynomial in relevant problem parameters. In other words, a PAC-MDP algorithm will make only a finite (polynomial) number of suboptimal decisions before it effectively converges to an <span class="arithmatex">\(\varepsilon\)</span>-optimal policy.</p>
<p>One representative PAC-MDP approach is Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) by Strehl and Littman (2008). This algorithm uses an optimistic model-based strategy: it learns an estimated MDP (transition probabilities <span class="arithmatex">\(\hat{T}\)</span> and rewards <span class="arithmatex">\(\hat{R}\)</span>) from experience and uses dynamic programming to compute a value function <span class="arithmatex">\(\tilde{Q}(s,a)\)</span> for that estimated model. Critically, MBIE-EB adds an exploration bonus term to reward or value updates for state-action pairs that have been infrequently visited. For example, the update might be:</p>
<div class="arithmatex">\[
\tilde{Q}(s,a) \leftarrow \hat{R}(s,a) + \gamma \sum_{s'} \hat{T}(s'|s,a) \max_{a'} \tilde{Q}(s',a') + \beta \frac{1}{\sqrt{N(s,a)}},
\]</div>
<p>where <span class="arithmatex">\(N(s,a)\)</span> counts visits to <span class="arithmatex">\((s,a)\)</span> and <span class="arithmatex">\(\beta\)</span> is a bonus scale derived from PAC confidence bounds. The <span class="arithmatex">\(\sqrt{1/N(s,a)}\)</span> bonus term is large for rarely tried state-action pairs, injecting optimism that encourages the agent to explore them. MBIE-EB selects actions according to the optimistic <span class="arithmatex">\(\tilde{Q}\)</span> values (i.e. optimism under uncertainty in an MDP context). Strehl and Littman proved that MBIE-EB is PAC-MDP: with probability <span class="arithmatex">\(1-\delta\)</span>, after a number of steps polynomial in <span class="arithmatex">\(|S|, |A|, 1/\varepsilon, 1/\delta\)</span>, etc., the algorithm’s policy is <span class="arithmatex">\(\varepsilon\)</span>-optimal. PAC algorithms like MBIE-EB (and related methods like R-MAX and UCRL) guarantee efficient exploration in theory, though they can be computationally demanding in practice for large domains. They illustrate how theoretical foundations (confidence intervals and PAC guarantees) directly inform algorithm design.</p>
<h3 id="reinforcement-14_final-monte-carlo-tree-search-mcts-for-planning">Monte Carlo Tree Search (MCTS) for Planning<a class="headerlink" href="#reinforcement-14_final-monte-carlo-tree-search-mcts-for-planning" title="Permanent link">¶</a></h3>
<p>So far we have discussed exploration in the context of learning unknown values or models. Another key idea in the RL toolkit is planning using simulation, particularly via Monte Carlo Tree Search (MCTS). MCTS is a family of simulation-based search algorithms that became famous through their use in game-playing AI (e.g. AlphaGo and AlphaZero). The idea is to build a partial search tree from the current state by simulating many random play-outs (rollouts) and using the results to gradually refine value estimates for states and actions.</p>
<p>One of the most widely used MCTS algorithms is UCT (Upper Confidence Trees), which blends the UCB idea with tree search. In each simulation (from root state until a terminal state or depth limit), UCT traverses the tree by choosing actions that maximize an upper confidence bound: at a state (tree node) <span class="arithmatex">\(s\)</span>, it selects the action <span class="arithmatex">\(a\)</span> that maximizes</p>
<div class="arithmatex">\[
\frac{w_{s,a}}{n_{s,a}} + c \sqrt{\frac{\ln N_s}{n_{s,a}}},
\]</div>
<p>where <span class="arithmatex">\(w_{s,a}\)</span> is the total reward accrued from past simulations taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>, <span class="arithmatex">\(n_{s,a}\)</span> is the number of simulations that took that action, and <span class="arithmatex">\(N_s = \sum_a n_{s,a}\)</span> is the total simulations from state <span class="arithmatex">\(s\)</span>. This formula is essentially the UCB1 formula extended to tree nodes: the first term is exploitation (the empirical mean reward), and the second is an exploration bonus that is higher for seldom-tried actions. By using this rule at each step of simulation (Selection phase), MCTS efficiently explores the game tree, focusing on promising moves while still trying less-visited moves once in a while. After selection, a random Simulation (rollout) is played out to the end, and the outcome is backpropagated to update <span class="arithmatex">\(w\)</span> and <span class="arithmatex">\(n\)</span> along the path. Repeating thousands or millions of simulations yields increasingly accurate value estimates for the root state and preferred actions.</p>
<p>MCTS does not learn parameters from data in the traditional sense; rather it is a planning method that can be applied if we have a generative model of the environment (e.g. a simulator or game rules). However, it connects to our theme as another approach to balancing exploration and exploitation via UCB-like algorithms. In practice, MCTS can be combined with learning. Notably, AlphaGo and AlphaZero combined deep neural networks (for state evaluation and policy guidance) with Monte Carlo Tree Search to achieve superhuman performance in Go, chess, and shogi. In those systems, the neural network’s value estimates guide the rollout, and MCTS provides a powerful lookahead search that complements the learned policy. This combination dramatically improves data efficiency – for example, AlphaZero uses MCTS to effectively explore the game space instead of needing an exorbitant amount of self-play games, and the knowledge gained from MCTS is distilled back into the network through training. MCTS exemplifies how models and planning can be leveraged in RL: if a model of the environment is available (or learned), one can simulate experience to aid decision-making without direct real-world trial-and-error for every decision. This is crucial in domains where real experiments are costly or limited.</p>
<p>Computational vs Data Efficiency: It is worth noting that methods like MCTS (and exhaustive exploration algorithms) tend to be computationally intensive – they trade computation for reduced real-world data needs. We often face a trade-off: algorithms that are very data-efficient (using fewer environment interactions) are often computationally expensive, whereas simpler algorithms that learn quickly in computation might require more data. In some domains (like games or simulated environments), we can afford massive computation, effectively converting computation into simulated “data” for learning. In others (like physical systems or online user interactions), data is scarce or expensive, so sample-efficient algorithms (even if computationally heavy) are preferred. This trade-off has been a recurring consideration as we moved from bandits to deep RL.</p>
<h2 id="reinforcement-14_final-exploration-paradigms-optimism-vs-probability-matching">Exploration Paradigms: Optimism vs. Probability Matching<a class="headerlink" href="#reinforcement-14_final-exploration-paradigms-optimism-vs-probability-matching" title="Permanent link">¶</a></h2>
<p>We have seen two major paradigms for addressing the exploration-exploitation challenge:</p>
<ul>
<li>
<p>Optimism in the face of uncertainty: The agent behaves as if the environment is as rewarding as plausibly possible, given the data. This leads to algorithms like UCB, optimistic initial values, exploration bonuses (e.g. MBIE-EB, optimistic Q-learning), and UCT in MCTS. Optimistic methods systematically encourage trying actions that could be best. They often come with strong theoretical guarantees (UCB’s regret bound, PAC-MDP bounds, etc.) because they ensure sufficient exploration of each alternative. Optimism tends to be a more worst-case (frequentist) approach: it doesn’t assume a prior, just relies on confidence intervals that hold with high probability for any reward distribution.</p>
</li>
<li>
<p>Probability matching (Thompson Sampling and Bayesian methods): The agent maintains a belief (probability distribution) about the environment’s parameters and randomizes its actions according to this belief. Effectively, it samples a hypothesis for the true model and then exploits that hypothesis (e.g., play the best action for that sampled model). Over time, the belief is updated with Bayes’ rule as more data comes in, so the sampling naturally shifts toward optimal actions. This approach is more Bayesian in spirit: it assumes a prior distribution and seeks to maximize performance on average with respect to that prior (i.e., good Bayesian regret). Probability matching can be very effective in practice and can incorporate prior knowledge elegantly. The downside is that providing theoretical guarantees in the worst-case sense can be challenging – the guarantees are often Bayesian (in expectation over the prior) rather than uniform for all environments. Recent theoretical work, however, has shown that even without a perfect prior, Thompson Sampling performs near-optimally in many settings, and there are ways to bound its regret. In terms of implementation complexity, Thompson Sampling may require the ability to sample from posterior distributions, which can be non-trivial in large-scale problems (though approximate methods exist). Optimistic methods, on the other hand, require confidence bound calculations, which for simple tabular cases are straightforward, but for complex function approximation can be difficult (leading to research on exploration bonuses using predictive models or uncertainty estimates).</p>
</li>
</ul>
<p>In summary, optimism vs. probability matching represents two different philosophies for exploration. Optimistic algorithms behave more deterministically (always picking the current optimistic-best option), ensuring systematic coverage of possibilities, while Thompson-style algorithms inject randomized exploration in proportion to uncertainty. Interestingly, human decision-making experiments suggest people may combine elements of both strategies – not purely optimistic nor purely Thompson. Both paradigms have influenced modern RL: for example, exploration bonuses (optimism) are commonly used in deep RL (e.g. with bonus rewards from prediction error or curiosity), and Bayesian RL approaches (like posterior sampling for MDPs) are gaining traction for problems where a reasonable prior is available or an ensemble can approximate uncertainty.</p>
<h3 id="reinforcement-14_final-theoretical-foundations-regret-pac-and-bayesian-optimality">Theoretical Foundations: Regret, PAC, and Bayesian Optimality<a class="headerlink" href="#reinforcement-14_final-theoretical-foundations-regret-pac-and-bayesian-optimality" title="Permanent link">¶</a></h3>
<p>Understanding how well an RL algorithm performs relative to an ideal standard is a major theme in RL theory. We revisited two main frameworks for this: regret analysis and PAC (sample complexity) analysis, along with the Bayesian viewpoint.</p>
<ul>
<li>Regret: Regret measures the opportunity loss from not acting optimally at each time step. Formally, in a bandit with optimal expected reward <span class="arithmatex">\(\mu^*\)</span>, the regret after <span class="arithmatex">\(T\)</span> plays is</li>
</ul>
<div class="arithmatex">\[
R(T) = T\mu^* - \sum_{t=1}^T r_t,
\]</div>
<p>i.e. the difference between the reward that would be obtained by always executing the optimal arm and the reward actually obtained. Sublinear regret (e.g. <span class="arithmatex">\(R(T) = o(T)\)</span>) implies the algorithm eventually learns the optimal policy (average regret <span class="arithmatex">\(\to 0\)</span> as <span class="arithmatex">\(T\)</span> grows). We saw that <span class="arithmatex">\(\varepsilon\)</span>-greedy exploration can lead to linear regret in the worst case (always pulling some suboptimal arm a constant fraction of the time yields <span class="arithmatex">\(R(T) \sim \Omega(T)\)</span>). In contrast, UCB1 achieves <span class="arithmatex">\(R(T) = O(\ln T)\)</span>, which is asymptotically optimal up to constant factors (matching the Lai &amp; Robbins lower bound for bandits that <span class="arithmatex">\(R(T) \ge \Omega(\ln T)\)</span> for any algorithm). Regret analysis can be extended to MDPs (though it becomes more complex). For example, algorithms like UCRL2 (an optimistic tabular RL algorithm) have regret bounds on the order of <span class="arithmatex">\(\tilde{O}(\sqrt{T})\)</span> in an MDP (reflecting the harder challenge of states) under certain assumptions. Regret is a worst-case, online metric – it asks how well we do even against an adversarially chosen problem (or in the unknown actual environment) without assumptions of a prior, focusing on long-term performance.</p>
<ul>
<li>
<p>PAC (Probably Approximately Correct) guarantees: PAC analysis focuses on sample complexity: how many time steps or episodes are required for the algorithm to achieve near-optimal performance with high probability. A PAC guarantee typically states: for any <span class="arithmatex">\(\varepsilon, \delta\)</span>, there exists <span class="arithmatex">\(N(\varepsilon,\delta)\)</span> (poly in relevant parameters) such that with probability at least <span class="arithmatex">\(1-\delta\)</span>, the algorithm’s policy is <span class="arithmatex">\(\varepsilon\)</span>-optimal after <span class="arithmatex">\(N\)</span> steps (or, equivalently, all but at most <span class="arithmatex">\(N\)</span> of the steps are <span class="arithmatex">\(\varepsilon\)</span>-suboptimal). This is a finite-sample guarantee, giving confidence that the learning will not take too long. We discussed that algorithms like MBIE-EB and R-MAX are PAC-MDP: for a given accuracy <span class="arithmatex">\(\varepsilon\)</span> and confidence <span class="arithmatex">\(1-\delta\)</span>, their sample complexity (number of suboptimal actions) is bounded by a polynomial in <span class="arithmatex">\(|S|, |A|, 1/\varepsilon, 1/\delta, 1/(1-\gamma)\)</span>, etc. PAC analysis is particularly useful when we care about guarantees in a learning phase before near-optimal performance is reached (important in safety-critical or costly domains where we need to know learning will be efficient with high probability). While regret goes to zero only asymptotically, PAC gives an explicit bound on how long it takes to be good. Often, achieving PAC guarantees in large-scale problems requires simplifying assumptions or limited function approximation classes, as general function approximation PAC results are quite difficult.</p>
</li>
<li>
<p>Bayesian approaches and Bayes-optimality: In a Bayesian formulation, we assume a prior distribution over environments (bandit reward distributions or MDP dynamics). We can then consider the Bayes-optimal policy, which is the policy that maximizes expected cumulative reward with respect to this prior. This leads to the concept of Bayesian regret – the expected regret under the prior. A Bayes-optimal algorithm minimizes Bayesian regret and, by definition, will outperform any other algorithm on average if the prior is correct. One famous result in this vein is the Gittins Index for multi-armed bandits, which gives an optimal solution when each arm has independent known priors (casting the problem as a Markov process and solving it via dynamic programming). However, computing Bayes-optimal solutions for general RL (especially with state) is usually intractable – it involves solving a POMDP (partially observable MDP) where the hidden state is the true environment parameters. Thompson Sampling can be interpreted as an approximation to the Bayes-optimal policy that is much easier to implement. It has low Bayesian regret and in some cases can be shown to be asymptotically Bayes-optimal. The Bayesian view is powerful because it allows incorporation of prior knowledge and gives a normative standard (what should we do if we know what we don’t know, in distribution). But its limitation is the computational difficulty and the dependence on having a reasonable prior. In practice, algorithms inspired by Bayesian ideas (like ensemble sampling or posterior sampling for reinforcement learning) try to capture some of the benefit without solving the full Bayes-optimal policy.</p>
</li>
</ul>
<p>These theoretical frameworks complement each other. Regret and PAC analyses give worst-case performance assurances (no matter what the true environment is, within assumptions) and often inspire optimistic algorithms. Bayesian analysis aims for average-case optimality given prior knowledge and often inspires probability matching or adaptive algorithms. As an RL practitioner or researcher, understanding these foundations helps in choosing and designing algorithms appropriate for the problem at hand – whether one prioritizes guaranteed efficiency, practical performance with prior info, or a mix of both.</p>
<h2 id="reinforcement-14_final-from-theory-to-practice-real-world-applications-and-achievements">From Theory to Practice: Real-World Applications and Achievements<a class="headerlink" href="#reinforcement-14_final-from-theory-to-practice-real-world-applications-and-achievements" title="Permanent link">¶</a></h2>
<p>One of the most exciting aspects of the recent decade in RL is seeing theoretical ideas translate into real-world (or at least real-problem) successes. In this section, we connect some of the classic algorithms and concepts to notable applications:</p>
<ul>
<li>
<p>Game Mastery and Planning – AlphaGo, AlphaZero, AlphaTensor: Starting with games, AlphaGo famously combined deep neural networks with MCTS (using UCT) and was trained with reinforcement learning to defeat human Go champions. Its successor AlphaZero took this further by learning from scratch (self-play) for multiple games, using Monte Carlo Tree Search guided by a learned value/policy network. The blend of planning (MCTS) and learning (deep RL) that AlphaZero employs is a direct embodiment of concepts we covered: it uses optimistic simulations (MCTS uses UCB in the tree) and improves data efficiency by leveraging a model (the game simulator) for exploration. The success of AlphaZero demonstrates the power of combining model-based search with model-free function approximation. Recently, these ideas have even extended to domains beyond traditional games. AlphaTensor (DeepMind, 2022) is a system that treated the discovery of new matrix multiplication algorithms as a single-player game, and it applied a variant of AlphaZero’s RL approach to find faster algorithms for matrix multiply. The AlphaTensor agent was trained via self-play reinforcement learning to manipulate tensor representations of matrix multiplication and achieved a breakthrough: it discovered matrix multiplication algorithms that surpass the decades-old human benchmarks in efficiency. This is a striking example of RL not just playing games but discovering algorithms – essentially using reward signals to guide a search through the space of mathematical formulas. It showcases how MCTS (for planning) and deep RL can work together on combinatorial optimization problems: the agent expands a search tree of partial solutions, guided by value networks and an exploration policy, very much like how it would approach a board game. AlphaTensor’s success underscores the generality of RL methods and how ideas like optimism (self-play explores new moves) and guided search can yield new discoveries.</p>
</li>
<li>
<p>Natural Language and Human Feedback – ChatGPT: A more recent and widely impactful application of reinforcement learning is in natural language processing – specifically, training large language models to better align with human intentions. ChatGPT (OpenAI, 2022) is a prime example, where RL was used to fine-tune a pretrained language model using human feedback. The technique, known as Reinforcement Learning from Human Feedback (RLHF), involves first collecting human preference data on model outputs and then training a reward model that predicts human preference. The language model (policy) is then optimized (via a policy gradient method like PPO) to maximize the reward model’s score, i.e. to produce answers humans would rate highly. This is essentially an RL loop on top of the language model, treating the task of generating helpful, correct responses as an MDP (or episodic decision problem) and using the learned reward function as the reward signal. The result, ChatGPT, is notably more aligned with user expectations than its predecessor models. In our context, ChatGPT’s training illustrates several RL ideas in action: offline data (pretraining on text) combined with online RL fine-tuning, and the critical role of a well-shaped reward function for alignment. It also highlights exploration in a different sense – exploring the space of possible answers to find those that yield high reward according to human feedback. The success of ChatGPT demonstrates that RL is not limited to games or robotics; it can be scaled to very high-dimensional action spaces (like generating entire paragraphs of text) when guided by human-informed rewards. From a theoretical lens, one can view RLHF as optimizing an objective that marries the model’s knowledge (from supervised training) with a policy optimization under a learned reward. While classical exploration algorithms (UCB, Thompson) are not directly apparent in ChatGPT’s training (since the “exploration” comes from the model generating varied outputs and the policy optimization process), the high-level principle remains: use feedback signals to iteratively refine behavior.</p>
</li>
<li>
<p>Scientific and Industrial Applications: Beyond these headline examples, RL is increasingly applied in scientific and industrial domains. The course of our study touched on a few, such as:</p>
</li>
</ul>
<p>Controlling nuclear fusion plasmas: Researchers applied deep RL to control the magnetic coils in a tokamak reactor to sustain plasma configurations. This is a complex continuous control problem with safety constraints, where function approximation and careful exploration (largely in simulations before real experiments) were key.</p>
<p>Optimizing public health interventions: An RL approach was used to design efficient COVID-19 border testing policies. Framing the problem as a sequential decision task (who to test and when) and using RL to maximize some health outcome or efficiency metric allowed automating policy design that adapted to data.</p>
<p>Robotics and Autonomous Systems: Many advances in robotics have come from RL algorithms that allow robots to learn locomotion, manipulation, or flight. Often these use deep RL and sometimes simulation-to-reality transfer. The exploration techniques we learned (like curiosity-driven bonuses or domain randomization) help address the challenge of learning in these complex environments.</p>
<p>Recommender Systems and Online Decision Making: Multi-armed bandit algorithms (including Thompson Sampling and UCB) are widely used in industry for things like A/B testing, website optimization, and personalized recommendations. For example, serving personalized content can be seen as a bandit problem where each content choice is an arm and click-through or engagement is the reward. Companies employ bandit algorithms to balance exploration of new content with exploitation of known user preferences, often in a context of contextual bandits (where the state or context is user features). The theoretical guarantees of bandit algorithms give confidence in their performance, and their simplicity makes them practical at scale.</p>
<p>In all these cases, the fundamental concepts from this course appear and validate themselves: whether it’s optimism guiding AlphaZero’s search, or Thompson Sampling driving an online recommendation strategy, or policy gradients tuning ChatGPT using human rewards, the same core ideas of reinforcement learning apply. Modern applications often hybridize approaches – for instance, using model-based simulations (AlphaTensor, AlphaZero), or combining learning from offline data with online exploration (ChatGPT’s RLHF, or robotics). This underscores the importance of mastering the basics: understanding value functions, policy optimization, exploration mechanisms, and theoretical limits has direct relevance even as we push RL into new territory.</p>
<h2 id="reinforcement-14_final-final-takeaways">Final Takeaways<a class="headerlink" href="#reinforcement-14_final-final-takeaways" title="Permanent link">¶</a></h2>
<p>In closing, we synthesize a few key insights and lessons from the full RL journey:</p>
<ul>
<li>
<p>Reinforcement Learning Unifies Many Themes: We saw that RL problems range from simple bandits to complex high-dimensional control, but they share the need for sequential decision making under uncertainty. Concepts like state, action, reward, policy, value function, model form a common language to describe problems as diverse as games, robotics, and recommendation systems. Recognizing an appropriate RL formulation (MDP, bandit, etc.) for a given real-world problem is the first step to applying these methods.</p>
</li>
<li>
<p>Exploration vs. Exploitation is Fundamental: The trade-off between trying new actions and leveraging known good actions underpins all of RL. We examined different strategies:</p>
<ul>
<li>
<p>Heuristics like <span class="arithmatex">\(\epsilon\)</span>-greedy (simple but can be suboptimal),</p>
</li>
<li>
<p>Optimistic algorithms (UCB, optimism in value iteration,  exploration bonuses) which ensure systematic exploration using confidence bounds,</p>
</li>
<li>
<p>Probabilistic approaches (Thompson Sampling, randomized value functions) which inject randomness based on uncertainty.</p>
</li>
</ul>
</li>
</ul>
<p>Each approach has its advantages – optimism often yields strong guarantees and is conceptually straightforward, while Thompson Sampling often gives excellent practical performance and naturally incorporates prior knowledge. In large-scale problems, clever exploration bonuses (intrinsic rewards for novelty) and approximate uncertainty estimates are key to maintaining exploration. The central lesson is that successful RL requires deliberate exploration strategies; naive exploration can lead to poor sample efficiency or getting stuck in suboptimal behaviors.</p>
<ul>
<li>
<p>Theoretical Foundations Guide Algorithm Design: Concepts like regret and PAC provide ways to formally measure learning efficiency. They not only help us compare algorithms (e.g. which has lower regret or better sample complexity) but have directly inspired algorithmic techniques (like UCB from the idea of minimizing regret, or PAC-inspired algorithms like MBIE-EB and R-MAX designed to guarantee learning within polynomial time). Meanwhile, the Bayesian perspective offers a gold-standard for optimal decision-making given prior info, even if it’s often computationally intractable – it guides us toward algorithms that perform well on average and informs approaches like posterior sampling. As RL practitioners, we should remember:</p>
<ul>
<li>
<p>Regret minimization focuses on not wasting too many opportunities – it’s about learning as fast as possible in an online sense.</p>
</li>
<li>
<p>PAC guarantees focus on bounding the learning time with high confidence – giving safety that an algorithm won’t do too poorly for too long.</p>
</li>
<li>
<p>Bayesian optimality focuses on using prior knowledge efficiently – it’s about doing the best given what you (probabilistically) know.</p>
</li>
</ul>
</li>
</ul>
<p>All three perspectives are important; balancing them or choosing the right one depends on the application (e.g., in a one-off A/B test you might care about regret, in a lifelong robot learning you care about sample efficiency with high probability, and in a personalized system you might incorporate Bayesian priors about users).</p>
<ul>
<li>
<p>Function Approximation and Deep RL Open New Possibilities (and Challenges): The leap from tabular or small-scale problems to real-world complexity required using function approximation (especially deep neural networks). This enabled RL to handle images, continuous states, and enormous state spaces – as seen in Atari games, Go, and continuous control benchmarks. The success of deep RL (DQN, policy gradient methods, etc.) comes from blending RL algorithms with powerful representation learning. However, it also brought challenges like stability of training, overfitting, exploration in high dimensions, and reproducibility issues. Key techniques to mitigate these include experience replay, target networks, regularization, large-scale parallel training, and reward shaping. The takeaway is that theoretical convergence guarantees often break down with function approximation, so a lot of practical know-how and experimentation is needed. Yet, the core ideas (Bellman equations, policy improvement, etc.) still apply – just approximate. The field is actively developing better theories for RL with function approximation (e.g. understanding generalization, error propagation) and techniques for more reliable training.</p>
</li>
<li>
<p>Real-World Impact and Ongoing Research: Reinforcement learning has graduated from textbook problems to impacting real-world systems. Its principles have powered superhuman game AIs, improved scientific research (e.g. algorithm discovery, experiment design), enhanced language models, and optimized business decisions. At the same time, truly robust and general-purpose RL is still an open challenge. Issues of stability, efficiency, and safety remain – for instance:</p>
<ul>
<li>Developing algorithms that work out-of-the-box with minimal tuning for any problem (robustness).</li>
<li>Improving data efficiency so that RL can be applied with limited real-world interactions (e.g., via model-based methods, better exploration, or transfer learning).</li>
<li>Integrating learning and planning seamlessly, and handling settings that mix offline data with online exploration.</li>
<li>Expanding the RL framework to account for multiple objectives, collaboration or competition (multi-agent RL), and richer feedback modalities beyond scalar rewards.</li>
</ul>
</li>
</ul>
<p>These are active research directions. The skills and concepts acquired – from understanding theoretical bounds to implementing algorithms – equip us to tackle these frontiers.</p>
<p>In summary, the journey from multi-armed bandits to deep reinforcement learning has taught us not only a catalogue of algorithms, but a way of thinking about sequential decision problems. We learned how to measure learning efficiency and why exploration is hard yet critical. We saw simple ideas like optimism and probability matching scale up to complex systems that play Go or converse in English. As you move forward from this textbook, remember the foundational principles: reward is your guide, value estimation is your tool, policy is your output, and exploration is your catalyst. With these in mind, you are well-prepared to both apply RL to challenging problems and to contribute to the advancing frontier of reinforcement learning research.</p></body></html></section></section></div><style>.print-site-enumerate-headings #index > h1:before { content: '1 ' }

                .print-site-enumerate-headings #index h2:before { content: '1.' counter(counter-index-2) ' ' }
                .print-site-enumerate-headings #index h2 {  counter-reset: counter-index-3 ;  counter-increment: counter-index-2 }
            
                .print-site-enumerate-headings #index h3:before { content: '1.' counter(counter-index-2) '.' counter(counter-index-3) ' ' }
                .print-site-enumerate-headings #index h3 {  counter-increment: counter-index-3 }
            
.print-site-enumerate-headings #section-2 > h1:before { content: '2 ' }
.print-site-enumerate-headings #reinforcement-1_intro > h1:before { content: '2.1 ' }

                .print-site-enumerate-headings #reinforcement-1_intro h2:before { content: '2.1.' counter(counter-reinforcement-1_intro-2) ' ' }
                .print-site-enumerate-headings #reinforcement-1_intro h2 {  counter-increment: counter-reinforcement-1_intro-2 }
            
.print-site-enumerate-headings #reinforcement-2_mdp > h1:before { content: '2.2 ' }

                .print-site-enumerate-headings #reinforcement-2_mdp h2:before { content: '2.2.' counter(counter-reinforcement-2_mdp-2) ' ' }
                .print-site-enumerate-headings #reinforcement-2_mdp h2 {  counter-increment: counter-reinforcement-2_mdp-2 }
            
.print-site-enumerate-headings #reinforcement-3_modelfree > h1:before { content: '2.3 ' }

                .print-site-enumerate-headings #reinforcement-3_modelfree h2:before { content: '2.3.' counter(counter-reinforcement-3_modelfree-2) ' ' }
                .print-site-enumerate-headings #reinforcement-3_modelfree h2 {  counter-increment: counter-reinforcement-3_modelfree-2 }
            
.print-site-enumerate-headings #reinforcement-4_model_free_control > h1:before { content: '2.4 ' }

                .print-site-enumerate-headings #reinforcement-4_model_free_control h2:before { content: '2.4.' counter(counter-reinforcement-4_model_free_control-2) ' ' }
                .print-site-enumerate-headings #reinforcement-4_model_free_control h2 {  counter-increment: counter-reinforcement-4_model_free_control-2 }
            
.print-site-enumerate-headings #reinforcement-5_policy_gradient > h1:before { content: '2.5 ' }

                .print-site-enumerate-headings #reinforcement-5_policy_gradient h2:before { content: '2.5.' counter(counter-reinforcement-5_policy_gradient-2) ' ' }
                .print-site-enumerate-headings #reinforcement-5_policy_gradient h2 {  counter-increment: counter-reinforcement-5_policy_gradient-2 }
            
.print-site-enumerate-headings #reinforcement-6_pg2 > h1:before { content: '2.6 ' }

                .print-site-enumerate-headings #reinforcement-6_pg2 h2:before { content: '2.6.' counter(counter-reinforcement-6_pg2-2) ' ' }
                .print-site-enumerate-headings #reinforcement-6_pg2 h2 {  counter-increment: counter-reinforcement-6_pg2-2 }
            
.print-site-enumerate-headings #reinforcement-7_gae > h1:before { content: '2.7 ' }

                .print-site-enumerate-headings #reinforcement-7_gae h2:before { content: '2.7.' counter(counter-reinforcement-7_gae-2) ' ' }
                .print-site-enumerate-headings #reinforcement-7_gae h2 {  counter-increment: counter-reinforcement-7_gae-2 }
            
.print-site-enumerate-headings #reinforcement-8_imitation_learning > h1:before { content: '2.8 ' }

                .print-site-enumerate-headings #reinforcement-8_imitation_learning h2:before { content: '2.8.' counter(counter-reinforcement-8_imitation_learning-2) ' ' }
                .print-site-enumerate-headings #reinforcement-8_imitation_learning h2 {  counter-increment: counter-reinforcement-8_imitation_learning-2 }
            
.print-site-enumerate-headings #reinforcement-9_rlhf > h1:before { content: '2.9 ' }

                .print-site-enumerate-headings #reinforcement-9_rlhf h2:before { content: '2.9.' counter(counter-reinforcement-9_rlhf-2) ' ' }
                .print-site-enumerate-headings #reinforcement-9_rlhf h2 {  counter-increment: counter-reinforcement-9_rlhf-2 }
            
.print-site-enumerate-headings #reinforcement-10_offline_rl > h1:before { content: '2.10 ' }

                .print-site-enumerate-headings #reinforcement-10_offline_rl h2:before { content: '2.10.' counter(counter-reinforcement-10_offline_rl-2) ' ' }
                .print-site-enumerate-headings #reinforcement-10_offline_rl h2 {  counter-increment: counter-reinforcement-10_offline_rl-2 }
            
.print-site-enumerate-headings #reinforcement-11_fast_rl > h1:before { content: '2.11 ' }

                .print-site-enumerate-headings #reinforcement-11_fast_rl h2:before { content: '2.11.' counter(counter-reinforcement-11_fast_rl-2) ' ' }
                .print-site-enumerate-headings #reinforcement-11_fast_rl h2 {  counter-increment: counter-reinforcement-11_fast_rl-2 }
            
.print-site-enumerate-headings #reinforcement-12_fast_mdps > h1:before { content: '2.12 ' }

                .print-site-enumerate-headings #reinforcement-12_fast_mdps h2:before { content: '2.12.' counter(counter-reinforcement-12_fast_mdps-2) ' ' }
                .print-site-enumerate-headings #reinforcement-12_fast_mdps h2 {  counter-increment: counter-reinforcement-12_fast_mdps-2 }
            
.print-site-enumerate-headings #reinforcement-13_montecarlo > h1:before { content: '2.13 ' }

                .print-site-enumerate-headings #reinforcement-13_montecarlo h2:before { content: '2.13.' counter(counter-reinforcement-13_montecarlo-2) ' ' }
                .print-site-enumerate-headings #reinforcement-13_montecarlo h2 {  counter-increment: counter-reinforcement-13_montecarlo-2 }
            
.print-site-enumerate-headings #reinforcement-14_final > h1:before { content: '2.14 ' }

                .print-site-enumerate-headings #reinforcement-14_final h2:before { content: '2.14.' counter(counter-reinforcement-14_final-2) ' ' }
                .print-site-enumerate-headings #reinforcement-14_final h2 {  counter-increment: counter-reinforcement-14_final-2 }
            </style>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/convex_optimization" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "/reinforcement_learning/", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>