{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0dade6",
   "metadata": {},
   "source": [
    "## Case Study: Preference-Driven Optimization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c8da3",
   "metadata": {},
   "source": [
    "Modern AI models often need to optimize against human preferences or satisfaction signals rather than simple supervised targets. For example, dialog systems or summarizers may be tuned using human ratings or pairwise preferences (e.g. “response A is better than B”). Historically this has been done via Reinforcement Learning from Human Feedback (RLHF): one first trains a reward model on pairwise human judgments, then uses policy optimization (e.g. PPO) to fine-tune a language model. However, RLHF can be complex and unstable. Recent methods like Direct Preference Optimization (DPO) sidestep explicit RL. DPO directly fits the language model to the preferences using a classification loss, while still implicitly solving the same reward-maximization objective. \n",
    "\n",
    "![image.png](images/dpo.png)\n",
    "\n",
    "Figure: RLHF vs DPO [Rafailov 2023](https://arxiv.org/pdf/2305.18290#:~:text=is%20reinforcement%20learning%20from%20human,to%20optimize%20a%20language%20model)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad27924",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization (DPO) for Language Models\n",
    "\n",
    "DPO is designed to fine-tune a pretrained language model (LM) on pairwise preference data (prompt plus two responses with a “chosen” better than “rejected” label) without a separate reward-model or RL stage. Conceptually, DPO maximizes the log-likelihood of the preferred response relative to the inferior one. Suppose a prompt $x$ has two candidate completions $y^+$ (preferred) and $y^-$ (rejected). DPO posits a Bradley–Terry (logistic) model over the pair, and optimizes:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\log \\sigma\\!\\left(\\beta \\big[ \\log P_\\theta(y^+ \\mid x) - \\log P_\\theta(y^- \\mid x) \\big]\\right),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\beta > 0$ is a scaling hyperparameter (analogous to an inverse KL-penalty). In words, this loss increases the LM’s probability of the preferred response and decreases it for the rejected. Unlike naive probability-ratio losses, DPO’s derivation shows this simple binary cross-entropy on the log-prob difference solves the same constrained RLHF objective (maximizing reward with a KL-constraint) in closed form.\n",
    "\n",
    "**Figure 1:** Workflow comparison of RLHF versus DPO. RLHF (top) first fits a reward model and then uses policy optimization (e.g. PPO) to tune the language model, whereas DPO (bottom) directly fine-tunes the model with a binary cross-entropy loss on preference pairs.\n",
    "\n",
    "In practice, we gather or use an existing preference dataset (e.g. the Anthropic HH-RLHF data with “chosen” and “rejected” responses). We first ensure the LM is in-distribution (e.g. by supervised fine-tuning on original prompts), then run DPO training by maximizing $\\log \\sigma(\\Delta)$ with respect to $\\theta$. The DPO paper and its Hugging Face TRL implementation show that this yields models that match or exceed PPO-trained models on many tasks (e.g. sentiment steering, summarization) while being simpler to train.\n",
    "\n",
    "## Implementation (PyTorch / Hugging Face)\n",
    "\n",
    "We can use the Hugging Face `transformers` and `trl` libraries. Below is a schematic snippet (assumes `trl` and `transformers` are installed):\n",
    "\n",
    "```python\n",
    "from trl import DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preference_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.1,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ade26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a pretrained LM and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load or prepare a preference dataset: each example has 'prompt', 'chosen', 'rejected'\n",
    "# Here we use Anthropic HH-RLHF helpfulness data as an example\n",
    "prefs = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"train\")\n",
    "\n",
    "# Configure DPO training (choose output dir, beta, etc.)\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"gpt2_dpo_demo\",\n",
    "    num_train_epochs=3,\n",
    "    batch_size=16,\n",
    "    beta=0.5   # controls strength of preference vs KL (tunable)\n",
    ")\n",
    "\n",
    "# Initialize the DPO trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=dpo_config,\n",
    "    train_dataset=prefs,\n",
    "    label_names=(\"chosen\", \"rejected\")  # specify which fields to use\n",
    ")\n",
    "\n",
    "# Run fine-tuning\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
