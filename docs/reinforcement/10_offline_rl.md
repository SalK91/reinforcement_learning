# Chapter 10: Batch / Offline RL Policy Evaluation & Optimization

Learning from the Past

* Learning from Past Human Demonstrations: Imitation Learning
* Learning from Past Human Preferences: RLHF and DPO
* Learning from Past Decisions and Actions: Offline RL

## Offline Reinforcement Learning: A Different Approach
Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.

In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.

> Why Can’t We Just Use Q-Learning?
>
> * Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values
> * But deadly triad of bootstrapping, function approximation and off
policy, and can fail

## Batch Policy Evaluation: Estimating the Performance of a Policy

Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.

1. Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.

    Specifically, it learns two main components from data: a reward function $\hat{r}(s,a)$ and transition dynamics $\hat{P}(s' \mid s,a)$. These are learned via supervised learning on the offline dataset $D$ of transitions collected by some behavior policy $\pi_b$. For example, $\hat{r}(s,a)$ can be trained by regression to predict the observed reward given state $s$ and action $a$, and $\hat{P}(s' \mid s,a)$ can be fit to predict the next-state $s'$ (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in $D$ under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to “learn the environment’s rules". Once learned, this model $\hat{\mathcal{M}} = (\hat{P}, \hat{r})$ serves as a proxy for the real environment, which we can use for evaluating any policy $\pi$ without further real experience.

    It’s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics $\hat{P}$ will be reliable only in regions covered by $D$; if $\pi$ later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).

    #### Algorithmic Outline - Offline Policy Evaluation via Model:

    1. Input: offline dataset $D$ of transitions (from behavior $\pi_b$), a policy $\pi$ to evaluate, discount $\gamma$.

    2. Model Learning: Fit $\hat{P}(s'|s,a)$ and $\hat{r}(s,a)$ using $D$ (e.g. maximum likelihood estimation for dynamics, regression for rewards).

    3. Policy Evaluation: Initialize $V(s)=0$ for all states (or some initial guess).

    4. Loop (Bellman backups using $\hat{P},\hat{r}$): For each state $s$ in the state space (or a representative set of states):
    
    * Compute $\hat{R}^\pi(s) = \sum_a \pi(a|s)\hat{r}(s,a)$.
    * Compute $V_{\text{new}}(s) = \hat{R}^\pi(s) + \gamma \sum_{s'} \hat{P}^\pi(s'\mid s),V(s')$.

    5. Update $V \leftarrow V_{\text{new}}$ and repeat until convergence (the changes in $V$ are below a threshold).

    6. Output: $V(s)$ for states of interest (e.g. the estimated value of $\pi$ under the initial state distribution $S_0$ can be obtained by $\mathbb{E}_{s_0\sim S_0}[V(s_0)]$).

2. Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.


    ### Algorithm 3 Fitted Q Evaluation: FQE $(\pi, c)$

    Input: Dataset $\mathcal{D} = \{(x_i, a_i, x'_i, c_i)\}_{i=1}^n \sim \pi_D$. Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy $\pi$ by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state $s_i$, taking action $a_i$, and then following policy $\pi$ thereafter.At each iteration, we construct a Bellman target:
    
    $$\tilde{Q}^\pi(s_i, a_i)
    =
    c_i + \gamma V_\theta^\pi(s_{i+1})
    $$

    where
    
    $$
    V_\theta^\pi(s_{i+1}) = Q_\theta^\pi(s_{i+1}, \pi(s_{i+1})).
    $$

    The Q-function is parameterized by $\theta$ (e.g., a neural network), and is learned by solving a supervised regression problem:
    

    $$\arg\min_\theta
    \sum_i
    \Big(
    Q_\theta^\pi(s_i, a_i)
    -
    \tilde{Q}^\pi(s_i, a_i)
    \Big)^2$$

    This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy $\pi$ under the data distribution induced by $\pi_D$.

    Function class $F$ (Let's assume we use a DNN for F). Policy 
    
    $\pi$ to be evaluated.
    1: Initialize $Q_0 \in F$ randomly  
    2: for $k = 1, 2, \dots, K$ do  
    3: $\quad$ Compute target $y_i = c_i + \gamma Q_{k-1}(x'_i, \pi(x'_i)) \quad \forall i$  
    4: $\quad$ Build training set $\tilde{\mathcal{D}}_k = \{(x_i, a_i), y_i\}_{i=1}^n$  
    5: $\quad$ Solve a supervised learning problem:  
    \[
    Q_k = \arg\min_{f \in F} \frac{1}{n} \sum_{i=1}^n \big(f(x_i, a_i) - y_i\big)^2
    \]  
    6: end for  
    Output: $\hat{C}^\pi(x) = Q_K(x, \pi(x)) \quad \forall x$


    > ## What is different vs DQN?
    > DQN learns an optimal policy by interacting with the environment, while FQE evaluates a *fixed policy* using a *fixed offline dataset*.

    >| Aspect | FQE (Fitted Q Evaluation) | DQN (Deep Q-Network) |
    |------|-------------------------------|--------------------------|
    | Goal | Policy evaluation | Policy optimization / control |
    | Policy | Fixed target policy $\pi$ | Implicitly learned via $\max_a Q(s,a)$ |
    | Data | Offline, fixed dataset $\mathcal{D}$ | Online, collected during training |
    | Bellman target | $c + \gamma Q(s', \pi(s'))$ | $r + \gamma \max_a Q(s', a)$ |
    | Action at next state | From given policy $\pi$ | Greedy over Q-values |
    | Exploration | None | Required (e.g. $\epsilon$-greedy) |
    | Dataset changes? | ❌ No | ✅ Yes |
    | Off-policy instability | Low | High |
    | Convergence guarantees | Yes (tabular / linear) | No (with function approximation) |
    >
    >
    > ### 1. No maximization bias in FQE
    > - DQN suffers from overestimation bias
    > - FQE does pure regression, no bootstrapped max
    > ### 2. Stability
    > - FQE ≈ supervised learning  
    > - DQN ≈ bootstrapped + non-stationary targets
    > ### 3. Offline vs Online
    > - FQE cannot improve the policy  
    > - DQN must interact with environment
    


3. Importance Sampling:
    This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy’s value – assuming coverage (i.e. the target policy doesn’t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.

## Offline Policy Learning / Optimization

Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.

1. Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.

2. Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.

3. Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.

## Challenges in Offline Policy Optimization

1. Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.

2. Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.

3. Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.


Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.


## Mental Map

```text
                 Offline / Batch Reinforcement Learning
        Goal: Learn and evaluate policies from fixed historical data
           when exploration is unsafe, expensive, or impossible
                                │
                                ▼
              Why Online RL Is Not Always Feasible
 ┌─────────────────────────────────────────────────────────────┐
 │ Exploration can be dangerous (healthcare, driving, robotics)│
 │ Data already exists from past decisions                     │
 │ Real systems cannot reset or freely experiment              │
 │ We must learn without interacting with the environment      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Offline RL vs Standard RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Standard RL:                                                │
 │  – Collect data with current policy                         │
 │  – Explore → improve → repeat                               │
 │                                                             │
 │ Offline RL:                                                 │
 │  – Fixed dataset D from behavior policy π_b                 │
 │  – No new interaction allowed                               │
 │  – Must generalize only from observed data                  │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Why “Just Use Q-Learning” Fails Offline
 ┌─────────────────────────────────────────────────────────────┐
 │ Q-learning is off-policy — but not offline-safe             │
 │ Deadly triad:                                               │
 │   • Bootstrapping                                           │
 │   • Function approximation                                  │
 │   • Off-policy learning                                     │
 │ Leads to divergence & overestimation                        │
 │ Especially severe with distribution mismatch                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Offline RL Decomposed into Two Core Problems
 ┌───────────────────────────────┬─────────────────────────────┐
 │ 1. Policy Evaluation (OPE)    │ 2. Policy Optimization      │
 │    “How good is this policy?” │    “How can we improve it?” │
 │    Without running it         │    Without new data         │
 └───────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
            Batch / Offline Policy Evaluation (OPE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate V^π or J(π) using only dataset D                   │
 │ Three major approaches:                                     │
 │  1. Model-based evaluation                                  │
 │  2. Model-free evaluation (FQE)                             │
 │  3. Importance Sampling                                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        1. Model-Based Offline Policy Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Learn a model from data:                                    │
 │   • Reward model: r̂(s,a)                                    │
 │   • Transition model: P̂(s'|s,a)                             │
 │ Treat batch data as supervised learning                     │
 │ Then simulate policy π inside learned model                 │
 │ Use Bellman backups on (P̂, r̂)                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Model-Based OPE: Key Limitation
 ┌─────────────────────────────────────────────────────────────┐
 │ Model is only reliable where data exists                    │
 │ Policy visiting unseen states/actions → extrapolation error │
 │ Model hallucination → highly biased value estimates         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)
 ┌─────────────────────────────────────────────────────────────┐
 │ Evaluate a *fixed policy* π                                 │
 │ Learn Q^π(s,a) from offline data via regression             │
 │ Bellman target:                                             │
 │   y = c + γ Q(s', π(s'))                                    │
 │ Pure supervised learning loop                               │
 │ Stable compared to Q-learning / DQN                         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             FQE vs DQN (Key Insight)
 ┌─────────────────────────────┬─────────────────────────────┐
 │ DQN                         │ FQE                         │
 │ Learns optimal policy       │ Evaluates fixed policy      │
 │ Uses max over actions       │ Uses given π(s')            │
 │ Online data collection      │ Fully offline               │
 │ Overestimation bias         │ No max → more stable        │
 └─────────────────────────────┴─────────────────────────────┘
                                │
                                ▼
        3. Importance Sampling (IS) Evaluation
 ┌─────────────────────────────────────────────────────────────┐
 │ Treat OPE as statistical estimation                         │
 │ Reweight trajectories by π / π_b                            │
 │ Unbiased if coverage holds                                  │
 │ Severe variance for long horizons or policy mismatch        │
 │ Variants:                                                   │
 │   • Per-decision IS                                         │
 │   • Weighted IS                                             │
 │   • Doubly robust estimators                                │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
            Offline Policy Optimization
 ┌─────────────────────────────────────────────────────────────┐
 │ Goal: improve policy using only dataset D                   │
 │ Model-free: Fitted Q Iteration (FQI)                        │
 │ Model-based: planning inside learned model                  │
 │ Core challenge: distribution shift                          │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        The Central Problem: Distribution Mismatch
 ┌─────────────────────────────────────────────────────────────┐
 │ Learned policy chooses actions unseen in data               │
 │ Q-values extrapolate → overly optimistic                    │
 │ Performance collapses at deployment                         │
 │ Offline RL ≠ just off-policy RL                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
        Conservative / Pessimistic Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Assume unknown actions are risky                            │
 │ Penalize state-action pairs with low data support           │
 │ Prefer policies close to behavior policy                    │
 │ Examples (conceptually):                                    │
 │   • Conservative Q-Learning (CQL)                           │
 │   • Regularization toward π_b                               │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Key Challenges in Offline RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Coverage / overlap requirement                              │
 │ Model misspecification                                      │
 │ Value overestimation                                        │
 │ Bias–variance tradeoffs                                     │
 │ Safety vs optimality                                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Final Takeaway (Chapter Summary)
 ┌─────────────────────────────────────────────────────────────┐
 │ Offline RL learns entirely from past experience             │
 │ Policy evaluation is foundational before optimization       │
 │ Model-based, FQE, and IS provide OPE tools                  │
 │ Main risk: distribution shift & extrapolation               │
 │ Conservative methods trade performance for safety           │
 │ Offline RL is essential for real-world decision systems     │
 └─────────────────────────────────────────────────────────────┘
```
