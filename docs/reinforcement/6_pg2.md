# Chapter 6: Policy Gradient Variance Reduction and Actor-Critic

In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy $\pi_\theta(a|s)$ in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:

- High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return $G_t$ depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.

- Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.

- Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.

- Parameter Space vs Policy Space Mismatch: A small change in policy parameters $\theta$ does not always translate to a small change in the policy’s behavior. For example, in a two-action policy with probability $\pi_\theta(a=1)=\sigma(\theta)$ (sigmoid), a slight shift in $\theta$ can swing the action probabilities significantly if $\theta$ is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.

The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor–critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.

## Policy Gradient Theorem and REINFORCE

n the previous chapter, we derived an expression for the gradient of the policy objective:

$$
\nabla_\theta V(\theta) \approx
\frac{1}{m} \sum_{i=1}^{m}
R(\tau^{(i)}) 
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)}),
$$

where each trajectory $\tau^{(i)}$ is generated by the current policy $\pi_\theta$.

This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:

- The return $R(\tau)$ depends on the entire trajectory.
- Different trajectories can have very different returns.
- Updates become noisy, unstable, and slow to converge.

In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.

Goal:  
Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.

### Reducing Variance with Baselines
One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action’s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return.
Mathematically, we modify the gradient as follows:

$$
\nabla_\theta \mathbb{E}_\tau [R] \;=\;
\mathbb{E}_\tau \left[
\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t \mid s_t)\;
\left(\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\right)
\right].
$$


where $b(s_t)$ is an arbitrary baseline that depends on the state $s_t$ (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, $b(s_t)$ represents a reference level for the return at state $s_t$; the term $(G_t - b(s_t))$ is asking: did the action at $s_t$ do better or worse than this baseline expectation?

A particularly good choice for the baseline is the value function under the current policy, $b(s_t) \approx V_{\pi}(s_t)$. This is the expected return from state $s_t$ if we continue following the current policy. Using $V_{\pi}(s_t)$ as $b(s_t)$ minimizes variance because it subtracts out the expected part of $G_t$, leaving only the unexpected advantage of the action $a_t$.
Using a value function baseline leads to defining the advantage function:
$$ A(s_t, a_t) \;=\; Q(s_t, a_t) - V(s_t), $$
where $Q(s_t,a_t)$ is the expected return for taking action $a_t$ in $s_t$ and following the policy thereafter, and $V(s_t)$ is the expected return from $s_t$ on average. The advantage $A(s_t,a_t)$ tells us how much better or worse the chosen action was compared to the policy’s typical action at that state. If $A(s_t,a_t)$ is positive, the action did better than expected; if negative, it did worse than expected. Using $A(s_t,a_t)$ in the gradient update focuses learning on the deviations from usual outcomes.

Benefits of Using a Baseline (Advantage):
1.	Variance Reduction: Subtracting $V(s_t)$ removes the predictable part of the return, reducing the variability of the term $(G_t - b(s_t))$. The policy update then depends on advantage, which typically has lower variance than raw returns.
2.	Focused Learning: The update ignores outcomes that are “as expected” and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment.
3.	Unbiased Gradient: Because $b(s_t)$ does not depend on the action, the expected value of $\nabla_\theta \log \pi_\theta(a_t|s_t)\, b(s_t)$ is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance

Using a baseline in practice usually means we need to estimate the value function $V_{\pi}(s)$ for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress $b(s)$ toward the observed returns $G_t$ to improve the baseline estimate.

Algorithm: Policy Gradient with Baseline (Advantage Estimation)

1: Initialize policy parameter $\theta$, baseline $b(s)$  
2: for iteration $= 1, 2, \dots$ do  
3: $\quad$ Collect a set of trajectories by executing the current policy $\pi_\theta$  
4: $\quad$ for each trajectory $\tau^{(i)}$ and each timestep $t$ do  
5: $\quad\quad$ Compute return:  
$\quad\quad\quad G_t^{(i)} = \sum_{t'=t}^{T-1} r_{t'}^{(i)}$  
6: $\quad\quad$ Compute advantage estimate:  
$\quad\quad\quad \hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})$  
7: $\quad$ end for  
8: $\quad$ Re-fit baseline by minimizing:  
$\quad\quad \sum_i \sum_t \big(b(s_t^{(i)}) - G_t^{(i)}\big)^2$  
9: $\quad$ Update policy parameters using gradient estimate:  
$\quad\quad \theta \leftarrow \theta + \alpha \sum_{i,t} \nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)})\, \hat{A}_t^{(i)}$  
10: $\quad$ (Plug into SGD or Adam optimizer)  
11: end for  




This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect ($b(s)=V_\pi(s)$), then $\hat{A}t = G_t - V\pi(s_t)$ is an estimate of the advantage $A(s_t,a_t)$. Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.


## Actor–Critic Methods
Using a learned baseline brings us to the idea of actor–critic algorithms. In the policy gradient with baseline above, the policy is the "actor" and the value function baseline is a "critic" that evaluates the actor’s decisions. Actor–critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.


 - Actor: the policy $\pi_\theta(a|s)$ that selects actions and is updated by gradient ascent.
 - Critic: a value function $V_w(s)$ (with parameters $w$) that estimates the return from state $s$ under the current policy. The critic provides the baseline or advantage estimates used in the actor’s update.

Instead of waiting for full episode returns $G_t$, an actor–critic uses the critic’s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic’s current estimate of advantage $A(s_t,a_t)$, and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.

Actor update: The policy (actor) update is similar to before, but using the critic’s advantage estimate $A_t$ at time $t$:
$$ \theta \;\leftarrow\; \theta + \alpha\, \nabla_\theta \log \pi_\theta(a_t \mid s_t)\; A_t. $$
Here $A_t \approx Q(s_t,a_t) - V(s_t)$ is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive $A_t$) and away from actions that seem worse than expected.

Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error $\delta_t$ to update $w$:
$$ \delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t), $$
which measures the discrepancy between the predicted value at $s_t$ and the reward plus discounted value of the next state. The critic’s parameters $w$ are updated by a gradient step proportional to $\delta_t \nabla_w V_w(s_t)$ (this is essentially a TD(0) update). In practice:
$$ w \;\leftarrow\; w + \beta\, \delta_t\, \nabla_w V_w(s_t), $$
with $\beta$ a critic learning rate. This update pushes the critic’s value estimate $V_w(s_t)$ toward the observed reward plus the estimated value of $s_{t+1}$. The TD error $\delta_t$ is also used as an advantage estimate for the actor: notice $\delta_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)$ serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.

Why Actor–Critic? Actor–critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor–critic algorithms tend to:
- Learn faster (lower variance updates thanks to the critic’s guidance)
- Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends)
- Naturally handle continuing (non-episodic) tasks via the critic’s ongoing value estimates
- Still allow stochastic policies and continuous actions (since the actor is explicit)

However, actor–critics introduce bias through the critic’s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor–critic; later algorithms address this).

Advantage Estimation: In an actor–critic, one often uses n-step returns or more generally $\lambda$-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return $R^{(3)}t = r_t + \gamma r{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3})$, and define $\hat{A}_t = R^{(3)}_t - V(s_t)$. Smaller $n$ gives lower variance but more bias; larger $n$ gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.

In summary, the actor–critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy’s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.

## Limitations of Vanilla Policy Gradient and Trust-Region Motivation
Despite using baselines and even actor–critic methods, vanilla policy gradient algorithms (including basic actor–critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:

- On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy $\pi_{\text{old}}$ to update the current policy $\pi_{\text{new}}$ introduces bias, because the gradient formula assumes data comes from $\pi_{\text{new}}$. In short, standard policy gradients waste a lot of data, making them sample-inefficient.

- Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios $r_t = \frac{\pi_{\text{new}}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$. This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if $\pi_{\text{new}}$ differs significantly from $\pi_{\text{old}}$. A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.

- Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile – even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not “too large” in terms of its impact on the policy’s behavior.

- Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in $\theta$-space. A small change in parameters can lead to a disproportionate change in the policy’s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes ≠ small policy changes, it’s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.

These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update – in other words, to stay within a “trust region” around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback–Leibler divergence as a distance between $\pi_{\text{new}}$ and $\pi_{\text{old}}$), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor–critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.