<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/reinforcement/4_model_free_control/">
      
      
        <link rel="prev" href="../3_modelfree/">
      
      
        <link rel="next" href="../5_policy_gradient/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>4. Model-Free Control - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-4-model-free-control-learning-optimal-behavior-without-a-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              4. Model-Free Control
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs &amp; Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#from-state-values-to-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      From State Values to Action Values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration vs. Exploitation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-control" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo Control
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-temporal-difference-td-control" class="md-nav__link">
    <span class="md-ellipsis">
      IV. Temporal Difference (TD) Control
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#value-function-approximation-vfa" class="md-nav__link">
    <span class="md-ellipsis">
      Value Function Approximation (VFA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-free-control-mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Model Free Control Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#from-state-values-to-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      From State Values to Action Values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration vs. Exploitation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-control" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo Control
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-temporal-difference-td-control" class="md-nav__link">
    <span class="md-ellipsis">
      IV. Temporal Difference (TD) Control
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#value-function-approximation-vfa" class="md-nav__link">
    <span class="md-ellipsis">
      Value Function Approximation (VFA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-free-control-mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Model Free Control Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/reinforcement/4_model_free_control.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/reinforcement/4_model_free_control.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-4-model-free-control-learning-optimal-behavior-without-a-model">Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model<a class="headerlink" href="#chapter-4-model-free-control-learning-optimal-behavior-without-a-model" title="Permanent link">¶</a></h1>
<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy <span class="arithmatex">\(\pi^*\)</span> without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>
<h2 id="from-state-values-to-action-values">From State Values to Action Values<a class="headerlink" href="#from-state-values-to-action-values" title="Permanent link">¶</a></h2>
<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p>
<p>
<script type="math/tex; mode=display">
\pi_{k+1}(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^{\pi_k}(s') \right]
</script>
This update requires two things:</p>
<ul>
<li>the transition probabilities <span class="arithmatex">\(P(s'|s,a)\)</span></li>
<li>the expected reward R(s,a)$</li>
</ul>
<p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p>
<p>Instead of learning the state-value function <span class="arithmatex">\(V^\pi(s)\)</span> and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.
<script type="math/tex; mode=display">
Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]
</script>
</p>
<p>The Model-Free Policy Iteration loop:</p>
<ol>
<li>Policy Evaluation: Compute <span class="arithmatex">\(Q^{\pi}\)</span> from experience.</li>
<li>Policy Improvement: Update the policy <span class="arithmatex">\(\pi\)</span> given the estimated <span class="arithmatex">\(Q^{\pi}\)</span>.</li>
</ol>
<p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>
<h2 id="exploration-vs-exploitation">Exploration vs. Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Permanent link">¶</a></h2>
<p>To learn optimal behavior, the agent must balance two goals:</p>
<ol>
<li>Exploitation: choose actions believed to yield high rewards.</li>
<li>Exploration: try actions whose consequences are uncertain or poorly understood.</li>
</ol>
<p>A common solution is the <span class="arithmatex">\(\epsilon\)</span>-greedy policy:</p>
<p>With probability <span class="arithmatex">\(1 - \epsilon\)</span>, choose the action with the highest estimated value.<br>
With probability <span class="arithmatex">\(\epsilon\)</span>, choose a random action.</p>
<p>Formally:</p>
<div class="arithmatex">\[
\pi(a|s) =
\begin{cases}
1 - \epsilon + \frac{\epsilon}{|A|} &amp; \text{if } a = \arg\max_{a'} Q(s,a') \\
\frac{\epsilon}{|A|} &amp; \text{otherwise}
\end{cases}
\]</div>
<p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>
<h2 id="monte-carlo-control">Monte Carlo Control<a class="headerlink" href="#monte-carlo-control" title="Permanent link">¶</a></h2>
<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating <span class="arithmatex">\(V(s)\)</span>, it estimates <span class="arithmatex">\(Q(s,a)\)</span> using sampled returns.</p>
<p>Monte Carlo Policy Evaluation, Now for Q:      <br>
1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  <span class="arithmatex">\(k=1\)</span>,  Input <span class="arithmatex">\(\epsilon=1\)</span>, <span class="arithmatex">\(\pi\)</span><br>
2: loop over epiosdes     <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
4: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
5: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
6: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
9: <span class="arithmatex">\(\quad\quad\)</span>       end if<br>
10: <span class="arithmatex">\(\quad\)</span>  end for<br>
11: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span><br>
12: end loop</p>
<p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using <span class="arithmatex">\(\epsilon\)</span>-greedy policies for exploration.</p>
<ul>
<li>Policy Evaluation: <span class="arithmatex">\(Q(s, a)\)</span> is updated using the full return (<span class="arithmatex">\(G_t\)</span>) observed after the state-action pair <span class="arithmatex">\((s_t, a_t)\)</span> has occurred in an episode. The incremental update uses the formula <span class="arithmatex">\(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\)</span>. </li>
<li>Policy Improvement: The new policy <span class="arithmatex">\(\pi_{k+1}\)</span> is set to be <span class="arithmatex">\(\epsilon\)</span>-greedy with respect to the updated <span class="arithmatex">\(Q\)</span> function.</li>
</ul>
<h3 id="greedy-in-the-limit-of-infinite-exploration-glie">Greedy in the Limit of Infinite Exploration (GLIE)<a class="headerlink" href="#greedy-in-the-limit-of-infinite-exploration-glie" title="Permanent link">¶</a></h3>
<p>For Monte Carlo Control to converge to the optimal action-value function <span class="arithmatex">\(Q^*(s, a)\)</span>, the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p>
<ol>
<li>Infinite Visits: All state-action pairs <span class="arithmatex">\((s, a)\)</span> must be visited an infinite number of times (<span class="arithmatex">\(\lim_{i \rightarrow \infty} N_i(s, a) \rightarrow \infty\)</span>).</li>
<li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li>
</ol>
<p>A simple strategy to satisfy GLIE is to use an <span class="arithmatex">\(\epsilon\)</span>-greedy policy where <span class="arithmatex">\(\epsilon\)</span> is decayed over time, such as <span class="arithmatex">\(\epsilon_i = 1/i\)</span> (where <span class="arithmatex">\(i\)</span> is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function <span class="arithmatex">\(Q^*(s, a)\)</span>.</p>
<p>Monte Carlo Online Control/On Policy Improvement:    </p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0\)</span>, <span class="arithmatex">\(N(s,a)=0\)</span>  <span class="arithmatex">\(\forall(s,a)\)</span>,  Set <span class="arithmatex">\(k=1\)</span>, <span class="arithmatex">\(\epsilon=1\)</span>.   <br>
2: <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> // Create initial <span class="arithmatex">\(\epsilon\)</span> - greedy policy.<br>
3: loop over epiosdes   <br>
4: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,T})\)</span> given <span class="arithmatex">\(\pi\)</span><br>
5: <span class="arithmatex">\(\quad\)</span> Compute <span class="arithmatex">\(G_{k,t} = r_{k,t} + \gamma r_{k,t+1} + \gamma^2 r_{k,t+2} + \dots + \gamma^{T-t-1} r_{k,T}\)</span>  <span class="arithmatex">\(\forall t\)</span><br>
6: <span class="arithmatex">\(\quad\)</span>   for <span class="arithmatex">\(t = 1, \dots, T\)</span> do<br>
7: <span class="arithmatex">\(\quad\quad\)</span>      if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then<br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span>          <span class="arithmatex">\(N(s,a) = N(s,a) + 1\)</span><br>
9: <span class="arithmatex">\(\quad\quad\quad\)</span>           <span class="arithmatex">\(Q(s_t,a_t) = Q(s_t,a_t) + \dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\)</span><br>
10: <span class="arithmatex">\(\quad\quad\)</span>       end if <br>
11: <span class="arithmatex">\(\quad\)</span>  end for  <br>
12:  <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi_k = \epsilon - greedy (Q)\)</span> //Policy improvement<br>
12: <span class="arithmatex">\(\quad\)</span>  <span class="arithmatex">\(k = k + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{k}\)</span>   <br>
13: end loop    </p>
<p>This process gradually adjusts the policy and the value estimates until they converge.</p>
<h2 id="iv-temporal-difference-td-control">IV. Temporal Difference (TD) Control<a class="headerlink" href="#iv-temporal-difference-td-control" title="Permanent link">¶</a></h2>
<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>
<h3 id="on-policy-td-control-sarsa">On-Policy TD Control: SARSA<a class="headerlink" href="#on-policy-td-control-sarsa" title="Permanent link">¶</a></h3>
<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy <em>currently being followed</em> (<span class="arithmatex">\(\pi\)</span>). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p>
<p>The update for the action-value <span class="arithmatex">\(Q(s_t, a_t)\)</span> uses the value of the <em>next</em> state-action pair, <span class="arithmatex">\((s_{t+1}, a_{t+1})\)</span>, selected by the current policy <span class="arithmatex">\(\pi\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</div>
<p>The TD Target here is <span class="arithmatex">\(r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})\)</span>. SARSA learns <span class="arithmatex">\(Q^{\pi}\)</span> while <span class="arithmatex">\(\pi\)</span> is improved greedily with respect to <span class="arithmatex">\(Q^{\pi}\)</span>, allowing it to find the optimal policy <span class="arithmatex">\(\pi^*\)</span>.</p>
<p>1: Set initial <span class="arithmatex">\(\epsilon\)</span>-greedy policy <span class="arithmatex">\(\pi\)</span> randomly, <span class="arithmatex">\(t=0\)</span>, initial state <span class="arithmatex">\(s_t=s_0\)</span>    <br>
2: Take <span class="arithmatex">\(a_t \sim \pi(s_t)\)</span>      <br>
3: Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>       <br>
4: loop       <br>
5: <span class="arithmatex">\(\quad\)</span> Take action <span class="arithmatex">\(a_{t+1} \sim \pi(s_{t+1})\)</span> // Sample action from policy        <br>
6: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_{t+1}, s_{t+2})\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Update <span class="arithmatex">\(Q\)</span> given <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\)</span>:    <script type="math/tex; mode=display">
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]</script>
8: <span class="arithmatex">\(\quad\)</span> Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p>
<div class="arithmatex">\[\forall s \in S,\;\;
\pi(s) =
\begin{cases}
\arg\max\limits_a Q(s,a) &amp; \text{with probability } 1 - \epsilon \\
\text{a random action}   &amp; \text{with probability } \epsilon
\end{cases}\]</div>
<p>9: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span> , <span class="arithmatex">\(\epsilon = \frac{1}{t}\)</span>           <br>
10: end loop        </p>
<h3 id="b-off-policy-td-control-q-learning">B. Off-Policy TD Control: Q-Learning<a class="headerlink" href="#b-off-policy-td-control-q-learning" title="Permanent link">¶</a></h3>
<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (<span class="arithmatex">\(\pi^*\)</span>, the <em>target policy</em>) using experience gathered by a different behavior policy (<span class="arithmatex">\(\pi_b\)</span>).</p>
<p>In Q-Learning, the agent acts using a soft, exploratory <span class="arithmatex">\(\pi_b\)</span> (like <span class="arithmatex">\(\epsilon\)</span>-greedy) but the value function update is based on the <em>best</em> possible action from the next state, effectively estimating <span class="arithmatex">\(Q^*\)</span>.</p>
<div class="arithmatex">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]</div>
<p>The key difference is the target: Q-Learning uses the value of the max action (<span class="arithmatex">\(\max_{a'} Q(s_{t+1}, a')\)</span>), regardless of what action was actually taken in the next step. This makes it a greedy update towards <span class="arithmatex">\(Q^*\)</span>.</p>
<p>Q-Learning (Off-Policy TD Control):</p>
<p>1: Initialize <span class="arithmatex">\(Q(s,a)=0 \quad \forall s \in S, a \in A\)</span>, set <span class="arithmatex">\(t = 0\)</span>, initial state <span class="arithmatex">\(s_t = s_0\)</span>       <br>
2: Set <span class="arithmatex">\(\pi_b\)</span> to be <span class="arithmatex">\(\epsilon\)</span>-greedy w.r.t. <span class="arithmatex">\(Q\)</span>     <br>
3: loop   <br>
4: <span class="arithmatex">\(\quad\)</span> Take <span class="arithmatex">\(a_t \sim \pi_b(s_t)\)</span> // Sample action from behavior policy   <br>
5: <span class="arithmatex">\(\quad\)</span> Observe <span class="arithmatex">\((r_t, s_{t+1})\)</span>   <br>
6: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[ r_t + \gamma \max\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \right]\)</span>      <br>
7: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\pi(s_t) =
\begin{cases}
\arg\max\limits_a Q(s_t,a) &amp; \text{with probability } 1 - \epsilon \
\text{a random action} &amp; \text{with probability } \epsilon
\end{cases}\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>    <br>
9: end loop     </p>
<h2 id="value-function-approximation-vfa">Value Function Approximation (VFA)<a class="headerlink" href="#value-function-approximation-vfa" title="Permanent link">¶</a></h2>
<p>All methods discussed so far assume a tabular representation, where a separate entry for <span class="arithmatex">\(Q(s, a)\)</span> is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>
<h3 id="motivation-for-approximation">Motivation for Approximation<a class="headerlink" href="#motivation-for-approximation" title="Permanent link">¶</a></h3>
<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p>
<ol>
<li>Memory: Explicitly storing every <span class="arithmatex">\(V\)</span> or <span class="arithmatex">\(Q\)</span> value is impossible.</li>
<li>Computation: Computing or updating every value is too slow.</li>
<li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li>
</ol>
<p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w}) \approx Q(s, a)\)</span>. The goal shifts from filling a table to finding the parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that minimizes the error between the true value and the estimate.</p>
<div class="arithmatex">\[
J(\mathbf{w}) = \mathbb{E}_{\pi} \left[ \left( Q^{\pi}(s, a) - \hat{Q}(s, a; \mathbf{w}) \right)^2 \right]
\]</div>
<p>The parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function <span class="arithmatex">\(J(\mathbf{w})\)</span>.</p>
<h3 id="model-free-control-with-vfa-policy-evaluation">Model-Free Control with VFA Policy Evaluation<a class="headerlink" href="#model-free-control-with-vfa-policy-evaluation" title="Permanent link">¶</a></h3>
<p>When using function approximation, we substitute the old <span class="arithmatex">\(Q(s, a)\)</span> in the update rules (MC, SARSA, Q-Learning) with the function approximator <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span>.</p>
<ul>
<li>
<p>MC VFA for Policy Evaluation: </p>
<p>The return <span class="arithmatex">\(G_t\)</span> is used as the target in an SGD update: <span class="arithmatex">\(\Delta \mathbf{w} \propto \alpha (G_t - \hat{Q}(s_t, a_t; \mathbf{w})) \nabla_{\mathbf{w}} \hat{Q}(s_t, a_t; \mathbf{w})\)</span>.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, set <span class="arithmatex">\(k = 1\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Sample k-th episode <span class="arithmatex">\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \dots, s_{k,L_k})\)</span> given <span class="arithmatex">\(\pi\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1, \dots, L_k\)</span> do     <br>
5: <span class="arithmatex">\(\quad\quad\)</span> if First visit to <span class="arithmatex">\((s,a)\)</span> in episode <span class="arithmatex">\(k\)</span> then     <br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(G_t(s,a) = \sum_{j=t}^{L_k} r_{k,j}\)</span>    <br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 \left[ G_t(s,a) - \hat{Q}(s_t,a_t;\mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})\)</span> // Compute Gradient   <br>
8: <span class="arithmatex">\(\quad\quad\quad\)</span> Update weights: <span class="arithmatex">\(\Delta \mathbf{w}\)</span>      <br>
9: <span class="arithmatex">\(\quad\quad\)</span> end if<br>
10: <span class="arithmatex">\(\quad\)</span> end for 
11: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(k = k + 1\)</span>       <br>
12: end loop    </p>
</li>
<li>
<p>SARSA with VFA: The TD target is <span class="arithmatex">\(r + \gamma \hat{Q}(s', a'; \mathbf{w})\)</span>, leveraging the current function approximation.</p>
<p>1: Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(s\)</span>   <br>
2: loop   <br>
3: <span class="arithmatex">\(\quad\)</span> Given <span class="arithmatex">\(s\)</span>, sample <span class="arithmatex">\(a \sim \pi(s)\)</span>, observe <span class="arithmatex">\(r(s,a)\)</span>, and <span class="arithmatex">\(s' \sim p(s'|s,a)\)</span>   <br>
4: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(\nabla_{\mathbf{w}} J(\mathbf{w}) = -2 [r + \gamma \hat{V}(s';\mathbf{w}) - \hat{V}(s;\mathbf{w})] \nabla_{\mathbf{w}} \hat{V}(s;\mathbf{w})\)</span>     <br>
5: <span class="arithmatex">\(\quad\)</span> Update weights <span class="arithmatex">\(\Delta \mathbf{w}\)</span>         <br>
6: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(s'\)</span> is not a terminal state then   <br>
7: <span class="arithmatex">\(\quad\quad\)</span> Set <span class="arithmatex">\(s = s'\)</span>      <br>
8: <span class="arithmatex">\(\quad\)</span> else       <br>
9: <span class="arithmatex">\(\quad\quad\)</span> Restart episode, sample initial state <span class="arithmatex">\(s\)</span>     <br>
10: <span class="arithmatex">\(\quad\)</span> end if    <br>
11: end loop      <br>
* Q-Learning with VFA: The TD target is <span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s', a'; \mathbf{w})\)</span>.</p>
</li>
</ul>
<h3 id="control-using-vfa">Control using VFA<a class="headerlink" href="#control-using-vfa" title="Permanent link">¶</a></h3>
<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular <span class="arithmatex">\(Q(s,a)\)</span> with a function approximator <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>, we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by <span class="arithmatex">\(\hat{Q}(s,a;\mathbf{w})\)</span>.</p>
<p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each <span class="arithmatex">\((s,a)\)</span>, the agent learns a parameter vector <span class="arithmatex">\(\mathbf{w}\)</span> that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function <span class="arithmatex">\(Q^*(s,a)\)</span>.</p>
<p>The learning problem becomes:</p>
<div class="arithmatex">\[
\min_{\mathbf{w}} \; J(\mathbf{w}) = \mathbb{E} \left[ \left( Q^*(s,a) - \hat{Q}(s,a;\mathbf{w}) \right)^2 \right]
\]</div>
<p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p>
<div class="arithmatex">\[
\Delta \mathbf{w} \propto \left( \text{target} - \hat{Q}(s_t,a_t;\mathbf{w}) \right) \nabla_{\mathbf{w}} \hat{Q}(s_t,a_t;\mathbf{w})
\]</div>
<p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Target for updating <span class="arithmatex">\(\mathbf{w}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Monte Carlo</td>
<td><span class="arithmatex">\(G_t\)</span></td>
</tr>
<tr>
<td>SARSA</td>
<td><span class="arithmatex">\(r + \gamma \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
<tr>
<td>Q-Learning</td>
<td><span class="arithmatex">\(r + \gamma \max_{a'} \hat{Q}(s',a';\mathbf{w})\)</span></td>
</tr>
</tbody>
</table>
<p>These methods now operate in the same way as before, except instead of updating a single <span class="arithmatex">\(Q(s,a)\)</span> entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>
<h3 id="challenges-the-deadly-triad">Challenges: The Deadly Triad<a class="headerlink" href="#challenges-the-deadly-triad" title="Permanent link">¶</a></h3>
<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p>
<div class="arithmatex">\[
\text{Function Approximation} \;+\; \text{Bootstrapping} \;+\; \text{Off-policy Learning}
\]</div>
<p>This combination is known as the Deadly Triad .</p>
<ul>
<li>Function Approximation : Generalizes across states but may introduce bias.</li>
<li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li>
<li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li>
</ul>
<p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p>
<p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate <span class="arithmatex">\(Q(s,a)\)</span>.</p>
<h3 id="deep-q-networks-dqn">Deep Q-Networks (DQN)<a class="headerlink" href="#deep-q-networks-dqn" title="Permanent link">¶</a></h3>
<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p>
<p>DQN stabilizes the non-linear learning process using two critical techniques:</p>
<ol>
<li>
<p>Experience Replay (ER): Transitions <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> are stored in a replay buffer (<span class="arithmatex">\(\mathcal{D}\)</span>). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from <span class="arithmatex">\(\mathcal{D}\)</span> for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p>
</li>
<li>
<p>Fixed Q-Targets: The Q-Learning update requires a target value <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w})\)</span>. To prevent the estimate <span class="arithmatex">\(\hat{Q}(s, a; \mathbf{w})\)</span> from chasing its own rapidly changing target, the parameters <span class="arithmatex">\(\mathbf{w}^{-}\)</span> used to compute the target are fixed for a period of time, then synchronized with the current parameters <span class="arithmatex">\(\mathbf{w}\)</span>. This provides a stable target <span class="arithmatex">\(y_i = r_i + \gamma \max_{a'} \hat{Q}(s_{i+1}, a'; \mathbf{w}^{-})\)</span>.</p>
</li>
</ol>
<p>Deep Q-Network (DQN) Algorithm:</p>
<p>1: Input <span class="arithmatex">\(C\)</span>, <span class="arithmatex">\(\alpha\)</span>, <span class="arithmatex">\(D = {}\)</span>, Initialize <span class="arithmatex">\(\mathbf{w}\)</span>, <span class="arithmatex">\(\mathbf{w}^- = \mathbf{w}\)</span>, <span class="arithmatex">\(t = 0\)</span>       <br>
2: Get initial state <span class="arithmatex">\(s_0\)</span>            <br>
3: loop       <br>
4: <span class="arithmatex">\(\quad\)</span> Sample action <span class="arithmatex">\(a_t\)</span> using <span class="arithmatex">\(\epsilon\)</span>-greedy policy w.r.t. current <span class="arithmatex">\(\hat{Q}(s_t, a; \mathbf{w})\)</span>    <br>
5: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span>      <br>
6: <span class="arithmatex">\(\quad\)</span> Store transition <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> in replay buffer <span class="arithmatex">\(D\)</span>   <br>
7: <span class="arithmatex">\(\quad\)</span> Sample a random minibatch of tuples <span class="arithmatex">\((s_i, a_i, r_i, s'i)\)</span> from <span class="arithmatex">\(D\)</span>    <br>
8: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(j\)</span> in minibatch do    <br>
9: <span class="arithmatex">\(\quad\quad\)</span> if episode terminates at step <span class="arithmatex">\(i+1\)</span> then      <br>
10: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i\)</span>     <br>
11: <span class="arithmatex">\(\quad\quad\)</span> else     <br>
12: <span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(y_i = r_i + \gamma \max\limits{a'} \hat{Q}(s'i, a'; \mathbf{w}^-)\)</span>     <br>
13: <span class="arithmatex">\(\quad\quad\)</span> end if   <br>
14: <span class="arithmatex">\(\quad\quad\)</span> Update <span class="arithmatex">\(\mathbf{w}\)</span> using gradient descent:  <br>
<span class="arithmatex">\(\quad\quad\quad\)</span> <span class="arithmatex">\(\Delta \mathbf{w} = \alpha \left( y_i - \hat{Q}(s_i, a_i; \mathbf{w}) \right) \nabla{\mathbf{w}} \hat{Q}(s_i, a_i; \mathbf{w})\)</span>    <br>
15: <span class="arithmatex">\(\quad\)</span> end for       <br>
16: <span class="arithmatex">\(\quad\)</span> <span class="arithmatex">\(t = t + 1\)</span>   <br>
17: <span class="arithmatex">\(\quad\)</span> if <span class="arithmatex">\(t \mod C == 0\)</span> then   <br>
18: <span class="arithmatex">\(\quad\quad\)</span> <span class="arithmatex">\(\mathbf{w}^- \leftarrow \mathbf{w}\)</span>     <br>
19: <span class="arithmatex">\(\quad\)</span> end if    <br>
20: end loop        </p>
<h2 id="model-free-control-mental-map">Model Free Control Mental Map<a class="headerlink" href="#model-free-control-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>                     Model-Free Control
    Goal: Learn the Optimal Policy π* without knowing P or R
                               │
                               ▼
           Key Concept: Action-Value Function Q(s,a)
       ┌─────────────────────────────────────────────┐
       │Qπ(s,a) = Expected return by taking action a │
       │in state s and following policy π thereafter │
       └─────────────────────────────────────────────┘
                               │
                      No model → Learn Q directly
                               │
                               ▼
                   Generalized Policy Iteration
       ┌───────────────────────────┬───────────────────────────┐
       │   Policy Evaluation       │     Policy Improvement    │
       │   Learn Qπ(s,a)           │   π ← greedy w.r.t Q      │
       └───────────────────────────┴───────────────────────────┘
                               │
                               ▼
                Challenge: Exploration vs. Exploitation
       ┌──────────────────────────────────────────────────────┐
       │Greedy policy → Exploits but stops exploring          │
       │ε-greedy policy → Balances exploration &amp; exploitation │
       │GLIE condition: ε → 0 and ∞ exploration               │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                Model-Free Control Families (Tabular)
       ┌────────────────────────────┬────────────────────────────┐
       │   Monte Carlo Control      │      Temporal Difference   │
       │   (Episode-based)          │      (Step-based)          │
       └────────────────────────────┴────────────────────────────┘
                               │
          ┌────────────────────┴───────────────────┐
          ▼                                        ▼
 Monte Carlo Control:                       TD Control:
 Estimates Q from full returns          Estimates Q usingbootstrapped targets
 Uses ε-greedy policy                   Works online, faster, low variance
 Episodic only                          Works for episodic &amp; continuing
          │                                        │
    ┌─────┴─────────────┐             ┌────────────┴───────────────┐
    │ GLIE MC Control   │             │ On-Policy TD: SARSA        │
    └───────────────────┘             │ Off-Policy TD: Q-Learning  │
                                      └────────────────────────────┘
                                                    |
                                                    ▼
┌──────────────────────────────────────────────────────────────────────────────┐
| On-Policy TD — SARSA                     |  Off-Policy TD — Q-Learning       |
| Learns Qπ for the policy being followed  |  Learns Q* while following π_b    |
| Update uses next action from π           |  Update uses max action (greedy)  |
| Update Target:                           |  Update Target:                   |
|  r + γ Q(s',a')                          |  r + γ maxₐ Q(s',a)               |
└────────────────────────────┴─────────────────────────────────────────────────┘



      Value Function Approximation (Large/Continuous spaces)
       ┌──────────────────────────────────────────────────────┐
       │ Replace Q(s,a) with Q̂(s,a;w) using function approx   │
       │ Generalization across states                         │
       │ Gradient-based updates (SGD)                         │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
            Deep Q-Learning (DQN) — Stable VFA Control
       ┌──────────────────────────────────────────────────────┐
       │ Experience Replay — decorrelate samples             │
       │ Target Networks — stabilize bootstrapped targets    │
       └──────────────────────────────────────────────────────┘
                               │
                               ▼
                      Final Outcome of Model-Free Control
       ┌───────────────────────────────────────────────────────┐
       │ Learn π* directly from experience without model       │
       │ Learn Q*(s,a) through MC, SARSA, or Q-Learning        │
       │ Scale to large spaces using function approximation    │
       │ DQN enables deep RL in complex environments           │
       └───────────────────────────────────────────────────────┘
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../3_modelfree/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 3. Model-Free Prediction">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                3. Model-Free Prediction
              </div>
            </div>
          </a>
        
        
          
          <a href="../5_policy_gradient/" class="md-footer__link md-footer__link--next" aria-label="Next: 5. Policy Gradient Methods">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                5. Policy Gradient Methods
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>