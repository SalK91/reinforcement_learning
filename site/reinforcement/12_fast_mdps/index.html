<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/reinforcement/12_fast_mdps/">
      
      
        <link rel="prev" href="../11_fast_rl/">
      
      
        <link rel="next" href="../13_montecarlo/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>12. Fast Reinforcement Learning in MDPs and Generalization - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-12-fast-reinforcement-learning-in-mdps-and-generalization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              12. Fast Reinforcement Learning in MDPs and Generalization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs &amp; Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_modelfree/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pac-framework-for-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      PAC Framework for MDPs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mbie-eb-model-based-interval-estimation-with-exploration-bonus" class="md-nav__link">
    <span class="md-ellipsis">
      MBIE-EB: Model-Based Interval Estimation with Exploration Bonus
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Model-Based Reinforcement Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      Key Characteristics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalization-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      Generalization in Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#strategic-exploration-in-deep-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Strategic Exploration in Deep RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling-for-deep-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Thompson Sampling for Deep RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pac-framework-for-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      PAC Framework for MDPs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mbie-eb-model-based-interval-estimation-with-exploration-bonus" class="md-nav__link">
    <span class="md-ellipsis">
      MBIE-EB: Model-Based Interval Estimation with Exploration Bonus
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayesian-model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Model-Based Reinforcement Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      Key Characteristics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalization-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      Generalization in Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#strategic-exploration-in-deep-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Strategic Exploration in Deep RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling-for-deep-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Thompson Sampling for Deep RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/reinforcement/12_fast_mdps.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/reinforcement/12_fast_mdps.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-12-fast-reinforcement-learning-in-mdps-and-generalization">Chapter 12: Fast Reinforcement Learning in MDPs and Generalization<a class="headerlink" href="#chapter-12-fast-reinforcement-learning-in-mdps-and-generalization" title="Permanent link">¶</a></h1>
<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p>
<ul>
<li>Bandits: Single-step decision-making problems.</li>
<li>MDPs: Sequential decision-making with transition dynamics.</li>
</ul>
<h3 id="evaluation-frameworks">Evaluation Frameworks<a class="headerlink" href="#evaluation-frameworks" title="Permanent link">¶</a></h3>
<p>To assess learning efficiency, we use:</p>
<ul>
<li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li>
<li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li>
<li>PAC (Probably Approximately Correct): Number of steps when the policy is not <span class="arithmatex">\(\epsilon\)</span>-optimal is bounded with high probability.</li>
</ul>
<h3 id="exploration-approaches">Exploration Approaches<a class="headerlink" href="#exploration-approaches" title="Permanent link">¶</a></h3>
<ul>
<li>Optimism under uncertainty (e.g., UCB)</li>
<li>Probability matching (e.g., Thompson Sampling)</li>
</ul>
<h2 id="pac-framework-for-mdps">PAC Framework for MDPs<a class="headerlink" href="#pac-framework-for-mdps" title="Permanent link">¶</a></h2>
<p>A reinforcement learning algorithm <span class="arithmatex">\(A\)</span> is PAC if with probability at least <span class="arithmatex">\(1 - \delta\)</span>, it selects an <span class="arithmatex">\(\epsilon\)</span>-optimal action on all but a bounded number of time steps <span class="arithmatex">\(N\)</span>, where:</p>
<div class="arithmatex">\[
N = \text{poly} \left( |S|, |A|, \frac{1}{1 - \gamma}, \frac{1}{\epsilon}, \frac{1}{\delta} \right)
\]</div>
<h2 id="mbie-eb-model-based-interval-estimation-with-exploration-bonus">MBIE-EB: Model-Based Interval Estimation with Exploration Bonus<a class="headerlink" href="#mbie-eb-model-based-interval-estimation-with-exploration-bonus" title="Permanent link">¶</a></h2>
<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function <span class="arithmatex">\(Q(s, a)\)</span>.</p>
<p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p>
<ul>
<li>
<p>It estimates <span class="arithmatex">\(\hat{R}(s, a)\)</span> and <span class="arithmatex">\(\hat{T}(s' \mid s, a)\)</span> from data using empirical counts.</p>
</li>
<li>
<p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p>
</li>
</ul>
<p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p>
<p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p>
<p>Algorithm:</p>
<p>1: Given <span class="arithmatex">\(\epsilon\)</span>, <span class="arithmatex">\(\delta\)</span>, <span class="arithmatex">\(m\)</span><br>
2: <span class="arithmatex">\(\beta = \dfrac{1}{1-\gamma}\sqrt{0.5 \ln \!\left(\dfrac{2|S||A|m}{\delta}\right)}\)</span><br>
3: <span class="arithmatex">\(n_{sas}(s,a,s') = 0\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A, s' \in S\)</span><br>
4: <span class="arithmatex">\(rc(s,a) = 0\)</span>, <span class="arithmatex">\(n_{sa}(s,a) = 0\)</span>, <span class="arithmatex">\(\hat{Q}(s,a) = \dfrac{1}{1-\gamma}\)</span>, <span class="arithmatex">\(\forall s \in S, a \in A\)</span><br>
5: <span class="arithmatex">\(t = 0\)</span>, <span class="arithmatex">\(s_t = s_{\text{init}}\)</span><br>
6: loop<br>
7: <span class="arithmatex">\(\quad a_t = \arg\max_{a \in A} \hat{Q}(s_t, a)\)</span><br>
8: <span class="arithmatex">\(\quad\)</span> Observe reward <span class="arithmatex">\(r_t\)</span> and state <span class="arithmatex">\(s_{t+1}\)</span><br>
9: <span class="arithmatex">\(\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\)</span>,<br>
<span class="arithmatex">\(\quad\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\)</span><br>
10: <span class="arithmatex">\(\quad rc(s_t,a_t) = \dfrac{rc(s_t,a_t)\big(n_{sa}(s_t,a_t)-1\big) + r_t}{n_{sa}(s_t,a_t)}\)</span><br>
11: <span class="arithmatex">\(\quad \hat{R}(s_t,a_t) = rc(s_t,a_t)\)</span> and<br>
<span class="arithmatex">\(\quad\quad \hat{T}(s' \mid s_t,a_t) = \dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\)</span>, <span class="arithmatex">\(\forall s' \in S\)</span><br>
12: <span class="arithmatex">\(\quad\)</span> while not converged do<br>
13: <span class="arithmatex">\(\quad\quad \hat{Q}(s,a) = \hat{R}(s,a) + \gamma \sum_{s'} \hat{T}(s' \mid s,a)\max_{a'} \hat{Q}(s',a') + \dfrac{\beta}{\sqrt{n_{sa}(s,a)}}\)</span>,<br>
<span class="arithmatex">\(\quad\quad\quad \forall s \in S, a \in A\)</span><br>
14: <span class="arithmatex">\(\quad\)</span> end while<br>
15: end loop</p>
<h2 id="bayesian-model-based-reinforcement-learning">Bayesian Model-Based Reinforcement Learning<a class="headerlink" href="#bayesian-model-based-reinforcement-learning" title="Permanent link">¶</a></h2>
<p>Bayesian RL methods maintain a posterior over MDP models <span class="arithmatex">\((P, R)\)</span> and sample plausible environments from the posterior to plan and act.</p>
<p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p>
<p>Formally, the Thompson sampling policy is:</p>
<div class="arithmatex">\[
\pi(s, a \mid h_t) = \mathbb{P}\left(Q(s, a) \ge Q(s, a'),\; \forall a' \ne a \;\middle|\; h_t \right)
= \mathbb{E}_{\mathcal{P}, \mathcal{R} \mid h_t} \left[ \mathbb{1}\left(a = \arg\max_{a \in \mathcal{A}} Q(s, a)\right) \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(h_t\)</span> is the history up to time <span class="arithmatex">\(t\)</span> (including all observed transitions and rewards),
- <span class="arithmatex">\(\mathcal{P}, \mathcal{R}\)</span> are the transition and reward functions respectively,
- The expectation is taken over the posterior belief on the MDP <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span>.</p>
<h3 id="thompson-sampling-algorithm-in-mdps">Thompson Sampling Algorithm in MDPs<a class="headerlink" href="#thompson-sampling-algorithm-in-mdps" title="Permanent link">¶</a></h3>
<ol>
<li>Maintain a posterior <span class="arithmatex">\(p(\mathcal{P}, \mathcal{R} \mid h_t)\)</span> over the transition and reward models based on all observed data.</li>
<li>Sample a model <span class="arithmatex">\((\mathcal{P}, \mathcal{R})\)</span> from the posterior distribution.</li>
<li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function <span class="arithmatex">\(Q^*(s, a)\)</span>.</li>
<li>Select the action according to the optimal action in the sampled model:
   <script type="math/tex; mode=display">
   a_t = \arg\max_{a \in \mathcal{A}} Q^*(s_t, a)
   </script>
</li>
</ol>
<h3 id="algorithm-thompson-sampling-for-mdps">Algorithm: Thompson Sampling for MDPs<a class="headerlink" href="#algorithm-thompson-sampling-for-mdps" title="Permanent link">¶</a></h3>
<p>1: Initialize prior over dynamics and reward models for each <span class="arithmatex">\((s, a)\)</span>:  <span class="arithmatex">\(\quad p(\mathcal{T}(s' \mid s, a)), \quad p(\mathcal{R}(s, a))\)</span><br>
2: Initialize initial state <span class="arithmatex">\(s_0\)</span><br>
3: for <span class="arithmatex">\(k = 1\)</span> to <span class="arithmatex">\(K\)</span> episodes do<br>
4: <span class="arithmatex">\(\quad\)</span> Sample an MDP <span class="arithmatex">\(\mathcal{M}\)</span>:<br>
5: <span class="arithmatex">\(\quad\quad\)</span> for each <span class="arithmatex">\((s, a)\)</span> pair do<br>
6: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample transition model <span class="arithmatex">\(\mathcal{T}(s' \mid s, a)\)</span> from posterior<br>
7: <span class="arithmatex">\(\quad\quad\quad\)</span> Sample reward model <span class="arithmatex">\(\mathcal{R}(s, a)\)</span> from posterior<br>
8: <span class="arithmatex">\(\quad\quad\)</span> end for<br>
9: <span class="arithmatex">\(\quad\)</span> Compute optimal value function <span class="arithmatex">\(Q_{\mathcal{M}}^*\)</span> for sampled MDP <span class="arithmatex">\(\mathcal{M}\)</span><br>
10: <span class="arithmatex">\(\quad\)</span> for <span class="arithmatex">\(t = 1\)</span> to <span class="arithmatex">\(H\)</span> do<br>
11: <span class="arithmatex">\(\quad\quad a_t = \arg\max_{a \in \mathcal{A}} Q_{\mathcal{M}}^*(s_t, a)\)</span><br>
12: <span class="arithmatex">\(\quad\quad\)</span> Take action <span class="arithmatex">\(a_t\)</span>, observe reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span><br>
13: <span class="arithmatex">\(\quad\)</span> end for<br>
14: <span class="arithmatex">\(\quad\)</span> Update posteriors: <span class="arithmatex">\(\quad\quad p(\mathcal{R}_{s_t, a_t} \mid r_t), \quad p(\mathcal{T}(s' \mid s_t, a_t) \mid s_{t+1})\)</span> using Bayes Rule<br>
15: end for</p>
<h2 id="key-characteristics">Key Characteristics<a class="headerlink" href="#key-characteristics" title="Permanent link">¶</a></h2>
<ul>
<li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li>
<li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li>
<li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li>
</ul>
<blockquote>
<p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>
</blockquote>
<h2 id="generalization-in-contextual-bandits">Generalization in Contextual Bandits<a class="headerlink" href="#generalization-in-contextual-bandits" title="Permanent link">¶</a></h2>
<p>Contextual bandits generalize standard bandits by associating a context or state <span class="arithmatex">\(s\)</span> with each decision:</p>
<ul>
<li>Reward depends on both context and action: <span class="arithmatex">\(r \sim P[r | s,a]\)</span></li>
<li>Often model reward as linear: <span class="arithmatex">\(r = \theta^\top \phi(s,a) + \epsilon\)</span>, with <span class="arithmatex">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span></li>
</ul>
<h3 id="benefits-of-generalization">Benefits of Generalization<a class="headerlink" href="#benefits-of-generalization" title="Permanent link">¶</a></h3>
<ul>
<li>Allows learning across states/actions</li>
<li>Enables sample-efficient exploration in large state/action spaces</li>
</ul>
<h2 id="strategic-exploration-in-deep-rl">Strategic Exploration in Deep RL<a class="headerlink" href="#strategic-exploration-in-deep-rl" title="Permanent link">¶</a></h2>
<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>
<h3 id="optimistic-q-learning-with-function-approximation">Optimistic Q-Learning with Function Approximation<a class="headerlink" href="#optimistic-q-learning-with-function-approximation" title="Permanent link">¶</a></h3>
<p>Modified Q-learning update:</p>
<div class="arithmatex">\[
\Delta w = \alpha \left( r + r_{\text{bonus}}(s,a) + \gamma \max_{a'} Q(s', a'; w) - Q(s,a;w) \right) \nabla_w Q(s,a;w)
\]</div>
<p>Bonus <span class="arithmatex">\(r_{\text{bonus}}\)</span> reflects novelty or epistemic uncertainty.</p>
<h3 id="count-based-and-density-based-exploration">Count-Based and Density-Based Exploration<a class="headerlink" href="#count-based-and-density-based-exploration" title="Permanent link">¶</a></h3>
<ul>
<li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li>
<li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li>
<li>Tang et al. (2017) use hashing-based counts.</li>
</ul>
<h2 id="thompson-sampling-for-deep-rl">Thompson Sampling for Deep RL<a class="headerlink" href="#thompson-sampling-for-deep-rl" title="Permanent link">¶</a></h2>
<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>
<h3 id="bootstrapped-dqn">Bootstrapped DQN<a class="headerlink" href="#bootstrapped-dqn" title="Permanent link">¶</a></h3>
<ul>
<li>Train multiple Q-networks on bootstrapped datasets.</li>
<li>Select one head randomly at each episode for exploration.</li>
</ul>
<h3 id="bayesian-deep-q-networks">Bayesian Deep Q-Networks<a class="headerlink" href="#bayesian-deep-q-networks" title="Permanent link">¶</a></h3>
<ul>
<li>Bayesian linear regression on final layer</li>
<li>Posterior used to sample Q-values, enabling optimism</li>
<li>Outperforms naive bootstrapped DQNs in some settings</li>
</ul>
<h2 id="mental-map">Mental Map<a class="headerlink" href="#mental-map" title="Permanent link">¶</a></h2>
<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization
  Goal: Learn near-optimal policies in MDPs with limited data
    Extend bandit exploration ideas to sequential decision making
                            │
                            ▼
             Why MDPs Are Harder Than Bandits
</code></pre>
<p>┌─────────────────────────────────────────────────────────────┐
 │ MDPs involve sequential decisions with transitions           │
 │ Agent must explore over states and transitions              │
 │ Exploration affects future knowledge &amp; rewards              │
 │ Sample inefficiency is a major practical bottleneck         │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                   Evaluation Frameworks for RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Regret: cumulative gap vs optimal policy over time          │
 │ PAC (Probably Approximately Correct):                       │
 │   Guarantees ε-optimality with high probability             │
 │ Bayesian Regret: expected regret under prior over MDPs      │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              PAC Learning in MDPs: Formal Guarantee
 ┌─────────────────────────────────────────────────────────────┐
 │ Algorithm is PAC if all but N steps are ε-optimal           │
 │ N = poly(|S|, |A|, 1/(1-γ), 1/ε, 1/δ)                        │
 │ Ensures high-probability performance bounds                 │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
               Optimism: MBIE-EB Algorithm (Model-Based)
 ┌─────────────────────────────────────────────────────────────┐
 │ Estimate reward + transitions from data                     │
 │ Add bonus to Q-values: encourages actions with high uncertainty │
 │ Optimistic model induces exploration                        │
 │ Dynamic programming over Q̂ + bonus → exploration policy     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
           Algorithmic Principle: Optimism Under Uncertainty
 ┌─────────────────────────────────────────────────────────────┐
 │ Add uncertainty-driven bonus to reward or Q-value           │
 │ Drives exploration to unknown regions                       │
 │ Simple but effective in tabular MDPs                        │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian RL and Posterior Sampling
 ┌─────────────────────────────────────────────────────────────┐
 │ Maintain belief (posterior) over MDP model (P, R)           │
 │ Sample MDP from posterior → plan optimally in sampled MDP   │
 │ Leads to probability matching via Thompson Sampling         │
 │ Posterior concentrates with data → convergence to optimal   │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
          Algorithm: Thompson Sampling in Model-Based RL
 ┌─────────────────────────────────────────────────────────────┐
 │ Sample dynamics + rewards from posterior                    │
 │ Solve sampled MDP for optimal Q<em>                            │
 │ Act according to Q</em> in sample MDP                           │
 │ Update posterior using Bayes rule after each step           │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
             Exploration via Posterior Variance (Bayes)
 ┌─────────────────────────────────────────────────────────────┐
 │ Thompson Sampling ≈ Probability Matching                    │
 │ Probabilistically favors optimal but uncertain policies     │
 │ Elegant &amp; adaptive exploration                             │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
              Generalization via Contextual Bandits
 ┌─────────────────────────────────────────────────────────────┐
 │ Rewards depend on both context and action                   │
 │ Learn generalizable function: Q(s,a) or π(a|s)              │
 │ Enables learning across states / actions                    │
 │ Use linear models or embeddings: φ(s,a)                     │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
         Exploration + Generalization in Deep RL Settings
 ┌─────────────────────────────────────────────────────────────┐
 │ Optimistic Q-learning: add r_bonus(s,a) in TD target        │
 │ r_bonus from novelty, density models, or uncertainty        │
 │ Count-based, hashing, or learned density bonuses            │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Bayesian Deep RL: Posterior Approximation
 ┌─────────────────────────────────────────────────────────────┐
 │ Bootstrapped DQN: ensemble of Q-networks for exploration    │
 │ Bayesian DQN: sample from approximate Q-posteriors          │
 │ Enables implicit Thompson-like behavior                     │
 │ Scales to high-dimensional state/action spaces              │
 └─────────────────────────────────────────────────────────────┘
                                │
                                ▼
                         Chapter Summary
 ┌─────────────────────────────────────────────────────────────┐
 │ Strategic exploration = key to fast learning in MDPs        │
 │ Optimism (MBIE-EB) and Bayesian methods (Thompson)          │
 │ PAC and Bayesian regret are key evaluation tools            │
 │ Generalization (via features or deep nets) enables scaling  │
 │ Thompson Sampling and bootstrapped approximations bridge gap│
 │ Between tabular and high-dimensional RL                     │
 └─────────────────────────────────────────────────────────────┘
````</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../11_fast_rl/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 11. Data-Efficient Reinforcement Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                11. Data-Efficient Reinforcement Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../13_montecarlo/" class="md-footer__link md-footer__link--next" aria-label="Next: 13. Monte Carlo Tree Search and Planning">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                13. Monte Carlo Tree Search and Planning
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>