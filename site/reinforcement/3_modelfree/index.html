<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Reinforcement Learning.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/reinforcement_learning/reinforcement/3_modelfree/">
      
      
        <link rel="prev" href="../2_mdp/">
      
      
        <link rel="next" href="../4_model_free_control/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>3. Model-Free Prediction - Reinforcement Learning Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-header__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. Model-Free Prediction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Reinforcement Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Reinforcement Learning Lecture Notes" class="md-nav__button md-logo" aria-label="Reinforcement Learning Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Reinforcement Learning Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Introduction to Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_mdp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. MDPs &amp; Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    3. Model-Free Prediction
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-policy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo Policy Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#temporal-difference-td-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Temporal Difference (TD) Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Example Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-first-visit-monte-carlo-mc" class="md-nav__link">
    <span class="md-ellipsis">
      1. First-Visit Monte Carlo (MC)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-every-visit-monte-carlo-mc" class="md-nav__link">
    <span class="md-ellipsis">
      2. Every-Visit Monte Carlo (MC)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-temporal-difference-td0" class="md-nav__link">
    <span class="md-ellipsis">
      3. Temporal Difference (TD(0))
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparison-of-results" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-free-prediction-mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Model Free Prediction Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../4_model_free_control/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Model-Free Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_policy_gradient/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Policy Gradient Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_pg2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Policy Gradient Variance Reduction and Actor-Critic
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_gae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Advances in Policy Optimization – GAE, TRPO, and PPO
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_imitation_learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Imitation Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_offline_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Offline Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_fast_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Data-Efficient Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_fast_mdps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Fast Reinforcement Learning in MDPs and Generalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_montecarlo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Monte Carlo Tree Search and Planning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_final/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Summary and Overview
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-policy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo Policy Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#temporal-difference-td-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Temporal Difference (TD) Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Example Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-first-visit-monte-carlo-mc" class="md-nav__link">
    <span class="md-ellipsis">
      1. First-Visit Monte Carlo (MC)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-every-visit-monte-carlo-mc" class="md-nav__link">
    <span class="md-ellipsis">
      2. Every-Visit Monte Carlo (MC)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-temporal-difference-td0" class="md-nav__link">
    <span class="md-ellipsis">
      3. Temporal Difference (TD(0))
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparison-of-results" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison of Results
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-free-prediction-mental-map" class="md-nav__link">
    <span class="md-ellipsis">
      Model Free Prediction Mental Map
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/reinforcement/3_modelfree.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/reinforcement/3_modelfree.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<h1 id="chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy">Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy<a class="headerlink" href="#chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy" title="Permanent link">¶</a></h1>
<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function <span class="arithmatex">\(V^\pi(s)\)</span> or <span class="arithmatex">\(Q^\pi(s,a)\)</span> for a given policy <span class="arithmatex">\(\pi\)</span> using data of the form:</p>
<div class="arithmatex">\[
s_0, a_0, r_1, s_1, a_1, r_2, s_2, \dots
\]</div>
<p>The true value of a state under policy <span class="arithmatex">\(\pi\)</span> is still defined by the expected return:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]
\]</div>
<p>but the agent must approximate this expectation using sampled experience.</p>
<p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p>
<ol>
<li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li>
<li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li>
</ol>
<h2 id="monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation<a class="headerlink" href="#monte-carlo-policy-evaluation" title="Permanent link">¶</a></h2>
<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function <span class="arithmatex">\(V^\pi(s)\)</span> is the expected return, we can approximate it by simply averaging the observed returns (<span class="arithmatex">\(G_t\)</span>) from many episodes that start at state <span class="arithmatex">\(s\)</span>.</p>
<div class="arithmatex">\[
V^\pi(s) \approx \text{Average of observed returns } G_t \text{ starting from } s
\]</div>
<h3 id="key-properties-of-mc">Key Properties of MC<a class="headerlink" href="#key-properties-of-mc" title="Permanent link">¶</a></h3>
<ol>
<li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (<span class="arithmatex">\(s_T\)</span>) to calculate the full return <span class="arithmatex">\(G_t\)</span>.</li>
<li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li>
</ol>
<p>We can maintain the value estimates <span class="arithmatex">\(V(s)\)</span> using counts and sums, or through incremental updates.</p>
<h4 id="a-first-visit-vs-every-visit-mc">A. First-Visit vs. Every-Visit MC<a class="headerlink" href="#a-first-visit-vs-every-visit-mc" title="Permanent link">¶</a></h4>
<p>When computing the return <span class="arithmatex">\(G_t\)</span> for a state <span class="arithmatex">\(s\)</span> in a single trajectory, a state might be visited multiple times.</p>
<ul>
<li>First-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> only the first time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: First-Visit MC is an unbiased estimator of <span class="arithmatex">\(V^\pi(s)\)</span>. It is also consistent (converges to the true value as data <span class="arithmatex">\(\rightarrow \infty\)</span>) by the Law of Large Numbers.</li>
</ul>
</li>
<li>Every-Visit MC: The return <span class="arithmatex">\(G_t\)</span> is used to update <span class="arithmatex">\(V(s)\)</span> every time state <span class="arithmatex">\(s\)</span> is visited in an episode.<ul>
<li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li>
</ul>
</li>
</ul>
<h4 id="b-incremental-monte-carlo">B. Incremental Monte Carlo<a class="headerlink" href="#b-incremental-monte-carlo" title="Permanent link">¶</a></h4>
<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p>
<div class="arithmatex">\[
V(s) \leftarrow V(s) + \alpha \left[ G_t - V(s) \right]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(G_t\)</span>: The actual observed return (our target).</li>
<li><span class="arithmatex">\(V(s)\)</span>: Our current estimate (our old value).</li>
<li><span class="arithmatex">\(\alpha\)</span>: The learning rate (<span class="arithmatex">\(\alpha \in (0, 1]\)</span>), which can be fixed or decayed.</li>
</ul>
<p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (<span class="arithmatex">\(V^\pi\)</span>), the learning rate <span class="arithmatex">\(\alpha_t\)</span> (which may be <span class="arithmatex">\(1/N(s)\)</span> or a fixed constant) must satisfy the following conditions:</p>
<ol>
<li>The sum of all learning rates for state <span class="arithmatex">\(s\)</span> must diverge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s) = \infty\)</span></li>
<li>The sum of the squared learning rates must converge: <span class="arithmatex">\(\sum_{t=1}^{\infty} \alpha_t(s)^2 &lt; \infty\)</span></li>
</ol>
<h2 id="temporal-difference-td-learning">Temporal Difference (TD) Learning<a class="headerlink" href="#temporal-difference-td-learning" title="Permanent link">¶</a></h2>
<p>While MC uses the full return <span class="arithmatex">\(G_t\)</span>, TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>
<h3 id="bootstrapping-the-core-idea">Bootstrapping: The Core Idea<a class="headerlink" href="#bootstrapping-the-core-idea" title="Permanent link">¶</a></h3>
<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the <em>next</em> state, <span class="arithmatex">\(V(s_{t+1})\)</span>, to update the value of the <em>current</em> state, <span class="arithmatex">\(V(s_t)\)</span>. The standard TD algorithm is TD(0) (or one-step TD).</p>
<h3 id="the-td0-update-rule">The TD(0) Update Rule<a class="headerlink" href="#the-td0-update-rule" title="Permanent link">¶</a></h3>
<p>The TD(0) update replaces the full return <span class="arithmatex">\(G_t\)</span> with the TD Target (<span class="arithmatex">\(r_t + \gamma V(s_{t+1})\)</span>):</p>
<div class="arithmatex">\[
V(s_t) \leftarrow V(s_t) + \alpha \left[ \underbrace{r_{t+1} + \gamma V(s_{t+1})}_{\text{TD Target}} - V(s_t) \right]
\]</div>
<p>The term inside the brackets is the TD Error (<span class="arithmatex">\(\delta_t\)</span>):
<script type="math/tex; mode=display">
\delta_t = (r_{t+1} + \gamma V(s_{t+1})) - V(s_t)
</script>
This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>
<h3 id="td-vs-monte-carlo">TD vs. Monte Carlo<a class="headerlink" href="#td-vs-monte-carlo" title="Permanent link">¶</a></h3>
<p>The distinction between TD and MC centers on what is used as the target value:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Monte Carlo (MC)</th>
<th style="text-align: left;">Temporal Difference (TD)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: left;"><span class="arithmatex">\(G_t\)</span> (Full observed return to episode end)</td>
<td style="text-align: left;"><span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span> (One-step return + estimated future value)</td>
</tr>
<tr>
<td style="text-align: left;">Bootstrapping</td>
<td style="text-align: left;">No (waits until episode end)</td>
<td style="text-align: left;">Yes (uses <span class="arithmatex">\(V(s_{t+1})\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;">Bias</td>
<td style="text-align: left;">Unbiased (First-Visit MC)</td>
<td style="text-align: left;">Biased (because <span class="arithmatex">\(V(s_{t+1})\)</span> is an estimate)</td>
</tr>
<tr>
<td style="text-align: left;">Variance</td>
<td style="text-align: left;">High Variance (Return <span class="arithmatex">\(G_t\)</span> is a sum of many random steps)</td>
<td style="text-align: left;">Low Variance (TD target depends on only one random reward/next state)</td>
</tr>
<tr>
<td style="text-align: left;">Convergence</td>
<td style="text-align: left;">Consistent (converges to true <span class="arithmatex">\(V^\pi\)</span>)</td>
<td style="text-align: left;">TD(0) converges to true <span class="arithmatex">\(V^\pi\)</span> in the tabular case</td>
</tr>
</tbody>
</table>
<p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>
<blockquote>
<h2 id="example-setup">Example Setup<a class="headerlink" href="#example-setup" title="Permanent link">¶</a></h2>
<h3 id="parameters">Parameters<a class="headerlink" href="#parameters" title="Permanent link">¶</a></h3>
<ul>
<li>States (<span class="arithmatex">\(S\)</span>): <span class="arithmatex">\(s_A, s_B, s_C\)</span></li>
<li>Discount Factor (<span class="arithmatex">\(\gamma\)</span>): <span class="arithmatex">\(0.9\)</span></li>
<li>Learning Rate (<span class="arithmatex">\(\alpha\)</span>): <span class="arithmatex">\(0.5\)</span> (Used for TD updates)</li>
<li>Initial Value Estimates (<span class="arithmatex">\(V_0\)</span>): <span class="arithmatex">\(V(s_A)=0, V(s_B)=0, V(s_C)=0\)</span></li>
</ul>
<h3 id="episodes-and-returns">Episodes and Returns<a class="headerlink" href="#episodes-and-returns" title="Permanent link">¶</a></h3>
<p>The full return (<span class="arithmatex">\(G_t\)</span>) is calculated for every visit in every episode:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Episode (E)</th>
<th style="text-align: left;">Trajectory (State <span class="arithmatex">\(\xrightarrow{r}\)</span> Next State)</th>
<th style="text-align: left;">Visit Time (<span class="arithmatex">\(t\)</span>)</th>
<th style="text-align: left;">State (<span class="arithmatex">\(s_t\)</span>)</th>
<th style="text-align: left;">Full Return (<span class="arithmatex">\(G_t\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B \xrightarrow{r=0} s_C \xrightarrow{r=5} s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.508}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.12}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.8}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{2.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{5.2}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{8.0}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">E3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C \xrightarrow{r=-5} s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (1st)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{6.31}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{-4.1}\)</span></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span> (2nd)</td>
<td style="text-align: left;"><span class="arithmatex">\(\mathbf{1.0}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="1-first-visit-monte-carlo-mc">1. First-Visit Monte Carlo (MC)<a class="headerlink" href="#1-first-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: Only the first return for a state in any given episode is used.</p>
<h3 id="a-data-selection-and-counts-ns">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#a-data-selection-and-counts-ns" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508\)</span> (E1), <span class="arithmatex">\(5.2\)</span> (E2)</td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12\)</span> (E1), <span class="arithmatex">\(6.31\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(12.43\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8\)</span> (E1), <span class="arithmatex">\(8.0\)</span> (E2), <span class="arithmatex">\(-4.1\)</span> (E3)</td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="b-final-estimates-vs-sum-g_t-ns">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#b-final-estimates-vs-sum-g_t-ns" title="Permanent link">¶</a></h3>
<p>
<script type="math/tex">
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{12.43}{2} = \mathbf{6.215} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
</script>
</p>
<h2 id="2-every-visit-monte-carlo-mc">2. Every-Visit Monte Carlo (MC)<a class="headerlink" href="#2-every-visit-monte-carlo-mc" title="Permanent link">¶</a></h2>
<p>Rule: The return from every time a state is encountered in any episode is used.</p>
<h3 id="a-data-selection-and-counts-ns_1">A. Data Selection and Counts (<span class="arithmatex">\(N(s)\)</span>)<a class="headerlink" href="#a-data-selection-and-counts-ns_1" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">State (<span class="arithmatex">\(s\)</span>)</th>
<th style="text-align: left;">Returns Used (<span class="arithmatex">\(G_t\)</span>)</th>
<th style="text-align: left;">Total Sum (<span class="arithmatex">\(\sum G_t\)</span>)</th>
<th style="text-align: left;">Count (<span class="arithmatex">\(N(s)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.508, 5.2\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(11.708\)</span></td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.12, 2.0, 6.31, 1.0\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(15.43\)</span></td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(6.8, 8.0, -4.1\)</span></td>
<td style="text-align: left;"><span class="arithmatex">\(10.7\)</span></td>
<td style="text-align: left;">3</td>
</tr>
</tbody>
</table>
<h3 id="b-final-estimates-vs-sum-g_t-ns_1">B. Final Estimates (<span class="arithmatex">\(V(s) = \sum G_t / N(s)\)</span>)<a class="headerlink" href="#b-final-estimates-vs-sum-g_t-ns_1" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
V(s_A) = \frac{11.708}{2} = \mathbf{5.854} \\
V(s_B) = \frac{15.43}{4} = \mathbf{3.858} \\
V(s_C) = \frac{10.7}{3} = \mathbf{3.567}
\]</div>
<h2 id="3-temporal-difference-td0">3. Temporal Difference (TD(0))<a class="headerlink" href="#3-temporal-difference-td0" title="Permanent link">¶</a></h2>
<p>Rule: The value is updated after every step using the TD Target (<span class="arithmatex">\(r_{t+1} + \gamma V(s_{t+1})\)</span>) and the learning rate <span class="arithmatex">\(\alpha\)</span>. The updated <span class="arithmatex">\(V(s)\)</span> estimates are carried over to the next step and episode.</p>
<h3 id="a-step-by-step-td-calculation-summary">A. Step-by-Step TD Calculation Summary<a class="headerlink" href="#a-step-by-step-td-calculation-summary" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Step</th>
<th style="text-align: left;">Transition</th>
<th style="text-align: left;">Old <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;">New <span class="arithmatex">\(V(s_t)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_A)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_B)\)</span></th>
<th style="text-align: left;"><span class="arithmatex">\(V(s_C)\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E1-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=1} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=0} s_C\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">E1-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=5} s_B\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E1-4</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=2} \text{T}\)</span></td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_A \xrightarrow{r=-2} s_C\)</span></td>
<td style="text-align: left;">0.500</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">2.500</td>
</tr>
<tr>
<td style="text-align: left;">E2-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=8} \text{T}\)</span></td>
<td style="text-align: left;">2.500</td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-1</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=10} s_C\)</span></td>
<td style="text-align: left;">1.000</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">5.250</td>
</tr>
<tr>
<td style="text-align: left;">E3-2</td>
<td style="text-align: left;"><span class="arithmatex">\(s_C \xrightarrow{r=-5} s_B\)</span></td>
<td style="text-align: left;">5.250</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">3.663</td>
</tr>
<tr>
<td style="text-align: left;">E3-3</td>
<td style="text-align: left;"><span class="arithmatex">\(s_B \xrightarrow{r=1} \text{T}\)</span></td>
<td style="text-align: left;">7.863</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">3.663</td>
</tr>
</tbody>
</table>
<h3 id="b-final-estimates-v_td0">B. Final Estimates (<span class="arithmatex">\(V_{TD(0)}\)</span>)<a class="headerlink" href="#b-final-estimates-v_td0" title="Permanent link">¶</a></h3>
<blockquote>
<div class="arithmatex">\[
V(s_A) = \mathbf{0.375} \\
V(s_B) = \mathbf{4.431} \\
V(s_C) = \mathbf{3.663}
\]</div>
</blockquote>
<h2 id="comparison-of-results">Comparison of Results<a class="headerlink" href="#comparison-of-results" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;">State</th>
<th style="text-align: left;">First-Visit MC</th>
<th style="text-align: left;">Every-Visit MC</th>
<th style="text-align: left;">TD(0) (<span class="arithmatex">\(\alpha=0.5, \gamma=0.9\)</span>)</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_A\)</span></td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">5.854</td>
<td style="text-align: left;">0.375</td>
<td style="text-align: left;">TD heavily penalized <span class="arithmatex">\(s_A\)</span> in E2 (Target 0.25), while MC averaged the full observed high returns.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_B\)</span></td>
<td style="text-align: left;">6.215</td>
<td style="text-align: left;">3.858</td>
<td style="text-align: left;">4.431</td>
<td style="text-align: left;">TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping.</td>
</tr>
<tr>
<td style="text-align: left;"><span class="arithmatex">\(s_C\)</span></td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.567</td>
<td style="text-align: left;">3.663</td>
<td style="text-align: left;">All methods are close for <span class="arithmatex">\(s_C\)</span>.</td>
</tr>
</tbody>
</table>
<p>This comparison illustrates the bias-variance trade-off:
* MC uses the sample return (<span class="arithmatex">\(G_t\)</span>), which has high variance but is an unbiased target (First-Visit).
* TD uses a bootstrapped estimate (<span class="arithmatex">\(r + \gamma V(s')\)</span>), which has lower variance but introduces bias by relying on an estimated successor value.</p>
</blockquote>
<h2 id="model-free-prediction-mental-map">Model Free Prediction Mental Map<a class="headerlink" href="#model-free-prediction-mental-map" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code>            Model-Free Prediction
     (Policy Evaluation without Model P or R)
                        │
                        ▼
                Goal: Estimate
       ┌───────────────────────────────────┐
       │ State Value: Vπ(s)                │
       │ Action Value: Qπ(s,a)             │
       └───────────────────────────────────┘
                        │
              Using Sampled Experience
        (s₀,a₀,r₁,s₁,a₁,r₂,... from π)
                        │
                        ▼
            Two Families of Methods
    ┌───────────────────────────────┬───────────────────────────────┐
    │ Monte Carlo (MC)              │ Temporal Difference (TD)      │
    │ "Learn from full episodes"    │ "Learn step-by-step"          │
    └───────────────────────────────┴───────────────────────────────┘
                        │                           │
                        │                           │
                        ▼                           ▼
             Monte Carlo (MC)              Temporal Difference (TD)
      ┌─────────────────────────┐       ┌────────────────────────────┐
      │ Needs full episodes     │       │Works on incomplete episodes│
      │ No bootstrapping        │       │Uses bootstrapping          │
      │ High variance           │       │Low variance                │
      │ Unbiased (first visit)  │       │Biased                      │
      └─────────────────────────┘       └────────────────────────────┘
                        │                           │
                        │                           │
        ┌───────────────┴───────────────┐           │
        │                               │           │
        ▼                               ▼           ▼
 First-Visit MC                  Every-Visit MC     TD(0) Update Rule
 (One update per episode          (Multiple updates │ V(s) ← V(s) +
  per state)                      per episode)      │ α[ r + γV(s') − V(s) ]
                        │                           │
                        │                           │
                        └──────────┬────────────────┘
                                   │
                                   ▼
                        Comparison (Bias–Variance)
               ┌─────────────────────────────────────────┐
               │ MC: Unbiased, High variance             │
               │ TD: Biased, Lower variance              │
               │ MC: Not bootstrapping                   │
               │ TD: Bootstraps using V(s’)              │
               │ MC: Episodic only                       │
               │ TD: Works for continuing tasks          │
               └─────────────────────────────────────────┘
                                   │
                                   ▼
                      Outcome: Learned Value Function
               ┌───────────────────────────────────────┐
               │ Vπ(s) or Qπ(s,a) from real experience │
               │ (No model of environment required)    │
               └───────────────────────────────────────┘
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../2_mdp/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 2. MDPs &amp; Dynamic Programming">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                2. MDPs &amp; Dynamic Programming
              </div>
            </div>
          </a>
        
        
          
          <a href="../4_model_free_control/" class="md-footer__link md-footer__link--next" aria-label="Next: 4. Model-Free Control">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                4. Model-Free Control
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>