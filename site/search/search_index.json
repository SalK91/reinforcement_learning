{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#lectures-on-reinforcement-learning","title":"Lectures on Reinforcement Learning","text":"<p>Welcome to Lectures on Reinforcement Learning (RL), a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and develop modern reinforcement learning methods.</p> <p>These notes are inspired by and draw heavily on material from: - Stanford CS234: Reinforcement Learning (Spring 2024) - Stanford CS224R: Deep Reinforcement Learning (Spring 2025) </p> <p>The goal is not to reproduce these courses, but to synthesize their core ideas into a coherent, optimization- and mathematics-first perspective suitable for practitioners and researchers.</p>"},{"location":"reinforcement/10_offline_rl/","title":"10. Offline Reinforcement Learning","text":""},{"location":"reinforcement/10_offline_rl/#chapter-10-batch-offline-rl-policy-evaluation-optimization","title":"Chapter 10: Batch / Offline RL Policy Evaluation &amp; Optimization","text":"<p>Learning from the Past</p> <ul> <li>Learning from Past Human Demonstrations: Imitation Learning</li> <li>Learning from Past Human Preferences: RLHF and DPO</li> <li>Learning from Past Decisions and Actions: Offline RL</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-reinforcement-learning-a-different-approach","title":"Offline Reinforcement Learning: A Different Approach","text":"<p>Offline Reinforcement Learning allows learning from a fixed dataset of past experiences rather than continuous exploration. The central idea of offline RL is that we can use data generated by any policy (including suboptimal ones) to evaluate and improve a new policy without further interaction with the environment. This can be particularly useful in real-world scenarios where it is expensive or unsafe to explore.</p> <p>In settings where exploration is costly, dangerous, or impractical (e.g., healthcare, autonomous driving), we rely on pre-existing datasets to learn the best possible policy. Offline RL is essential in such cases as it enables learning from historical data while avoiding risky interactions with the environment. However, this task is more challenging due to the limited data distribution, which may not cover all relevant state-action pairs for effective learning.</p> <p>Why Can\u2019t We Just Use Q-Learning?</p> <ul> <li>Q-learning is an off policy RL algorithm: Can be used with data different than the state--action pairs would visit under the optimal Q state action values</li> <li>But deadly triad of bootstrapping, function approximation and off policy, and can fail</li> </ul>"},{"location":"reinforcement/10_offline_rl/#batch-policy-evaluation-estimating-the-performance-of-a-policy","title":"Batch Policy Evaluation: Estimating the Performance of a Policy","text":"<p>Offline RL starts with batch policy evaluation, where we aim to estimate the performance of a given policy without interacting with the environment.</p> <ol> <li> <p>Using Models: A model-based approach uses a learned model of the environment to simulate what would have happened if the policy had been followed. The model learns both the reward and transition dynamics from the data.</p> <p>Specifically, it learns two main components from data: a reward function \\(\\hat{r}(s,a)\\) and transition dynamics \\(\\hat{P}(s' \\mid s,a)\\). These are learned via supervised learning on the offline dataset \\(D\\) of transitions collected by some behavior policy \\(\\pi_b\\). For example, \\(\\hat{r}(s,a)\\) can be trained by regression to predict the observed reward given state \\(s\\) and action \\(a\\), and \\(\\hat{P}(s' \\mid s,a)\\) can be fit to predict the next-state \\(s'\\) (or a distribution over next states) given the current state and action. In practice, one might maximize the likelihood of observed transitions in \\(D\\) under the model or minimize prediction error. In essence, the agent treats the batch data as supervised examples to \u201clearn the environment\u2019s rules\". Once learned, this model \\(\\hat{\\mathcal{M}} = (\\hat{P}, \\hat{r})\\) serves as a proxy for the real environment, which we can use for evaluating any policy \\(\\pi\\) without further real experience.</p> <p>It\u2019s important to note the limitations of the learned model at this stage. The offline dataset may cover only a subset of the state-action space (the ones the behavior policy visited). The learned dynamics \\(\\hat{P}\\) will be reliable only in regions covered by \\(D\\); if \\(\\pi\\) later tries unfamiliar states or actions, the model will be forced to extrapolate, potentially generating highly biased predictions (the dreaded extrapolation error or model hallucination).</p> </li> <li> <p>Model-Free Methods: In a model-free approach, we evaluate the policy directly based on the observed dataset without using a learned model of the environment. Fitted Q Evaluation (FQE): Uses historical data to estimate the value function of a policy by fitting a Q-function to the data and iterating until convergence.</p> </li> <li> <p>Importance Sampling:     This is a trajectory re-weighting approach that treats the off-policy evaluation as a statistical estimation problem. By weighting each reward in the dataset by the ratio of the probabilities of that trajectory under the target policy vs. the behavior policy (the one that generated the data) , IS provides an unbiased estimator of the target policy\u2019s value \u2013 assuming coverage (i.e. the target policy doesn\u2019t go where behavior policy never went). The big drawback is variance: if the policy difference is significant or the horizon is long, importance weights can explode, making estimates very noisy. In practice, vanilla importance sampling is often too high-variance to be useful for long-horizon RL. There are variants like weighted importance sampling, per-decision IS, and doubly robust estimators to mitigate variance at the cost of some bias.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithmic-outline-offline-policy-evaluation-via-model","title":"Algorithmic Outline - Offline Policy Evaluation via Model:","text":"<ol> <li> <p>Input: offline dataset \\(D\\) of transitions (from behavior \\(\\pi_b\\)), a policy \\(\\pi\\) to evaluate, discount \\(\\gamma\\).</p> </li> <li> <p>Model Learning: Fit \\(\\hat{P}(s'|s,a)\\) and \\(\\hat{r}(s,a)\\) using \\(D\\) (e.g. maximum likelihood estimation for dynamics, regression for rewards).</p> </li> <li> <p>Policy Evaluation: Initialize \\(V(s)=0\\) for all states (or some initial guess).</p> </li> <li> <p>Loop (Bellman backups using \\(\\hat{P},\\hat{r}\\)): For each state \\(s\\) in the state space (or a representative set of states):</p> </li> <li> <p>Compute \\(\\hat{R}^\\pi(s) = \\sum_a \\pi(a|s)\\hat{r}(s,a)\\).</p> </li> <li> <p>Compute \\(V_{\\text{new}}(s) = \\hat{R}^\\pi(s) + \\gamma \\sum_{s'} \\hat{P}^\\pi(s'\\mid s),V(s')\\).</p> </li> <li> <p>Update \\(V \\leftarrow V_{\\text{new}}\\) and repeat until convergence (the changes in \\(V\\) are below a threshold).</p> </li> <li> <p>Output: \\(V(s)\\) for states of interest (e.g. the estimated value of \\(\\pi\\) under the initial state distribution \\(S_0\\) can be obtained by \\(\\mathbb{E}_{s_0\\sim S_0}[V(s_0)]\\)).</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#algorithm-3-fitted-q-evaluation-fqe-pi-c","title":"Algorithm 3 Fitted Q Evaluation: FQE \\((\\pi, c)\\)","text":"<p>Input: Dataset \\(\\mathcal{D} = \\{(x_i, a_i, x'_i, c_i)\\}_{i=1}^n \\sim \\pi_D\\). Data set here is a bunch of just different tuples of state, action, reward and next state. FQE aims to evaluate a fixed policy \\(\\pi\\) by learning its Q-function from this offline dataset. The Q-function represents the expected cumulative cost starting from state \\(s_i\\), taking action \\(a_i\\), and then following policy \\(\\pi\\) thereafter.At each iteration, we construct a Bellman target:</p> \\[\\tilde{Q}^\\pi(s_i, a_i) = c_i + \\gamma V_\\theta^\\pi(s_{i+1}) \\] <p>where</p> \\[ V_\\theta^\\pi(s_{i+1}) = Q_\\theta^\\pi(s_{i+1}, \\pi(s_{i+1})). \\] <p>The Q-function is parameterized by \\(\\theta\\) (e.g., a neural network), and is learned by solving a supervised regression problem:</p> \\[\\arg\\min_\\theta \\sum_i \\Big( Q_\\theta^\\pi(s_i, a_i) - \\tilde{Q}^\\pi(s_i, a_i) \\Big)^2\\] <p>This procedure is repeated iteratively, yielding an increasingly accurate estimate of the value of policy \\(\\pi\\) under the data distribution induced by \\(\\pi_D\\).</p> <p>Function class \\(F\\) (Let's assume we use a DNN for F). Policy </p> <p>\\(\\pi\\) to be evaluated. 1: Initialize \\(Q_0 \\in F\\) randomly 2: for \\(k = 1, 2, \\dots, K\\) do 3: \\(\\quad\\) Compute target \\(y_i = c_i + \\gamma Q_{k-1}(x'_i, \\pi(x'_i)) \\quad \\forall i\\) 4: \\(\\quad\\) Build training set \\(\\tilde{\\mathcal{D}}_k = \\{(x_i, a_i), y_i\\}_{i=1}^n\\) 5: \\(\\quad\\) Solve a supervised learning problem:  6: end for Output: \\(\\hat{C}^\\pi(x) = Q_K(x, \\pi(x)) \\quad \\forall x\\)</p>"},{"location":"reinforcement/10_offline_rl/#what-is-different-vs-dqn","title":"What is different vs DQN?","text":"<p>DQN learns an optimal policy by interacting with the environment, while FQE evaluates a fixed policy using a fixed offline dataset.</p> Aspect FQE (Fitted Q Evaluation) DQN (Deep Q-Network) Goal Policy evaluation Policy optimization / control Policy Fixed target policy \\(\\pi\\) Implicitly learned via \\(\\max_a Q(s,a)\\) Data Offline, fixed dataset \\(\\mathcal{D}\\) Online, collected during training Bellman target \\(c + \\gamma Q(s', \\pi(s'))\\) \\(r + \\gamma \\max_a Q(s', a)\\) Action at next state From given policy \\(\\pi\\) Greedy over Q-values Exploration None Required (e.g. \\(\\epsilon\\)-greedy) Dataset changes? \u274c No \u2705 Yes Off-policy instability Low High Convergence guarantees Yes (tabular / linear) No (with function approximation)"},{"location":"reinforcement/10_offline_rl/#1-no-maximization-bias-in-fqe","title":"1. No maximization bias in FQE","text":"<ul> <li>DQN suffers from overestimation bias</li> <li>FQE does pure regression, no bootstrapped max</li> </ul>"},{"location":"reinforcement/10_offline_rl/#2-stability","title":"2. Stability","text":"<ul> <li>FQE \u2248 supervised learning  </li> <li>DQN \u2248 bootstrapped + non-stationary targets</li> </ul>"},{"location":"reinforcement/10_offline_rl/#3-offline-vs-online","title":"3. Offline vs Online","text":"<ul> <li>FQE cannot improve the policy  </li> <li>DQN must interact with environment</li> </ul>"},{"location":"reinforcement/10_offline_rl/#offline-policy-learning-optimization","title":"Offline Policy Learning / Optimization","text":"<p>Once we've evaluated the policy using offline methods, the next step is to optimize the policy based on the evaluation.</p> <ol> <li> <p>Optimization with Model-Free Methods: Offline RL with model-free methods like Fitted Q Iteration (FQI) focuses on directly improving the policy by optimizing the action-value function based on past data. This approach may suffer from inefficiency if the data is sparse or does not adequately represent the true state-action space.</p> </li> <li> <p>Dealing with the Distribution Mismatch: One challenge in offline RL is the distribution shift between the data used to learn the model and the policy we are optimizing. Policies derived from the data may end up performing poorly when deployed due to overfitting to the data distribution.</p> </li> <li> <p>Conservative Batch RL: A solution to the distribution mismatch is pessimistic learning. We assume that areas of the state-action space with little data coverage are likely to lead to suboptimal outcomes and penalize actions from those regions during optimization. This helps to avoid overestimation of the policy's performance in underrepresented areas.</p> </li> </ol>"},{"location":"reinforcement/10_offline_rl/#challenges-in-offline-policy-optimization","title":"Challenges in Offline Policy Optimization","text":"<ol> <li> <p>Overlap Requirement: Offline RL methods often assume that there is sufficient overlap between the behavior policy (the one that collected the data) and the target policy (the one we are optimizing). Without this overlap, the model may fail to generalize well.</p> </li> <li> <p>Model Misspecification: If the dynamics of the environment are not well specified or if the model has errors, the optimization may fail. This is particularly problematic in real-world applications where we may not have access to a perfect model.</p> </li> <li> <p>Pessimistic Approaches: Recent methods in Conservative Offline RL advocate for penalizing regions of the state-action space that have insufficient support in the data. This helps prevent overly optimistic policy performance estimates and ensures safer, more reliable policy learning.</p> </li> </ol> <p>Offline RL provides a valuable framework for learning policies from existing data when exploration is not feasible. By using batch policy evaluation techniques like Importance Sampling, Fitted Q Iteration, and Pessimistic Learning, we can develop policies that perform well even with limited or imperfect data. However, challenges such as distribution mismatch and model misspecification need careful handling to avoid overfitting or biased outcomes.</p>"},{"location":"reinforcement/10_offline_rl/#mental-map","title":"Mental Map","text":"<pre><code>                 Offline / Batch Reinforcement Learning\n        Goal: Learn and evaluate policies from fixed historical data\n           when exploration is unsafe, expensive, or impossible\n                                \u2502\n                                \u25bc\n              Why Online RL Is Not Always Feasible\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Exploration can be dangerous (healthcare, driving, robotics)\u2502\n \u2502 Data already exists from past decisions                     \u2502\n \u2502 Real systems cannot reset or freely experiment              \u2502\n \u2502 We must learn without interacting with the environment      \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Offline RL vs Standard RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Standard RL:                                                \u2502\n \u2502  \u2013 Collect data with current policy                         \u2502\n \u2502  \u2013 Explore \u2192 improve \u2192 repeat                               \u2502\n \u2502                                                             \u2502\n \u2502 Offline RL:                                                 \u2502\n \u2502  \u2013 Fixed dataset D from behavior policy \u03c0_b                 \u2502\n \u2502  \u2013 No new interaction allowed                               \u2502\n \u2502  \u2013 Must generalize only from observed data                  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             Why \u201cJust Use Q-Learning\u201d Fails Offline\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Q-learning is off-policy \u2014 but not offline-safe             \u2502\n \u2502 Deadly triad:                                               \u2502\n \u2502   \u2022 Bootstrapping                                           \u2502\n \u2502   \u2022 Function approximation                                  \u2502\n \u2502   \u2022 Off-policy learning                                     \u2502\n \u2502 Leads to divergence &amp; overestimation                        \u2502\n \u2502 Especially severe with distribution mismatch                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Offline RL Decomposed into Two Core Problems\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 1. Policy Evaluation (OPE)    \u2502 2. Policy Optimization      \u2502\n \u2502    \u201cHow good is this policy?\u201d \u2502    \u201cHow can we improve it?\u201d \u2502\n \u2502    Without running it         \u2502    Without new data         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Batch / Offline Policy Evaluation (OPE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Estimate V^\u03c0 or J(\u03c0) using only dataset D                   \u2502\n \u2502 Three major approaches:                                     \u2502\n \u2502  1. Model-based evaluation                                  \u2502\n \u2502  2. Model-free evaluation (FQE)                             \u2502\n \u2502  3. Importance Sampling                                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        1. Model-Based Offline Policy Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn a model from data:                                    \u2502\n \u2502   \u2022 Reward model: r\u0302(s,a)                                    \u2502\n \u2502   \u2022 Transition model: P\u0302(s'|s,a)                             \u2502\n \u2502 Treat batch data as supervised learning                     \u2502\n \u2502 Then simulate policy \u03c0 inside learned model                 \u2502\n \u2502 Use Bellman backups on (P\u0302, r\u0302)                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Model-Based OPE: Key Limitation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Model is only reliable where data exists                    \u2502\n \u2502 Policy visiting unseen states/actions \u2192 extrapolation error \u2502\n \u2502 Model hallucination \u2192 highly biased value estimates         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        2. Model-Free Evaluation: Fitted Q Evaluation (FQE)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Evaluate a *fixed policy* \u03c0                                 \u2502\n \u2502 Learn Q^\u03c0(s,a) from offline data via regression             \u2502\n \u2502 Bellman target:                                             \u2502\n \u2502   y = c + \u03b3 Q(s', \u03c0(s'))                                    \u2502\n \u2502 Pure supervised learning loop                               \u2502\n \u2502 Stable compared to Q-learning / DQN                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n             FQE vs DQN (Key Insight)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 DQN                         \u2502 FQE                         \u2502\n \u2502 Learns optimal policy       \u2502 Evaluates fixed policy      \u2502\n \u2502 Uses max over actions       \u2502 Uses given \u03c0(s')            \u2502\n \u2502 Online data collection      \u2502 Fully offline               \u2502\n \u2502 Overestimation bias         \u2502 No max \u2192 more stable        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        3. Importance Sampling (IS) Evaluation\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat OPE as statistical estimation                         \u2502\n \u2502 Reweight trajectories by \u03c0 / \u03c0_b                            \u2502\n \u2502 Unbiased if coverage holds                                  \u2502\n \u2502 Severe variance for long horizons or policy mismatch        \u2502\n \u2502 Variants:                                                   \u2502\n \u2502   \u2022 Per-decision IS                                         \u2502\n \u2502   \u2022 Weighted IS                                             \u2502\n \u2502   \u2022 Doubly robust estimators                                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n            Offline Policy Optimization\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Goal: improve policy using only dataset D                   \u2502\n \u2502 Model-free: Fitted Q Iteration (FQI)                        \u2502\n \u2502 Model-based: planning inside learned model                  \u2502\n \u2502 Core challenge: distribution shift                          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        The Central Problem: Distribution Mismatch\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learned policy chooses actions unseen in data               \u2502\n \u2502 Q-values extrapolate \u2192 overly optimistic                    \u2502\n \u2502 Performance collapses at deployment                         \u2502\n \u2502 Offline RL \u2260 just off-policy RL                             \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n        Conservative / Pessimistic Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Assume unknown actions are risky                            \u2502\n \u2502 Penalize state-action pairs with low data support           \u2502\n \u2502 Prefer policies close to behavior policy                    \u2502\n \u2502 Examples (conceptually):                                    \u2502\n \u2502   \u2022 Conservative Q-Learning (CQL)                           \u2502\n \u2502   \u2022 Regularization toward \u03c0_b                               \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n              Key Challenges in Offline RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Coverage / overlap requirement                              \u2502\n \u2502 Model misspecification                                      \u2502\n \u2502 Value overestimation                                        \u2502\n \u2502 Bias\u2013variance tradeoffs                                     \u2502\n \u2502 Safety vs optimality                                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n               Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Offline RL learns entirely from past experience             \u2502\n \u2502 Policy evaluation is foundational before optimization       \u2502\n \u2502 Model-based, FQE, and IS provide OPE tools                  \u2502\n \u2502 Main risk: distribution shift &amp; extrapolation               \u2502\n \u2502 Conservative methods trade performance for safety           \u2502\n \u2502 Offline RL is essential for real-world decision systems     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/11_fast_rl/","title":"11. Data-Efficient Reinforcement Learning","text":""},{"location":"reinforcement/11_fast_rl/#chapter-11-data-efficient-reinforcement-learning-bandit-foundations","title":"Chapter 11: Data-Efficient Reinforcement Learning \u2014 Bandit Foundations","text":"<p>In real-world applications of Reinforcement Learning (RL), data is expensive, time-consuming, or risky to collect. This necessitates data-efficient RL: designing agents that learn effectively from limited interaction. Bandits provide a foundational setting to study such principles. In this chapter, we explore multi-armed banditsas the prototypical framework for understanding the exploration-exploitation tradeoff, and examine several algorithmic approaches and regret-based evaluation criteria.</p>"},{"location":"reinforcement/11_fast_rl/#the-multi-armed-bandit-model","title":"The Multi-Armed Bandit Model","text":"<p>A multi-armed bandit is defined as a tuple \\((\\mathcal{A}, \\mathcal{R})\\), where:</p> <ul> <li>\\(\\mathcal{A} = \\{a_1, \\dots, a_m\\}\\) is a known, finite set of actions (arms),</li> <li>\\(R_a(r) = \\mathbb{P}[r \\mid a]\\) is an unknown probability distribution over rewards for each action.</li> <li>there is no \"state\".</li> </ul> <p>At each timestep \\(t\\), the agent:</p> <ol> <li>Chooses an action \\(a_t \\in \\mathcal{A}\\),</li> <li>Receives a stochastic reward \\(r_t \\sim R_{a_t}\\).</li> </ol> <p>Goal: Maximize cumulative reward: </p> <p>This simple model embodies the core RL challenges\u2014particularly exploration vs. exploitation\u2014in an isolated setting.</p>"},{"location":"reinforcement/11_fast_rl/#evaluating-algorithms-regret-framework","title":"Evaluating Algorithms: Regret Framework","text":"<p>Regret: </p> <ul> <li>\\(Q(a) = \\mathbb{E}[r \\mid a]\\) be the expected reward for action \\(a\\),</li> <li>\\(a^* = \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\),</li> <li>Optimal Value \\(V^* = Q(a^*)\\)</li> </ul> <p>Then regret is the opportunity loss for one step:  </p> <p>Total Regret is the total opportunity loss: Total regret over \\(T\\) timesteps</p> <p>  Where:</p> <ul> <li>\\(N_T(a)\\): Number of times arm \\(a\\) is selected by time \\(T\\),</li> <li>\\(\\Delta_a = V^* - Q(a)\\): Suboptimality gap.</li> </ul> <p>Maximize cumulative reward &lt;=&gt; minimize total regret</p>"},{"location":"reinforcement/11_fast_rl/#baseline-approaches-and-their-regret","title":"Baseline Approaches and Their Regret","text":""},{"location":"reinforcement/11_fast_rl/#greedy-algorithm","title":"Greedy Algorithm","text":"\\[ \\hat{Q}_t(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^t r_\\tau \\cdot \\mathbb{1}(a_\\tau = a) \\] \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\hat{Q}_t(a) \\]"},{"location":"reinforcement/11_fast_rl/#key-insight","title":"Key Insight:","text":"<ul> <li>Exploits current estimates.</li> <li>May lock onto suboptimal arms due to early bad luck.</li> <li>Linear regret in expectation.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#example","title":"Example:","text":"<p>If \\(Q(a_1) = 0.95, Q(a_2) = 0.90, Q(a_3) = 0.1\\), and the first sample of \\(a_1\\) yields 0, the greedy agent may ignore it indefinitely.</p>"},{"location":"reinforcement/11_fast_rl/#varepsilon-greedy-algorithm","title":"\\(\\varepsilon\\)-Greedy Algorithm","text":"<p>At each timestep:</p> <ul> <li>With probability \\(1 - \\varepsilon\\): exploit (\\(\\arg\\max \\hat{Q}_t(a)\\)),</li> <li>With probability \\(\\varepsilon\\): explore uniformly at random.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#performance","title":"Performance:","text":"<ul> <li>Guarantees exploration.</li> <li>Linear regret unless \\(\\varepsilon\\) decays over time.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#decaying-varepsilon-greedy","title":"Decaying \\(\\varepsilon\\)-Greedy","text":"<p>Allows \\(\\varepsilon_t \\to 0\\) as \\(t \\to \\infty\\), enabling convergence.</p>"},{"location":"reinforcement/11_fast_rl/#optimism-in-the-face-of-uncertainty","title":"Optimism in the Face of Uncertainty","text":"<p>Prefer actions with uncertain but potentially high value:</p> <p>Why? Two possible outcomes:</p> <ol> <li> <p>Getting a high reward:    If the arm really has a high mean reward.</p> </li> <li> <p>Learning something : If the arm really has a lower mean reward, pulling it will (in expectation) reduce its average reward estimate and the uncertainty over its value.</p> </li> </ol> <p>Algorithm: </p> <ul> <li> <p>Estimate an upper confidence bound \\(U_t(a)\\) for each action value, such that   \\(Q(a) \\le U_t(a)\\) with high probability.</p> </li> <li> <p>This depends on the number of times \\(N_t(a)\\) action \\(a\\) has been selected.</p> </li> <li> <p>Select the action maximizing the Upper Confidence Bound (UCB):</p> </li> </ul> \\[a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ U_t(a) \\right]\\] <p>Hoeffding Bound Justification:  Given i.i.d. bounded rewards \\(X_i \\in [0,1]\\),  </p> <p>Setting the right-hand side equal to \\(\\delta\\) and solving for \\(u\\),  Here, \\(\\delta\\) is the failure probability, and the confidence interval holds with probability at least \\(1 - \\delta\\). This means that, with probability at least \\(1 - \\delta\\),  </p> \\[ a_t = \\arg\\max_{a \\in \\mathcal{A}} \\left[ \\hat{Q}_t(a) + \\text{UCB}_t(a) \\right] \\]"},{"location":"reinforcement/11_fast_rl/#ucb1-algorithm","title":"UCB1 Algorithm","text":"\\[ \\text{UCB}_t(a) = \\hat{Q}_t(a) + \\sqrt{\\frac{2 \\log \\frac{1}{\\delta} }{N_t(a)}} \\] <ul> <li>where \\(\\hat{Q}_t(a)\\) is empirical average</li> <li>\\(N_t(a)\\) is number of samples of \\(a\\) after \\(t\\) timesteps.</li> <li>Provable sublinear regret.</li> <li>Balances estimated value and exploration bonus.</li> </ul> <p>Algorithm: UCB1 (Auer, Cesa-Bianchi, Fischer, 2002)</p> <p>1: Initialize for each arm \\(a \\in \\mathcal{A}\\):  \\(\\quad N(a) \\leftarrow 0,\\;\\; \\hat{Q}(a) \\leftarrow 0\\) 2: Warm start (sample each arm once): 3: for each arm \\(a \\in \\mathcal{A}\\) do 4: \\(\\quad\\) Pull arm \\(a\\), observe reward \\(r \\in [0,1]\\) 5: \\(\\quad N(a) \\leftarrow 1\\) 6: \\(\\quad \\hat{Q}(a) \\leftarrow r\\) 7: end for 8: Set \\(t \\leftarrow |\\mathcal{A}|\\)</p> <p>9: for \\(t = |\\mathcal{A}|+1, |\\mathcal{A}|+2, \\dots\\) do 10: \\(\\quad\\) Compute UCB for each arm: \\(\\quad \\mathrm{UCB}_t(a) = \\hat{Q}(a) + \\sqrt{\\frac{2\\log t}{N(a)}}\\)</p> <p>11: \\(\\quad\\) Select action:\\(\\quad a_t \\leftarrow \\arg\\max_{a \\in \\mathcal{A}} \\mathrm{UCB}_t(a)\\)</p> <p>12: \\(\\quad\\) Pull arm \\(a_t\\), observe reward \\(r_t\\)</p> <p>13: \\(\\quad\\) Update count: \\(\\quad N(a_t) \\leftarrow N(a_t) + 1\\)</p> <p>14: \\(\\quad\\) Update empirical mean (incremental): </p> <p>15: end for</p>"},{"location":"reinforcement/11_fast_rl/#119-optimistic-initialization-in-greedy-bandit-algorithms","title":"11.9 Optimistic Initialization in Greedy Bandit Algorithms","text":"<p>One of the simplest yet powerful strategies for promoting exploration in bandit algorithms is optimistic initialization. This method enhances a greedy policy with a strong initial incentive to explore, simply by setting the initial action-value estimates to unrealistically high values.</p>"},{"location":"reinforcement/11_fast_rl/#motivation","title":"Motivation","text":"<p>Greedy algorithms, by default, select actions with the highest estimated value:</p> \\[ a_t = \\arg\\max_a \\hat{Q}_t(a) \\] <p>If these \\(\\hat{Q}_t(a)\\) estimates start at zero (or some neutral value), the agent may never try better actions if initial random outcomes favor suboptimal arms. Optimistic initialization addresses this by initializing all action values with high values, thereby making unexplored actions look promising until proven otherwise.</p>"},{"location":"reinforcement/11_fast_rl/#algorithmic-details","title":"Algorithmic Details","text":"<p>We initialize:</p> <ul> <li>\\(\\hat{Q}_0(a) = Q_{\\text{init}}\\) for all \\(a \\in \\mathcal{A}\\), where \\(Q_{\\text{init}}\\) is set higher than any reasonable expected reward (e.g., \\(Q_{\\text{init}} = 1\\) if rewards are bounded in \\([0, 1]\\)).</li> <li>\\(N(a) = 1\\) to ensure initial update is well-defined.</li> </ul> <p>Then we update action values using an incremental Monte Carlo estimate:</p> \\[ \\hat{Q}_{t}(a_t) = \\hat{Q}_{t-1}(a_t) + \\frac{1}{N_t(a_t)} \\left( r_t - \\hat{Q}_{t-1}(a_t) \\right) \\] <p>This update encourages each arm to be pulled at least once, because its high initial estimate makes it look appealing.</p> <ul> <li>Encourages systematic early exploration: Untried actions appear promising and are thus selected.</li> <li>Simple to implement: No need for tuning \\(\\varepsilon\\) or computing uncertainty estimates.</li> <li>Can still lock onto suboptimal arms if the initial values are not optimistic enough.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#key-design-considerations","title":"Key Design Considerations","text":"<ul> <li>How optimistic is optimistic enough?   If \\(Q_{\\text{init}}\\) is not much larger than the true values, the agent may not explore effectively.</li> <li>What if \\(Q_{\\text{init}}\\) is too high?   Overly optimistic values may lead to long periods of exploring clearly suboptimal actions, slowing down learning.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#function-approximation","title":"Function Approximation","text":"<p>Optimistic initialization is non-trivial under function approximation (e.g., with neural networks). With global function approximators, setting optimistic values for one state-action pair may affect others due to shared parameters, making it harder to ensure controlled optimism.</p>"},{"location":"reinforcement/11_fast_rl/#1110-theoretical-frameworks-regret-and-pac","title":"11.10 Theoretical Frameworks: Regret and PAC","text":""},{"location":"reinforcement/11_fast_rl/#regret-based-evaluation","title":"Regret-Based Evaluation","text":"<p>As discussed earlier, regret captures the cumulative shortfall from not always acting optimally. Total regret may arise from:</p> <ul> <li>Many small mistakes (frequent near-optimal actions),</li> <li>A few large mistakes (infrequent but very suboptimal actions).</li> </ul> <p>Minimizing regret growth with \\(T\\) is the dominant criterion in theoretical analysis of bandit and RL algorithms.</p>"},{"location":"reinforcement/11_fast_rl/#probably-approximately-correct-pac-framework","title":"Probably Approximately Correct (PAC) Framework","text":"<p>PAC-style analysis seeks stronger, step-wise performance guarantees, rather than just bounding cumulative regret.</p> <p>An algorithm is \\((\\varepsilon, \\delta)\\)-PAC if, on each time step \\(t\\), it chooses an action \\(a_t\\) such that:</p> \\[ Q(a_t) \\ge Q(a^*) - \\varepsilon \\quad \\text{with probability at least } 1 - \\delta \\] <p>on all but a polynomial number of time steps (in \\(|\\mathcal{A}|\\), \\(1/\\varepsilon\\), \\(1/\\delta\\), etc). This ensures:</p> <ul> <li>The agent almost always behaves nearly optimally,</li> <li>With high probability, after a reasonable amount of time.</li> </ul> <p>PAC is a natural framework when you care about individual-time-step performance rather than only cumulative regret.</p>"},{"location":"reinforcement/11_fast_rl/#comparing-exploration-strategies","title":"Comparing Exploration Strategies","text":"Strategy Regret Behavior Notes Greedy Linear No exploration mechanism Constant \\(\\varepsilon\\)-greedy Linear Fixed chance of exploring Decaying \\(\\varepsilon\\)-greedy Sublinear (if tuned) Requires prior knowledge of reward gaps Optimistic Initialization Sublinear (if optimistic enough) Simple, effective in tabular settings <p>Bottom Line: Optimistic initialization is a computationally simple strategy to induce exploration, but its effectiveness depends crucially on how optimistic the initialization is. In function approximation settings, more principled strategies like UCB or Thompson Sampling may scale better and provide stronger guarantees.</p>"},{"location":"reinforcement/11_fast_rl/#bayesian-bandits","title":"Bayesian Bandits","text":"<p>So far, our treatment of bandits has made no assumptions about the underlying reward distributions, aside from basic bounds (e.g., rewards in \\([0,1]\\)). Bayesian bandits offer a powerful alternative by leveraging prior knowledge about the reward-generating process, and updating our beliefs as data is observed.</p>"},{"location":"reinforcement/11_fast_rl/#key-idea-maintain-beliefs-over-arm-reward-distributions","title":"Key Idea: Maintain Beliefs Over Arm Reward Distributions","text":"<p>In the Bayesian framework, we treat the reward distribution for each arm as governed by an unknown parameter \\(\\\\phi_i\\) for arm \\(i\\). Instead of maintaining a point estimate (e.g., average reward), we maintain a distribution over possible values of \\(\\\\phi_i\\), representing our uncertainty.</p>"},{"location":"reinforcement/11_fast_rl/#prior-and-posterior","title":"Prior and Posterior","text":"<ul> <li>Prior: Our initial belief about \\(\\\\phi_i\\) is encoded in a probability distribution \\(p(\\\\phi_i)\\).</li> <li>Data: After pulling arm \\(i\\) and observing reward \\(r_{i1}\\), we update our belief.</li> <li>Posterior: The new belief is computed using Bayes' rule:</li> </ul> \\[p(\\phi_i \\mid r_{i1}) = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ p(r_{i1}) } = \\frac{ p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i) }{ \\int p(r_{i1} \\mid \\phi_i)\\, p(\\phi_i)\\, d\\phi_i }\\] <p>This posterior becomes the new prior for future updates as more data arrives.</p>"},{"location":"reinforcement/11_fast_rl/#practical-considerations","title":"Practical Considerations","text":"<p>Computing the posterior \\(p(\\phi_i \\mid D)\\) (where \\(D\\) is the observed data for arm \\(i\\)) can be analytically intractable in many cases. However, tractability improves significantly if we use:</p> <ul> <li>Conjugate priors: If the prior and likelihood combine to yield a posterior in the same family as the prior.</li> <li>Many common bandit models use exponential family distributions, which have well-known conjugate priors (e.g., Beta prior for Bernoulli rewards).</li> </ul>"},{"location":"reinforcement/11_fast_rl/#why-use-bayesian-bandits","title":"Why Use Bayesian Bandits?","text":"<ul> <li>Instead of upper-confidence bounds (as in UCB), Bayesian bandits reason directly about uncertainty via posterior distributions.</li> <li>The agent chooses actions based on sampling from or optimizing over the posterior (as in Thompson Sampling).</li> <li>Captures uncertainty in a principled and statistically coherent manner.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#summary","title":"Summary","text":"<ul> <li>Bayesian bandits treat the reward-generating parameters \\(\\phi_i\\) as random variables.</li> <li>We maintain a posterior belief \\(p(\\phi_i \\mid D)\\) using Bayes' rule.</li> <li>When conjugate priors are used, analytical updates are possible.</li> <li>This leads to more informed exploration strategies based on posterior uncertainty rather than hand-designed confidence bounds.</li> </ul>"},{"location":"reinforcement/11_fast_rl/#thompson-sampling","title":"Thompson Sampling:","text":"<p>Thompson Sampling is a principled Bayesian algorithm for balancing exploration and exploitation in bandit problems. It maintains a posterior distribution over the expected reward of each arm and samples from these distributions to make decisions. By sampling, it naturally explores arms with higher uncertainty while favoring those with higher expected rewards, embodying an elegant form of probabilistic optimism.</p> <p>This approach is also known as probability matching: at each time step, the agent selects each arm with probability equal to the chance that it is the optimal arm, according to the current posterior. Unlike greedy methods, Thompson Sampling doesn\u2019t deterministically select the arm with the highest mean\u2014it selects arms in proportion to their likelihood of being best, leading to efficient exploration in uncertain settings.</p> <p>Algorithm: Thompson Sampling:</p> <p>1: Initialize prior over each arm \\(a\\), \\(p(\\mathcal{R}_a)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) For each arm \\(a\\) sample a reward distribution \\(\\mathcal{R}_a\\) from posterior 4: \\(\\quad\\) Compute action-value function \\(Q(a) = \\mathbb{E}[\\mathcal{R}_a]\\) 5: \\(\\quad a_t \\equiv \\arg\\max_{a \\in \\mathcal{A}} Q(a)\\) 6: \\(\\quad\\) Observe reward \\(r\\) 7: \\(\\quad\\) Update posterior \\(p(\\mathcal{R}_a)\\) using Bayes Rule 8: end for  </p>"},{"location":"reinforcement/11_fast_rl/#contextual-bandits","title":"Contextual Bandits","text":"<p>The contextual bandit problem extends the standard multi-armed bandit framework by incorporating side information or context. At each time step, before choosing an action, the agent observes a context \\(x_t\\) drawn i.i.d. from some unknown distribution. The expected reward of each arm depends on this observed context.</p> <p>In this setting, the goal is to learn a context-dependent policy \\(\\pi(a \\mid x)\\) that maps the observed context \\(x_t\\) to a suitable arm \\(a_t\\), maximizing expected reward. Unlike the vanilla bandit setting, where each arm has a fixed reward distribution, here the rewards vary as a function of the context. This makes the problem more expressive and applicable to real-world decision-making scenarios, such as personalized recommendations, ad placement, or clinical treatment selection.</p> <p>Formally, the interaction at each time step \\(t\\) is:</p> <ol> <li>Observe context \\(x_t \\in \\mathcal{X}\\)</li> <li>Choose action \\(a_t \\in \\mathcal{A}\\) based on policy \\(\\pi(a \\mid x_t)\\)</li> <li>Receive reward \\(r_t(a_t, x_t)\\)</li> </ol> <p>Over time, the algorithm must learn to choose actions that maximize expected reward conditioned on context, i.e.,</p> \\[ \\pi^*(x) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E}[r(a, x)] \\] <p>This setting balances exploration across both actions and contexts, and introduces rich generalization capabilities by leveraging contextual information to predict the value of unseen actions in new situations.</p>"},{"location":"reinforcement/11_fast_rl/#mental-map","title":"Mental Map","text":"<pre><code>                Bandits: Foundations of Data-Efficient RL\n     Goal: Understand exploration-exploitation in simplest setting\n           Learn to act with minimal data through principled tradeoffs\n                                \u2502\n                                \u25bc\n               What Are Multi-Armed Bandits (MAB)?\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Single-state (stateless) decision problems                  \u2502\n \u2502 Fixed set of actions (arms)                                 \u2502\n \u2502 Unknown reward distribution per arm                         \u2502\n \u2502 Choose an action, receive reward, repeat                    \u2502\n \u2502 No transition dynamics \u2014 unlike full RL                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Objective: Maximize Reward\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Maximize total reward = minimize regret                     \u2502\n \u2502 Regret = missed opportunity vs optimal action               \u2502\n \u2502 Total regret used to evaluate algorithm efficiency          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                  Basic Bandit Algorithms\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Greedy: exploit current best estimates (linear regret)      \u2502\n \u2502 \u03b5-Greedy: random exploration with fixed \u03b5                   \u2502\n \u2502 Decaying \u03b5-Greedy: reduces \u03b5 over time                      \u2502\n \u2502 Optimistic Initialization: set high initial Q\u0302 values        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Principle: Optimism in the Face of Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat unvisited arms as potentially good                    \u2502\n \u2502 Upper Confidence Bound (UCB) algorithms                     \u2502\n \u2502 Tradeoff: mean reward + exploration bonus                   \u2502\n \u2502 Guarantees sublinear regret                                 \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Algorithmic Realization: UCB1\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 UCB_t(a) = Q\u0302_t(a) + \u221a(2 log t / N_t(a))                     \u2502\n \u2502 Encourages pulling uncertain arms early                     \u2502\n \u2502 Regret \u2248 O(\u221a(T log T))                                      \u2502\n \u2502 Theoretically grounded and simple to implement              \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Theoretical Frameworks: Regret vs PAC\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Regret: cumulative gap from always acting optimally         \u2502\n \u2502 PAC: guarantees near-optimal behavior with high probability \u2502\n \u2502 Regret cares about sum of mistakes; PAC focuses on steps    \u2502\n \u2502 Both evaluate quality and efficiency of learning            \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Bayesian Bandits and Uncertainty\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat arm rewards as random variables                       \u2502\n \u2502 Use prior + observed data \u2192 posterior via Bayes rule        \u2502\n \u2502 Conjugate priors simplify computation                       \u2502\n \u2502 Enable principled uncertainty reasoning                     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Thompson Sampling (Bayesian)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Sample reward distribution from posterior per arm           \u2502\n \u2502 Pull arm with highest sampled reward                        \u2502\n \u2502 Probabilistic optimism: match probability of being best     \u2502\n \u2502 Natural exploration and strong empirical performance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Probability Matching Perspective\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Thompson Sampling \u2248 sample optimal arm w/ correct frequency \u2502\n \u2502 Avoids hard-coded uncertainty bonuses                       \u2502\n \u2502 Simpler and often better in practice                        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                       Contextual Bandits\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Input context x_t at each timestep                          \u2502\n \u2502 Reward distribution depends on (action, context)            \u2502\n \u2502 Learn policy \u03c0(a | x): context-aware decision making        \u2502\n \u2502 Real-world applications: ads, medicine, personalization     \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n          Summary: Bandits as Foundation for Efficient RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Bandits isolate the exploration-exploitation tradeoff       \u2502\n \u2502 Simpler than full RL, but deeply insightful                 \u2502\n \u2502 Concepts generalize to value estimation, uncertainty        \u2502\n \u2502 Key tools: regret, PAC bounds, posterior reasoning          \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/12_fast_mdps/","title":"12. Fast Reinforcement Learning in MDPs and Generalization","text":""},{"location":"reinforcement/12_fast_mdps/#chapter-12-fast-reinforcement-learning-in-mdps-and-generalization","title":"Chapter 12: Fast Reinforcement Learning in MDPs and Generalization","text":"<p>In previous chapters, we focused on exploration strategies in bandits. This chapter builds on those foundations and explores fast learning in Markov Decision Processes (MDPs). We consider various settings (e.g., tabular MDPs, large state/action spaces), evaluation frameworks (e.g., regret, PAC), and principled exploration approaches (e.g., optimism and probability matching).</p> <ul> <li>Bandits: Single-step decision-making problems.</li> <li>MDPs: Sequential decision-making with transition dynamics.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#evaluation-frameworks","title":"Evaluation Frameworks","text":"<p>To assess learning efficiency, we use:</p> <ul> <li>Regret: Cumulative difference between the rewards of the optimal policy and the agent's policy.</li> <li>Bayesian Regret: Expected regret under a prior distribution over MDPs.</li> <li>PAC (Probably Approximately Correct): Number of steps when the policy is not \\(\\epsilon\\)-optimal is bounded with high probability.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#exploration-approaches","title":"Exploration Approaches","text":"<ul> <li>Optimism under uncertainty (e.g., UCB)</li> <li>Probability matching (e.g., Thompson Sampling)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#pac-framework-for-mdps","title":"PAC Framework for MDPs","text":"<p>A reinforcement learning algorithm \\(A\\) is PAC if with probability at least \\(1 - \\delta\\), it selects an \\(\\epsilon\\)-optimal action on all but a bounded number of time steps \\(N\\), where:</p> \\[ N = \\text{poly} \\left( |S|, |A|, \\frac{1}{1 - \\gamma}, \\frac{1}{\\epsilon}, \\frac{1}{\\delta} \\right) \\]"},{"location":"reinforcement/12_fast_mdps/#mbie-eb-model-based-interval-estimation-with-exploration-bonus","title":"MBIE-EB: Model-Based Interval Estimation with Exploration Bonus","text":"<p>The MBIE-EB algorithm (Model-Based Interval Estimation with Exploration Bonuses) is a principled model-based approach to PAC reinforcement learning. It implements the idea of optimism in the face of uncertainty by constructing an upper confidence bound (UCB) on the action-value function \\(Q(s, a)\\).</p> <p>Rather than maintaining optimistic value estimates directly, MBIE-EB achieves optimism indirectly by learning optimistic models of both the reward function and transition dynamics. That is:</p> <ul> <li> <p>It estimates \\(\\hat{R}(s, a)\\) and \\(\\hat{T}(s' \\mid s, a)\\) from data using empirical counts.</p> </li> <li> <p>It augments these estimates with confidence bonuses that reflect the uncertainty due to limited experience.</p> </li> </ul> <p>The Q-function is then computed using dynamic programming over these optimistically biased models, which encourages the agent to explore actions and transitions that are less well understood.</p> <p>In essence, MBIE-EB balances exploitation and exploration by behaving as if the world is more favorable in parts where it has limited data, thereby systematically guiding the agent to reduce its uncertainty over time.</p> <p>Algorithm:</p> <p>1: Given \\(\\epsilon\\), \\(\\delta\\), \\(m\\) 2: \\(\\beta = \\dfrac{1}{1-\\gamma}\\sqrt{0.5 \\ln \\!\\left(\\dfrac{2|S||A|m}{\\delta}\\right)}\\) 3: \\(n_{sas}(s,a,s') = 0\\), \\(\\forall s \\in S, a \\in A, s' \\in S\\) 4: \\(rc(s,a) = 0\\), \\(n_{sa}(s,a) = 0\\), \\(\\hat{Q}(s,a) = \\dfrac{1}{1-\\gamma}\\), \\(\\forall s \\in S, a \\in A\\) 5: \\(t = 0\\), \\(s_t = s_{\\text{init}}\\) 6: loop 7: \\(\\quad a_t = \\arg\\max_{a \\in A} \\hat{Q}(s_t, a)\\) 8: \\(\\quad\\) Observe reward \\(r_t\\) and state \\(s_{t+1}\\) 9: \\(\\quad n_{sa}(s_t,a_t) = n_{sa}(s_t,a_t) + 1\\), \\(\\quad\\quad n_{sas}(s_t,a_t,s_{t+1}) = n_{sas}(s_t,a_t,s_{t+1}) + 1\\) 10: \\(\\quad rc(s_t,a_t) = \\dfrac{rc(s_t,a_t)\\big(n_{sa}(s_t,a_t)-1\\big) + r_t}{n_{sa}(s_t,a_t)}\\) 11: \\(\\quad \\hat{R}(s_t,a_t) = rc(s_t,a_t)\\) and \\(\\quad\\quad \\hat{T}(s' \\mid s_t,a_t) = \\dfrac{n_{sas}(s_t,a_t,s')}{n_{sa}(s_t,a_t)}\\), \\(\\forall s' \\in S\\) 12: \\(\\quad\\) while not converged do 13: \\(\\quad\\quad \\hat{Q}(s,a) = \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s' \\mid s,a)\\max_{a'} \\hat{Q}(s',a') + \\dfrac{\\beta}{\\sqrt{n_{sa}(s,a)}}\\), \\(\\quad\\quad\\quad \\forall s \\in S, a \\in A\\) 14: \\(\\quad\\) end while 15: end loop</p>"},{"location":"reinforcement/12_fast_mdps/#bayesian-model-based-reinforcement-learning","title":"Bayesian Model-Based Reinforcement Learning","text":"<p>Bayesian RL methods maintain a posterior over MDP models \\((P, R)\\) and sample plausible environments from the posterior to plan and act.</p> <p>Thompson Sampling extends naturally from bandits to MDPs by using probability matching over policies. The idea is to choose actions with a probability equal to the probability that they are optimal under the current posterior distribution over MDPs.</p> <p>Formally, the Thompson sampling policy is:</p> \\[ \\pi(s, a \\mid h_t) = \\mathbb{P}\\left(Q(s, a) \\ge Q(s, a'),\\; \\forall a' \\ne a \\;\\middle|\\; h_t \\right) = \\mathbb{E}_{\\mathcal{P}, \\mathcal{R} \\mid h_t} \\left[ \\mathbb{1}\\left(a = \\arg\\max_{a \\in \\mathcal{A}} Q(s, a)\\right) \\right] \\] <p>Where: - \\(h_t\\) is the history up to time \\(t\\) (including all observed transitions and rewards), - \\(\\mathcal{P}, \\mathcal{R}\\) are the transition and reward functions respectively, - The expectation is taken over the posterior belief on the MDP \\((\\mathcal{P}, \\mathcal{R})\\).</p>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-algorithm-in-mdps","title":"Thompson Sampling Algorithm in MDPs","text":"<ol> <li>Maintain a posterior \\(p(\\mathcal{P}, \\mathcal{R} \\mid h_t)\\) over the transition and reward models based on all observed data.</li> <li>Sample a model \\((\\mathcal{P}, \\mathcal{R})\\) from the posterior distribution.</li> <li>Solve the sampled MDP using any planning algorithm (e.g., Value Iteration, Policy Iteration) to obtain the optimal Q-function \\(Q^*(s, a)\\).</li> <li>Select the action according to the optimal action in the sampled model:     </li> </ol>"},{"location":"reinforcement/12_fast_mdps/#algorithm-thompson-sampling-for-mdps","title":"Algorithm: Thompson Sampling for MDPs","text":"<p>1: Initialize prior over dynamics and reward models for each \\((s, a)\\):  \\(\\quad p(\\mathcal{T}(s' \\mid s, a)), \\quad p(\\mathcal{R}(s, a))\\) 2: Initialize initial state \\(s_0\\) 3: for \\(k = 1\\) to \\(K\\) episodes do 4: \\(\\quad\\) Sample an MDP \\(\\mathcal{M}\\): 5: \\(\\quad\\quad\\) for each \\((s, a)\\) pair do 6: \\(\\quad\\quad\\quad\\) Sample transition model \\(\\mathcal{T}(s' \\mid s, a)\\) from posterior 7: \\(\\quad\\quad\\quad\\) Sample reward model \\(\\mathcal{R}(s, a)\\) from posterior 8: \\(\\quad\\quad\\) end for 9: \\(\\quad\\) Compute optimal value function \\(Q_{\\mathcal{M}}^*\\) for sampled MDP \\(\\mathcal{M}\\) 10: \\(\\quad\\) for \\(t = 1\\) to \\(H\\) do 11: \\(\\quad\\quad a_t = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\mathcal{M}}^*(s_t, a)\\) 12: \\(\\quad\\quad\\) Take action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\) 13: \\(\\quad\\) end for 14: \\(\\quad\\) Update posteriors: \\(\\quad\\quad p(\\mathcal{R}_{s_t, a_t} \\mid r_t), \\quad p(\\mathcal{T}(s' \\mid s_t, a_t) \\mid s_{t+1})\\) using Bayes Rule 15: end for</p>"},{"location":"reinforcement/12_fast_mdps/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Exploration via Sampling: Exploration arises implicitly by occasionally sampling optimistic MDPs where uncertain actions appear optimal.</li> <li>Posterior-Driven Behavior: As more data is collected, the posterior concentrates, leading to increasingly greedy behavior.</li> <li>Bayesian Approach: Incorporates prior knowledge and uncertainty in a principled way.</li> </ul> <p>Thompson Sampling combines Bayesian inference with planning and offers a natural extension of bandit-style exploration to full reinforcement learning.</p>"},{"location":"reinforcement/12_fast_mdps/#generalization-in-contextual-bandits","title":"Generalization in Contextual Bandits","text":"<p>Contextual bandits generalize standard bandits by associating a context or state \\(s\\) with each decision:</p> <ul> <li>Reward depends on both context and action: \\(r \\sim P[r | s,a]\\)</li> <li>Often model reward as linear: \\(r = \\theta^\\top \\phi(s,a) + \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#benefits-of-generalization","title":"Benefits of Generalization","text":"<ul> <li>Allows learning across states/actions</li> <li>Enables sample-efficient exploration in large state/action spaces</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#strategic-exploration-in-deep-rl","title":"Strategic Exploration in Deep RL","text":"<p>For high-dimensional domains, tabular methods fail. We must combine exploration with generalization.</p>"},{"location":"reinforcement/12_fast_mdps/#optimistic-q-learning-with-function-approximation","title":"Optimistic Q-Learning with Function Approximation","text":"<p>Modified Q-learning update:</p> \\[ \\Delta w = \\alpha \\left( r + r_{\\text{bonus}}(s,a) + \\gamma \\max_{a'} Q(s', a'; w) - Q(s,a;w) \\right) \\nabla_w Q(s,a;w) \\] <p>Bonus \\(r_{\\text{bonus}}\\) reflects novelty or epistemic uncertainty.</p>"},{"location":"reinforcement/12_fast_mdps/#count-based-and-density-based-exploration","title":"Count-Based and Density-Based Exploration","text":"<ul> <li>Bellemare et al. (2016) use pseudo-counts derived from density models.</li> <li>Ostrovski et al. (2017) leverage pixel-CNNs for density estimation.</li> <li>Tang et al. (2017) use hashing-based counts.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#thompson-sampling-for-deep-rl","title":"Thompson Sampling for Deep RL","text":"<p>Applying Thompson sampling in deep RL is challenging due to the intractability of posterior distributions.</p>"},{"location":"reinforcement/12_fast_mdps/#bootstrapped-dqn","title":"Bootstrapped DQN","text":"<ul> <li>Train multiple Q-networks on bootstrapped datasets.</li> <li>Select one head randomly at each episode for exploration.</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#bayesian-deep-q-networks","title":"Bayesian Deep Q-Networks","text":"<ul> <li>Bayesian linear regression on final layer</li> <li>Posterior used to sample Q-values, enabling optimism</li> <li>Outperforms naive bootstrapped DQNs in some settings</li> </ul>"},{"location":"reinforcement/12_fast_mdps/#mental-map","title":"Mental Map","text":"<pre><code>         Fast Reinforcement Learning in MDPs &amp; Generalization\n  Goal: Learn near-optimal policies in MDPs with limited data\n    Extend bandit exploration ideas to sequential decision making\n                            \u2502\n                            \u25bc\n             Why MDPs Are Harder Than Bandits\n</code></pre> <p>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 MDPs involve sequential decisions with transitions           \u2502  \u2502 Agent must explore over states and transitions              \u2502  \u2502 Exploration affects future knowledge &amp; rewards              \u2502  \u2502 Sample inefficiency is a major practical bottleneck         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                    Evaluation Frameworks for RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Regret: cumulative gap vs optimal policy over time          \u2502  \u2502 PAC (Probably Approximately Correct):                       \u2502  \u2502   Guarantees \u03b5-optimality with high probability             \u2502  \u2502 Bayesian Regret: expected regret under prior over MDPs      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               PAC Learning in MDPs: Formal Guarantee  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Algorithm is PAC if all but N steps are \u03b5-optimal           \u2502  \u2502 N = poly(|S|, |A|, 1/(1-\u03b3), 1/\u03b5, 1/\u03b4)                        \u2502  \u2502 Ensures high-probability performance bounds                 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                Optimism: MBIE-EB Algorithm (Model-Based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Estimate reward + transitions from data                     \u2502  \u2502 Add bonus to Q-values: encourages actions with high uncertainty \u2502  \u2502 Optimistic model induces exploration                        \u2502  \u2502 Dynamic programming over Q\u0302 + bonus \u2192 exploration policy     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Algorithmic Principle: Optimism Under Uncertainty  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Add uncertainty-driven bonus to reward or Q-value           \u2502  \u2502 Drives exploration to unknown regions                       \u2502  \u2502 Simple but effective in tabular MDPs                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian RL and Posterior Sampling  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maintain belief (posterior) over MDP model (P, R)           \u2502  \u2502 Sample MDP from posterior \u2192 plan optimally in sampled MDP   \u2502  \u2502 Leads to probability matching via Thompson Sampling         \u2502  \u2502 Posterior concentrates with data \u2192 convergence to optimal   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Algorithm: Thompson Sampling in Model-Based RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Sample dynamics + rewards from posterior                    \u2502  \u2502 Solve sampled MDP for optimal Q                            \u2502  \u2502 Act according to Q in sample MDP                           \u2502  \u2502 Update posterior using Bayes rule after each step           \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              Exploration via Posterior Variance (Bayes)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Thompson Sampling \u2248 Probability Matching                    \u2502  \u2502 Probabilistically favors optimal but uncertain policies     \u2502  \u2502 Elegant &amp; adaptive exploration                             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Generalization via Contextual Bandits  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Rewards depend on both context and action                   \u2502  \u2502 Learn generalizable function: Q(s,a) or \u03c0(a|s)              \u2502  \u2502 Enables learning across states / actions                    \u2502  \u2502 Use linear models or embeddings: \u03c6(s,a)                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc          Exploration + Generalization in Deep RL Settings  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Optimistic Q-learning: add r_bonus(s,a) in TD target        \u2502  \u2502 r_bonus from novelty, density models, or uncertainty        \u2502  \u2502 Count-based, hashing, or learned density bonuses            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                  Bayesian Deep RL: Posterior Approximation  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Bootstrapped DQN: ensemble of Q-networks for exploration    \u2502  \u2502 Bayesian DQN: sample from approximate Q-posteriors          \u2502  \u2502 Enables implicit Thompson-like behavior                     \u2502  \u2502 Scales to high-dimensional state/action spaces              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc                          Chapter Summary  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Strategic exploration = key to fast learning in MDPs        \u2502  \u2502 Optimism (MBIE-EB) and Bayesian methods (Thompson)          \u2502  \u2502 PAC and Bayesian regret are key evaluation tools            \u2502  \u2502 Generalization (via features or deep nets) enables scaling  \u2502  \u2502 Thompson Sampling and bootstrapped approximations bridge gap\u2502  \u2502 Between tabular and high-dimensional RL                     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ````</p>"},{"location":"reinforcement/13_montecarlo/","title":"13. Monte Carlo Tree Search and Planning","text":""},{"location":"reinforcement/13_montecarlo/#chapter-13-monte-carlo-tree-search","title":"Chapter 13: Monte Carlo Tree Search","text":"<p>Monte Carlo Tree Search (MCTS) is a powerful planning algorithm that uses simulation-based search to select actions in complex decision-making problems. It is especially effective in large or unknown environments where exact planning is infeasible. MCTS balances exploration and exploitation through sampling and is the backbone of major AI breakthroughs like AlphaGo and AlphaZero.</p>"},{"location":"reinforcement/13_montecarlo/#131-motivation","title":"13.1 Motivation","text":"<p>In classical reinforcement learning (RL), agents often compute policies over the entire state space. MCTS takes a different approach: it performs local search from the current state, using simulated episodes to estimate action values and make near-optimal decisions on the fly.</p> <p>This method is particularly useful in:</p> <ul> <li>Large state/action spaces</li> <li>Games with high branching factor (e.g., Go, Chess)</li> <li>Black-box or simulator-only environments</li> </ul>"},{"location":"reinforcement/13_montecarlo/#132-monte-carlo-search","title":"13.2 Monte Carlo Search","text":"<p>A simple Monte Carlo search uses a model \\(\\mathcal{M}\\) (dynamics and resward model) and a rollout policy \\(\\pi\\) to simulate \\(K\\) trajectories for each action \\(a\\) from the current state \\(s_t\\):</p> <ol> <li>Simulate episodes \\(\\{s_t, a, r_{t+1}^{(k)}, \\ldots, s_T^{(k)}\\}\\) from \\(\\mathcal{M}, \\pi\\).</li> <li>Estimate \\(Q(s_t, a)\\) via sample average:</li> </ol> \\[ Q(s_t, a) = \\frac{1}{K} \\sum_{k=1}^K G_t^{(k)} \\rightarrow q^\\pi(s_t, a) \\] <ol> <li>Select the best action:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\] <p>This performs one-step policy improvement, but does not build deeper search trees.</p>"},{"location":"reinforcement/13_montecarlo/#133-expectimax-search","title":"13.3 Expectimax Search","text":"<p>To go beyond single-step rollouts, expectimax trees compute \\(Q^*(s, a)\\) recursively using the model:</p> <ul> <li>Each node expands by looking ahead using the transition model.</li> <li>Combines maximization (over actions) and expectation (over next states).</li> <li>Forward search avoids solving the entire MDP and focuses only on the subtree starting at \\(s_t\\).</li> </ul> <p>However, the number of nodes grows exponentially with horizon \\(H\\): \\(O(|S||A|)^H\\).</p>"},{"location":"reinforcement/13_montecarlo/#134-monte-carlo-tree-search-mcts","title":"13.4 Monte Carlo Tree Search (MCTS)","text":"<p>MCTS improves on expectimax by sampling rather than fully expanding the tree:</p> <ol> <li>Build a tree rooted at current state \\(s_t\\).</li> <li>Perform \\(K\\) simulations to expand and update parts of the tree.</li> <li>Estimate \\(Q(s, a)\\) using sampled returns.</li> <li>Select the best action at the root:</li> </ol> \\[ a_t = \\arg\\max_a Q(s_t, a) \\]"},{"location":"reinforcement/13_montecarlo/#135-upper-confidence-tree-uct","title":"13.5 Upper Confidence Tree (UCT)","text":"<p>A key challenge in MCTS is deciding which action to simulate at each tree node. UCT addresses this by treating each decision as a multi-armed bandit problem and using an Upper Confidence Bound:</p> \\[ Q(s, a, i) = \\underbrace{\\frac{1}{N(i, a)} \\sum_{k=1}^{N(i,a)} G_k(i,a)}_{\\text{Mean Return}} + \\underbrace{c \\sqrt{\\frac{\\log N(i)}{N(i, a)}}}_{\\text{Exploration Bonus}} \\] <ul> <li>\\(N(i, a)\\): number of times action \\(a\\) taken at node \\(i\\)</li> <li>\\(N(i)\\): total visits to node \\(i\\)</li> <li>\\(c\\): exploration constant</li> <li>\\(G_k(i, a)\\): return from simulation \\(k\\) for \\((i, a)\\)</li> </ul> <p>Action selection:</p> \\[ a_k^i = \\arg\\max_a Q(s, a, i) \\] <p>This balances exploitation of known good actions and exploration of uncertain ones.</p>"},{"location":"reinforcement/13_montecarlo/#136-advantages-of-mcts","title":"13.6 Advantages of MCTS","text":"<ul> <li>Anytime: Can stop search at any time and use the best estimates so far.</li> <li>Model-based or black-box: Only needs sample access to the environment.</li> <li>Best-first: Focuses computation on promising actions.</li> <li>Scalable: Avoids full enumeration of action/state spaces.</li> <li>Parallelizable: Independent simulations can be run in parallel.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#137-alphazero-and-deep-mcts","title":"13.7 AlphaZero and Deep MCTS","text":"<p>AlphaZero revolutionized game-playing AI by combining deep learning with MCTS. Key ideas:</p>"},{"location":"reinforcement/13_montecarlo/#policy-and-value-networks","title":"Policy and Value Networks","text":"<p>A neural network \\(f_\\theta(s)\\) outputs:</p> <ul> <li>\\(P\\): action probabilities</li> <li>\\(V\\): value estimate</li> </ul> \\[ (p, v) = f_\\theta(s) \\]"},{"location":"reinforcement/13_montecarlo/#alphazero-mcts-steps","title":"AlphaZero MCTS Steps","text":"<ol> <li>Select: Traverse tree using \\(Q + U\\) to choose child nodes.</li> <li>Expand: Add a new node, initialized with \\(P\\) from \\(f_\\theta\\).</li> <li>Evaluate: Use \\(v\\) from the network as the value of the leaf.</li> <li>Backup: Propagate value estimates up the tree.</li> <li>Repeat: Perform many rollouts to refine the tree.</li> </ol>"},{"location":"reinforcement/13_montecarlo/#root-action-selection","title":"Root Action Selection","text":"<p>At the root, use visit counts \\(N(s,a)\\) to compute the improved policy:</p> \\[ \\pi(s, a) \\propto N(s, a)^{1/\\tau} \\] <p>where \\(\\tau\\) controls exploration vs exploitation.</p>"},{"location":"reinforcement/13_montecarlo/#138-self-play-and-training","title":"13.8 Self-Play and Training","text":"<p>AlphaZero uses self-play to generate training data:</p> <ol> <li>Play full games using MCTS.</li> <li>Record \\((s, \\pi, z)\\) tuples where:</li> <li>\\(s\\): game state</li> <li>\\(\\pi\\): improved policy from MCTS</li> <li>\\(z\\): final game outcome</li> <li>Train \\(f_\\theta\\) to minimize combined loss:</li> </ol> \\[ \\mathcal{L} = (z - v)^2 - \\pi^\\top \\log p + \\lambda \\|\\theta\\|^2 \\] <p>This allows continual improvement without human supervision.</p>"},{"location":"reinforcement/13_montecarlo/#139-evaluation-and-impact","title":"13.9 Evaluation and Impact","text":"<ul> <li>MCTS dramatically improves performance over raw policy/value networks.</li> <li>Essential to surpassing human performance in Go, Chess, and Shogi.</li> <li>Eliminates the need for human expert data.</li> </ul> <p>Insights:</p> <ul> <li>UCT enables principled tree search with exploration.</li> <li>Neural nets guide and accelerate MCTS.</li> <li>MCTS can be used in any environment where lookahead is possible.</li> </ul>"},{"location":"reinforcement/13_montecarlo/#1310-summary","title":"13.10 Summary","text":"<ul> <li>MCTS uses simulation-based planning with a growing search tree.</li> <li>UCT adds upper confidence bounds to balance exploration/exploitation.</li> <li>AlphaZero combines MCTS with deep learning for superhuman performance.</li> <li>Self-play enables autonomous training without labeled data.</li> </ul> <p>MCTS represents a powerful bridge between planning and learning, enabling agents to make strong decisions under uncertainty in complex domains.</p>"},{"location":"reinforcement/14_final/","title":"14. Summary and Overview","text":""},{"location":"reinforcement/14_final/#chapter-14-c","title":"Chapter 14: c","text":"<p>In this final chapter, we recap the journey of reinforcement learning (RL) from its foundational ideas in multi-armed bandits through to the cutting-edge of deep RL. Along the way we will revisit key algorithmic concepts \u2013 including Upper Confidence Bounds (UCB), Thompson Sampling, Model-Based Interval Estimation with Exploration Bonus (MBIE-EB), and Monte Carlo Tree Search (MCTS) \u2013 and highlight how different approaches to exploration (optimism vs. probability matching) have shaped the field. We will also emphasize the theoretical foundations of RL (regret minimization, PAC guarantees, Bayesian methods) and illustrate how these principles connect to real-world successes like AlphaTensor and ChatGPT. Throughout, the aim is to provide a high-level summary and synthesis, reinforcing the insights gained across previous chapters.</p>"},{"location":"reinforcement/14_final/#recap-from-bandits-to-deep-reinforcement-learning","title":"Recap: From Bandits to Deep Reinforcement Learning","text":"<p>Reinforcement learning can be defined as learning through experience (data) to make good decisions under uncertainty. In an RL problem, an agent interacts with an environment, observes states \\(s\\), takes actions \\(a\\), and receives rewards \\(r\\), with the goal of learning a policy \\(\\pi(a|s)\\) that maximizes future expected reward. Several core features distinguish RL from other learning paradigms:</p> <ul> <li> <p>Optimization of Long-Term Reward: The agent seeks to maximize cumulative reward, accounting for delayed consequences of actions.</p> </li> <li> <p>Trial-and-Error Learning: The agent learns by exploring different actions and observing outcomes, balancing exploration vs. exploitation.</p> </li> <li> <p>Generalization: The agent must generalize from limited experience to new situations (often via function approximation in large state spaces).</p> </li> <li> <p>Data Distribution Shift: Unlike supervised learning, the agent\u2019s own actions affect the data it collects and the states it visits, creating a feedback loop in the learning process.</p> </li> </ul> <p>We began our journey with multi-armed bandits, the simplest RL setting. In a bandit problem there is a single state (no state transitions); each action (arm) yields a reward drawn from an unknown distribution, and the goal is to maximize reward over repeated plays. A bandit is essentially a stateless decision problem \u2013 the next situation does not depend on the previous action. This contrasts with the general Markov Decision Process (MDP) setting, where each action can change the state and influence future rewards and decisions. Bandits capture the essence of exploration-exploitation without the complication of state transitions, making them a perfect starting point.</p> <p>From bandits we progressed to MDPs and multi-step RL problems, which introduce state dynamics and temporal credit assignment. We studied model-free methods (like Q-learning and policy gradient) and model-based methods (like planning with known models or learned models), as well as combinations thereof. As tasks grew more complex, we incorporated function approximation (e.g. using deep neural networks) to handle large or continuous state spaces. This led us into the realm of deep reinforcement learning, where algorithms like DQN and policy optimization methods (PPO, etc.) leverage deep networks as powerful function approximators. While function approximation enables scaling to complex domains, it also introduced new challenges such as stability of learning (e.g. off-policy learning instability, need for techniques like experience replay, target networks, or trust region methods). In parallel, we discussed how off-policy learning and exploration in large domains remain critical challenges, and saw approaches to address these (from clipped policy optimization (PPO) for stability, to imitation learning like DAGGER to incorporate expert knowledge, to pessimistic value adjustments for safer offline learning).</p> <p>Throughout this journey, a unifying theme has been the exploration-exploitation dilemma and the development of algorithms to efficiently learn optimal strategies. In the following sections, we summarize some key algorithmic ideas for exploration and discuss how they exemplify different strategies to address this core challenge.</p>"},{"location":"reinforcement/14_final/#key-algorithmic-ideas-in-exploration-and-planning","title":"Key Algorithmic Ideas in Exploration and Planning","text":""},{"location":"reinforcement/14_final/#optimistic-exploration-upper-confidence-bounds-ucb","title":"Optimistic Exploration: Upper Confidence Bounds (UCB)","text":"<p>A foundational idea for efficient exploration is optimism in the face of uncertainty. The principle is simple: assume the best about untried actions so that the agent is driven to explore them. The Upper Confidence Bound (UCB) algorithm is a classic realization of this idea for multi-armed bandits. UCB maintains an estimate \\(\\hat{Q}_t(a)\\) for the mean reward of each arm \\(a\\) and an uncertainty interval (confidence bound) around that estimate. At each time \\(t\\), it selects the action maximizing an upper-confidence estimate of the reward:</p> \\[ a_t = \\arg\\max_{a \\in A} \\left[ \\hat{Q}_t(a) + c \\frac{\\ln t}{N_t(a)} \\right], \\] <p>where \\(N_t(a)\\) is the number of times action \\(a\\) has been taken up to time \\(t\\), and \\(c\\) is a constant (e.g. \\(c=\\sqrt{2}\\) for the UCB1 algorithm).</p> <p>This selection rule balances exploitation (the \\(\\hat{Q}_t(a)\\) term) with exploration (the bonus term that is large for rarely-selected actions). Intuitively, UCB explores actions with high potential payoffs or high uncertainty. This approach yields strong theoretical guarantees: for instance, UCB1 achieves sublinear regret on the order of \\(O(\\ln T)\\) for bandits, meaning the gap between the accumulated reward of UCB and that of an oracle choosing the best arm at each play grows only logarithmically with time. Optimistic algorithms like UCB are attractive because they are simple and provide worst-case performance guarantees (they will eventually try everything enough to near-certainty). Variants of UCB and optimism-driven exploration have been extended beyond bandits, for example to MDPs via exploration bonus terms.</p>"},{"location":"reinforcement/14_final/#probability-matching-thompson-sampling","title":"Probability Matching: Thompson Sampling","text":"<p>An alternative approach to exploration comes from a Bayesian perspective. Instead of confidence bounds, the agent maintains a posterior distribution over the reward parameters of each action and samples an action according to the probability it is optimal. This strategy is known as Thompson Sampling (or probability matching). In the multi-armed bandit setting, Thompson Sampling can be implemented by assuming a prior for each arm\u2019s mean reward, updating it with observed rewards, and then at each step sampling a value \\(\\tilde{\\theta}_a\\) from the posterior of each arm\u2019s mean. The agent then plays the arm with the highest sampled value. By randomly exploring according to its uncertainty, Thompson Sampling naturally balances exploration and exploitation in a Bayesian-optimal way for certain problems.</p> <p>For example, if rewards are Bernoulli and a Beta prior is used for each arm\u2019s success probability, Thompson Sampling draws a sample from each arm\u2019s Beta posterior and picks the arm with the largest sample. This probability matching tends to allocate more trials to arms that are likely to be best, yet still occasionally tries others proportional to uncertainty. Empirically, Thompson Sampling often performs exceptionally well, sometimes even outperforming UCB in practice, and it has a Bayesian regret that is optimal in certain settings. The caveat is that analyzing Thompson Sampling\u2019s worst-case performance is more complex; however, theoretical advances have shown Thompson Sampling achieves \\(O(\\ln T)\\) regret for many bandit problems as well. A key appeal of Thompson Sampling is its flexibility \u2013 it can be applied to complex problems if one can sample from a posterior (or an approximate posterior) of the model\u2019s parameters. In modern RL, variants of Thompson Sampling inspire approaches like Bootstrapped DQN (which maintains an ensemble of value networks to generate randomized Q-value estimates for exploration).</p>"},{"location":"reinforcement/14_final/#pac-mdp-algorithms-and-exploration-bonuses-mbie-eb","title":"PAC-MDP Algorithms and Exploration Bonuses (MBIE-EB)","text":"<p>In full reinforcement learning problems (MDPs), the exploration challenge becomes more intricate due to state transitions. PAC-MDP algorithms provide a framework for efficient exploration with theoretical guarantees. PAC stands for \u201cProbably Approximately Correct,\u201d meaning these algorithms guarantee that with high probability (\\(1-\\delta\\)) the agent will behave near-optimally (within \\(\\varepsilon\\) of the optimal return) after a certain number of time steps that is polynomial in relevant problem parameters. In other words, a PAC-MDP algorithm will make only a finite (polynomial) number of suboptimal decisions before it effectively converges to an \\(\\varepsilon\\)-optimal policy.</p> <p>One representative PAC-MDP approach is Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) by Strehl and Littman (2008). This algorithm uses an optimistic model-based strategy: it learns an estimated MDP (transition probabilities \\(\\hat{T}\\) and rewards \\(\\hat{R}\\)) from experience and uses dynamic programming to compute a value function \\(\\tilde{Q}(s,a)\\) for that estimated model. Critically, MBIE-EB adds an exploration bonus term to reward or value updates for state-action pairs that have been infrequently visited. For example, the update might be:</p> \\[ \\tilde{Q}(s,a) \\leftarrow \\hat{R}(s,a) + \\gamma \\sum_{s'} \\hat{T}(s'|s,a) \\max_{a'} \\tilde{Q}(s',a') + \\beta \\frac{1}{\\sqrt{N(s,a)}}, \\] <p>where \\(N(s,a)\\) counts visits to \\((s,a)\\) and \\(\\beta\\) is a bonus scale derived from PAC confidence bounds. The \\(\\sqrt{1/N(s,a)}\\) bonus term is large for rarely tried state-action pairs, injecting optimism that encourages the agent to explore them. MBIE-EB selects actions according to the optimistic \\(\\tilde{Q}\\) values (i.e. optimism under uncertainty in an MDP context). Strehl and Littman proved that MBIE-EB is PAC-MDP: with probability \\(1-\\delta\\), after a number of steps polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta\\), etc., the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal. PAC algorithms like MBIE-EB (and related methods like R-MAX and UCRL) guarantee efficient exploration in theory, though they can be computationally demanding in practice for large domains. They illustrate how theoretical foundations (confidence intervals and PAC guarantees) directly inform algorithm design.</p>"},{"location":"reinforcement/14_final/#monte-carlo-tree-search-mcts-for-planning","title":"Monte Carlo Tree Search (MCTS) for Planning","text":"<p>So far we have discussed exploration in the context of learning unknown values or models. Another key idea in the RL toolkit is planning using simulation, particularly via Monte Carlo Tree Search (MCTS). MCTS is a family of simulation-based search algorithms that became famous through their use in game-playing AI (e.g. AlphaGo and AlphaZero). The idea is to build a partial search tree from the current state by simulating many random play-outs (rollouts) and using the results to gradually refine value estimates for states and actions.</p> <p>One of the most widely used MCTS algorithms is UCT (Upper Confidence Trees), which blends the UCB idea with tree search. In each simulation (from root state until a terminal state or depth limit), UCT traverses the tree by choosing actions that maximize an upper confidence bound: at a state (tree node) \\(s\\), it selects the action \\(a\\) that maximizes</p> \\[ \\frac{w_{s,a}}{n_{s,a}} + c \\sqrt{\\frac{\\ln N_s}{n_{s,a}}}, \\] <p>where \\(w_{s,a}\\) is the total reward accrued from past simulations taking action \\(a\\) in state \\(s\\), \\(n_{s,a}\\) is the number of simulations that took that action, and \\(N_s = \\sum_a n_{s,a}\\) is the total simulations from state \\(s\\). This formula is essentially the UCB1 formula extended to tree nodes: the first term is exploitation (the empirical mean reward), and the second is an exploration bonus that is higher for seldom-tried actions. By using this rule at each step of simulation (Selection phase), MCTS efficiently explores the game tree, focusing on promising moves while still trying less-visited moves once in a while. After selection, a random Simulation (rollout) is played out to the end, and the outcome is backpropagated to update \\(w\\) and \\(n\\) along the path. Repeating thousands or millions of simulations yields increasingly accurate value estimates for the root state and preferred actions.</p> <p>MCTS does not learn parameters from data in the traditional sense; rather it is a planning method that can be applied if we have a generative model of the environment (e.g. a simulator or game rules). However, it connects to our theme as another approach to balancing exploration and exploitation via UCB-like algorithms. In practice, MCTS can be combined with learning. Notably, AlphaGo and AlphaZero combined deep neural networks (for state evaluation and policy guidance) with Monte Carlo Tree Search to achieve superhuman performance in Go, chess, and shogi. In those systems, the neural network\u2019s value estimates guide the rollout, and MCTS provides a powerful lookahead search that complements the learned policy. This combination dramatically improves data efficiency \u2013 for example, AlphaZero uses MCTS to effectively explore the game space instead of needing an exorbitant amount of self-play games, and the knowledge gained from MCTS is distilled back into the network through training. MCTS exemplifies how models and planning can be leveraged in RL: if a model of the environment is available (or learned), one can simulate experience to aid decision-making without direct real-world trial-and-error for every decision. This is crucial in domains where real experiments are costly or limited.</p> <p>Computational vs Data Efficiency: It is worth noting that methods like MCTS (and exhaustive exploration algorithms) tend to be computationally intensive \u2013 they trade computation for reduced real-world data needs. We often face a trade-off: algorithms that are very data-efficient (using fewer environment interactions) are often computationally expensive, whereas simpler algorithms that learn quickly in computation might require more data. In some domains (like games or simulated environments), we can afford massive computation, effectively converting computation into simulated \u201cdata\u201d for learning. In others (like physical systems or online user interactions), data is scarce or expensive, so sample-efficient algorithms (even if computationally heavy) are preferred. This trade-off has been a recurring consideration as we moved from bandits to deep RL.</p>"},{"location":"reinforcement/14_final/#exploration-paradigms-optimism-vs-probability-matching","title":"Exploration Paradigms: Optimism vs. Probability Matching","text":"<p>We have seen two major paradigms for addressing the exploration-exploitation challenge:</p> <ul> <li> <p>Optimism in the face of uncertainty: The agent behaves as if the environment is as rewarding as plausibly possible, given the data. This leads to algorithms like UCB, optimistic initial values, exploration bonuses (e.g. MBIE-EB, optimistic Q-learning), and UCT in MCTS. Optimistic methods systematically encourage trying actions that could be best. They often come with strong theoretical guarantees (UCB\u2019s regret bound, PAC-MDP bounds, etc.) because they ensure sufficient exploration of each alternative. Optimism tends to be a more worst-case (frequentist) approach: it doesn\u2019t assume a prior, just relies on confidence intervals that hold with high probability for any reward distribution.</p> </li> <li> <p>Probability matching (Thompson Sampling and Bayesian methods): The agent maintains a belief (probability distribution) about the environment\u2019s parameters and randomizes its actions according to this belief. Effectively, it samples a hypothesis for the true model and then exploits that hypothesis (e.g., play the best action for that sampled model). Over time, the belief is updated with Bayes\u2019 rule as more data comes in, so the sampling naturally shifts toward optimal actions. This approach is more Bayesian in spirit: it assumes a prior distribution and seeks to maximize performance on average with respect to that prior (i.e., good Bayesian regret). Probability matching can be very effective in practice and can incorporate prior knowledge elegantly. The downside is that providing theoretical guarantees in the worst-case sense can be challenging \u2013 the guarantees are often Bayesian (in expectation over the prior) rather than uniform for all environments. Recent theoretical work, however, has shown that even without a perfect prior, Thompson Sampling performs near-optimally in many settings, and there are ways to bound its regret. In terms of implementation complexity, Thompson Sampling may require the ability to sample from posterior distributions, which can be non-trivial in large-scale problems (though approximate methods exist). Optimistic methods, on the other hand, require confidence bound calculations, which for simple tabular cases are straightforward, but for complex function approximation can be difficult (leading to research on exploration bonuses using predictive models or uncertainty estimates).</p> </li> </ul> <p>In summary, optimism vs. probability matching represents two different philosophies for exploration. Optimistic algorithms behave more deterministically (always picking the current optimistic-best option), ensuring systematic coverage of possibilities, while Thompson-style algorithms inject randomized exploration in proportion to uncertainty. Interestingly, human decision-making experiments suggest people may combine elements of both strategies \u2013 not purely optimistic nor purely Thompson. Both paradigms have influenced modern RL: for example, exploration bonuses (optimism) are commonly used in deep RL (e.g. with bonus rewards from prediction error or curiosity), and Bayesian RL approaches (like posterior sampling for MDPs) are gaining traction for problems where a reasonable prior is available or an ensemble can approximate uncertainty.</p>"},{"location":"reinforcement/14_final/#theoretical-foundations-regret-pac-and-bayesian-optimality","title":"Theoretical Foundations: Regret, PAC, and Bayesian Optimality","text":"<p>Understanding how well an RL algorithm performs relative to an ideal standard is a major theme in RL theory. We revisited two main frameworks for this: regret analysis and PAC (sample complexity) analysis, along with the Bayesian viewpoint.</p> <ul> <li>Regret: Regret measures the opportunity loss from not acting optimally at each time step. Formally, in a bandit with optimal expected reward \\(\\mu^*\\), the regret after \\(T\\) plays is</li> </ul> \\[ R(T) = T\\mu^* - \\sum_{t=1}^T r_t, \\] <p>i.e. the difference between the reward that would be obtained by always executing the optimal arm and the reward actually obtained. Sublinear regret (e.g. \\(R(T) = o(T)\\)) implies the algorithm eventually learns the optimal policy (average regret \\(\\to 0\\) as \\(T\\) grows). We saw that \\(\\varepsilon\\)-greedy exploration can lead to linear regret in the worst case (always pulling some suboptimal arm a constant fraction of the time yields \\(R(T) \\sim \\Omega(T)\\)). In contrast, UCB1 achieves \\(R(T) = O(\\ln T)\\), which is asymptotically optimal up to constant factors (matching the Lai &amp; Robbins lower bound for bandits that \\(R(T) \\ge \\Omega(\\ln T)\\) for any algorithm). Regret analysis can be extended to MDPs (though it becomes more complex). For example, algorithms like UCRL2 (an optimistic tabular RL algorithm) have regret bounds on the order of \\(\\tilde{O}(\\sqrt{T})\\) in an MDP (reflecting the harder challenge of states) under certain assumptions. Regret is a worst-case, online metric \u2013 it asks how well we do even against an adversarially chosen problem (or in the unknown actual environment) without assumptions of a prior, focusing on long-term performance.</p> <ul> <li> <p>PAC (Probably Approximately Correct) guarantees: PAC analysis focuses on sample complexity: how many time steps or episodes are required for the algorithm to achieve near-optimal performance with high probability. A PAC guarantee typically states: for any \\(\\varepsilon, \\delta\\), there exists \\(N(\\varepsilon,\\delta)\\) (poly in relevant parameters) such that with probability at least \\(1-\\delta\\), the algorithm\u2019s policy is \\(\\varepsilon\\)-optimal after \\(N\\) steps (or, equivalently, all but at most \\(N\\) of the steps are \\(\\varepsilon\\)-suboptimal). This is a finite-sample guarantee, giving confidence that the learning will not take too long. We discussed that algorithms like MBIE-EB and R-MAX are PAC-MDP: for a given accuracy \\(\\varepsilon\\) and confidence \\(1-\\delta\\), their sample complexity (number of suboptimal actions) is bounded by a polynomial in \\(|S|, |A|, 1/\\varepsilon, 1/\\delta, 1/(1-\\gamma)\\), etc. PAC analysis is particularly useful when we care about guarantees in a learning phase before near-optimal performance is reached (important in safety-critical or costly domains where we need to know learning will be efficient with high probability). While regret goes to zero only asymptotically, PAC gives an explicit bound on how long it takes to be good. Often, achieving PAC guarantees in large-scale problems requires simplifying assumptions or limited function approximation classes, as general function approximation PAC results are quite difficult.</p> </li> <li> <p>Bayesian approaches and Bayes-optimality: In a Bayesian formulation, we assume a prior distribution over environments (bandit reward distributions or MDP dynamics). We can then consider the Bayes-optimal policy, which is the policy that maximizes expected cumulative reward with respect to this prior. This leads to the concept of Bayesian regret \u2013 the expected regret under the prior. A Bayes-optimal algorithm minimizes Bayesian regret and, by definition, will outperform any other algorithm on average if the prior is correct. One famous result in this vein is the Gittins Index for multi-armed bandits, which gives an optimal solution when each arm has independent known priors (casting the problem as a Markov process and solving it via dynamic programming). However, computing Bayes-optimal solutions for general RL (especially with state) is usually intractable \u2013 it involves solving a POMDP (partially observable MDP) where the hidden state is the true environment parameters. Thompson Sampling can be interpreted as an approximation to the Bayes-optimal policy that is much easier to implement. It has low Bayesian regret and in some cases can be shown to be asymptotically Bayes-optimal. The Bayesian view is powerful because it allows incorporation of prior knowledge and gives a normative standard (what should we do if we know what we don\u2019t know, in distribution). But its limitation is the computational difficulty and the dependence on having a reasonable prior. In practice, algorithms inspired by Bayesian ideas (like ensemble sampling or posterior sampling for reinforcement learning) try to capture some of the benefit without solving the full Bayes-optimal policy.</p> </li> </ul> <p>These theoretical frameworks complement each other. Regret and PAC analyses give worst-case performance assurances (no matter what the true environment is, within assumptions) and often inspire optimistic algorithms. Bayesian analysis aims for average-case optimality given prior knowledge and often inspires probability matching or adaptive algorithms. As an RL practitioner or researcher, understanding these foundations helps in choosing and designing algorithms appropriate for the problem at hand \u2013 whether one prioritizes guaranteed efficiency, practical performance with prior info, or a mix of both.</p>"},{"location":"reinforcement/14_final/#from-theory-to-practice-real-world-applications-and-achievements","title":"From Theory to Practice: Real-World Applications and Achievements","text":"<p>One of the most exciting aspects of the recent decade in RL is seeing theoretical ideas translate into real-world (or at least real-problem) successes. In this section, we connect some of the classic algorithms and concepts to notable applications:</p> <ul> <li> <p>Game Mastery and Planning \u2013 AlphaGo, AlphaZero, AlphaTensor: Starting with games, AlphaGo famously combined deep neural networks with MCTS (using UCT) and was trained with reinforcement learning to defeat human Go champions. Its successor AlphaZero took this further by learning from scratch (self-play) for multiple games, using Monte Carlo Tree Search guided by a learned value/policy network. The blend of planning (MCTS) and learning (deep RL) that AlphaZero employs is a direct embodiment of concepts we covered: it uses optimistic simulations (MCTS uses UCB in the tree) and improves data efficiency by leveraging a model (the game simulator) for exploration. The success of AlphaZero demonstrates the power of combining model-based search with model-free function approximation. Recently, these ideas have even extended to domains beyond traditional games. AlphaTensor (DeepMind, 2022) is a system that treated the discovery of new matrix multiplication algorithms as a single-player game, and it applied a variant of AlphaZero\u2019s RL approach to find faster algorithms for matrix multiply. The AlphaTensor agent was trained via self-play reinforcement learning to manipulate tensor representations of matrix multiplication and achieved a breakthrough: it discovered matrix multiplication algorithms that surpass the decades-old human benchmarks in efficiency. This is a striking example of RL not just playing games but discovering algorithms \u2013 essentially using reward signals to guide a search through the space of mathematical formulas. It showcases how MCTS (for planning) and deep RL can work together on combinatorial optimization problems: the agent expands a search tree of partial solutions, guided by value networks and an exploration policy, very much like how it would approach a board game. AlphaTensor\u2019s success underscores the generality of RL methods and how ideas like optimism (self-play explores new moves) and guided search can yield new discoveries.</p> </li> <li> <p>Natural Language and Human Feedback \u2013 ChatGPT: A more recent and widely impactful application of reinforcement learning is in natural language processing \u2013 specifically, training large language models to better align with human intentions. ChatGPT (OpenAI, 2022) is a prime example, where RL was used to fine-tune a pretrained language model using human feedback. The technique, known as Reinforcement Learning from Human Feedback (RLHF), involves first collecting human preference data on model outputs and then training a reward model that predicts human preference. The language model (policy) is then optimized (via a policy gradient method like PPO) to maximize the reward model\u2019s score, i.e. to produce answers humans would rate highly. This is essentially an RL loop on top of the language model, treating the task of generating helpful, correct responses as an MDP (or episodic decision problem) and using the learned reward function as the reward signal. The result, ChatGPT, is notably more aligned with user expectations than its predecessor models. In our context, ChatGPT\u2019s training illustrates several RL ideas in action: offline data (pretraining on text) combined with online RL fine-tuning, and the critical role of a well-shaped reward function for alignment. It also highlights exploration in a different sense \u2013 exploring the space of possible answers to find those that yield high reward according to human feedback. The success of ChatGPT demonstrates that RL is not limited to games or robotics; it can be scaled to very high-dimensional action spaces (like generating entire paragraphs of text) when guided by human-informed rewards. From a theoretical lens, one can view RLHF as optimizing an objective that marries the model\u2019s knowledge (from supervised training) with a policy optimization under a learned reward. While classical exploration algorithms (UCB, Thompson) are not directly apparent in ChatGPT\u2019s training (since the \u201cexploration\u201d comes from the model generating varied outputs and the policy optimization process), the high-level principle remains: use feedback signals to iteratively refine behavior.</p> </li> <li> <p>Scientific and Industrial Applications: Beyond these headline examples, RL is increasingly applied in scientific and industrial domains. The course of our study touched on a few, such as:</p> </li> </ul> <p>Controlling nuclear fusion plasmas: Researchers applied deep RL to control the magnetic coils in a tokamak reactor to sustain plasma configurations. This is a complex continuous control problem with safety constraints, where function approximation and careful exploration (largely in simulations before real experiments) were key.</p> <p>Optimizing public health interventions: An RL approach was used to design efficient COVID-19 border testing policies. Framing the problem as a sequential decision task (who to test and when) and using RL to maximize some health outcome or efficiency metric allowed automating policy design that adapted to data.</p> <p>Robotics and Autonomous Systems: Many advances in robotics have come from RL algorithms that allow robots to learn locomotion, manipulation, or flight. Often these use deep RL and sometimes simulation-to-reality transfer. The exploration techniques we learned (like curiosity-driven bonuses or domain randomization) help address the challenge of learning in these complex environments.</p> <p>Recommender Systems and Online Decision Making: Multi-armed bandit algorithms (including Thompson Sampling and UCB) are widely used in industry for things like A/B testing, website optimization, and personalized recommendations. For example, serving personalized content can be seen as a bandit problem where each content choice is an arm and click-through or engagement is the reward. Companies employ bandit algorithms to balance exploration of new content with exploitation of known user preferences, often in a context of contextual bandits (where the state or context is user features). The theoretical guarantees of bandit algorithms give confidence in their performance, and their simplicity makes them practical at scale.</p> <p>In all these cases, the fundamental concepts from this course appear and validate themselves: whether it\u2019s optimism guiding AlphaZero\u2019s search, or Thompson Sampling driving an online recommendation strategy, or policy gradients tuning ChatGPT using human rewards, the same core ideas of reinforcement learning apply. Modern applications often hybridize approaches \u2013 for instance, using model-based simulations (AlphaTensor, AlphaZero), or combining learning from offline data with online exploration (ChatGPT\u2019s RLHF, or robotics). This underscores the importance of mastering the basics: understanding value functions, policy optimization, exploration mechanisms, and theoretical limits has direct relevance even as we push RL into new territory.</p>"},{"location":"reinforcement/14_final/#final-takeaways","title":"Final Takeaways","text":"<p>In closing, we synthesize a few key insights and lessons from the full RL journey:</p> <ul> <li> <p>Reinforcement Learning Unifies Many Themes: We saw that RL problems range from simple bandits to complex high-dimensional control, but they share the need for sequential decision making under uncertainty. Concepts like state, action, reward, policy, value function, model form a common language to describe problems as diverse as games, robotics, and recommendation systems. Recognizing an appropriate RL formulation (MDP, bandit, etc.) for a given real-world problem is the first step to applying these methods.</p> </li> <li> <p>Exploration vs. Exploitation is Fundamental: The trade-off between trying new actions and leveraging known good actions underpins all of RL. We examined different strategies:</p> <ul> <li> <p>Heuristics like \\(\\epsilon\\)-greedy (simple but can be suboptimal),</p> </li> <li> <p>Optimistic algorithms (UCB, optimism in value iteration,  exploration bonuses) which ensure systematic exploration using confidence bounds,</p> </li> <li> <p>Probabilistic approaches (Thompson Sampling, randomized value functions) which inject randomness based on uncertainty.</p> </li> </ul> </li> </ul> <p>Each approach has its advantages \u2013 optimism often yields strong guarantees and is conceptually straightforward, while Thompson Sampling often gives excellent practical performance and naturally incorporates prior knowledge. In large-scale problems, clever exploration bonuses (intrinsic rewards for novelty) and approximate uncertainty estimates are key to maintaining exploration. The central lesson is that successful RL requires deliberate exploration strategies; naive exploration can lead to poor sample efficiency or getting stuck in suboptimal behaviors.</p> <ul> <li> <p>Theoretical Foundations Guide Algorithm Design: Concepts like regret and PAC provide ways to formally measure learning efficiency. They not only help us compare algorithms (e.g. which has lower regret or better sample complexity) but have directly inspired algorithmic techniques (like UCB from the idea of minimizing regret, or PAC-inspired algorithms like MBIE-EB and R-MAX designed to guarantee learning within polynomial time). Meanwhile, the Bayesian perspective offers a gold-standard for optimal decision-making given prior info, even if it\u2019s often computationally intractable \u2013 it guides us toward algorithms that perform well on average and informs approaches like posterior sampling. As RL practitioners, we should remember:</p> <ul> <li> <p>Regret minimization focuses on not wasting too many opportunities \u2013 it\u2019s about learning as fast as possible in an online sense.</p> </li> <li> <p>PAC guarantees focus on bounding the learning time with high confidence \u2013 giving safety that an algorithm won\u2019t do too poorly for too long.</p> </li> <li> <p>Bayesian optimality focuses on using prior knowledge efficiently \u2013 it\u2019s about doing the best given what you (probabilistically) know.</p> </li> </ul> </li> </ul> <p>All three perspectives are important; balancing them or choosing the right one depends on the application (e.g., in a one-off A/B test you might care about regret, in a lifelong robot learning you care about sample efficiency with high probability, and in a personalized system you might incorporate Bayesian priors about users).</p> <ul> <li> <p>Function Approximation and Deep RL Open New Possibilities (and Challenges): The leap from tabular or small-scale problems to real-world complexity required using function approximation (especially deep neural networks). This enabled RL to handle images, continuous states, and enormous state spaces \u2013 as seen in Atari games, Go, and continuous control benchmarks. The success of deep RL (DQN, policy gradient methods, etc.) comes from blending RL algorithms with powerful representation learning. However, it also brought challenges like stability of training, overfitting, exploration in high dimensions, and reproducibility issues. Key techniques to mitigate these include experience replay, target networks, regularization, large-scale parallel training, and reward shaping. The takeaway is that theoretical convergence guarantees often break down with function approximation, so a lot of practical know-how and experimentation is needed. Yet, the core ideas (Bellman equations, policy improvement, etc.) still apply \u2013 just approximate. The field is actively developing better theories for RL with function approximation (e.g. understanding generalization, error propagation) and techniques for more reliable training.</p> </li> <li> <p>Real-World Impact and Ongoing Research: Reinforcement learning has graduated from textbook problems to impacting real-world systems. Its principles have powered superhuman game AIs, improved scientific research (e.g. algorithm discovery, experiment design), enhanced language models, and optimized business decisions. At the same time, truly robust and general-purpose RL is still an open challenge. Issues of stability, efficiency, and safety remain \u2013 for instance:</p> <ul> <li>Developing algorithms that work out-of-the-box with minimal tuning for any problem (robustness).</li> <li>Improving data efficiency so that RL can be applied with limited real-world interactions (e.g., via model-based methods, better exploration, or transfer learning).</li> <li>Integrating learning and planning seamlessly, and handling settings that mix offline data with online exploration.</li> <li>Expanding the RL framework to account for multiple objectives, collaboration or competition (multi-agent RL), and richer feedback modalities beyond scalar rewards.</li> </ul> </li> </ul> <p>These are active research directions. The skills and concepts acquired \u2013 from understanding theoretical bounds to implementing algorithms \u2013 equip us to tackle these frontiers.</p> <p>In summary, the journey from multi-armed bandits to deep reinforcement learning has taught us not only a catalogue of algorithms, but a way of thinking about sequential decision problems. We learned how to measure learning efficiency and why exploration is hard yet critical. We saw simple ideas like optimism and probability matching scale up to complex systems that play Go or converse in English. As you move forward from this textbook, remember the foundational principles: reward is your guide, value estimation is your tool, policy is your output, and exploration is your catalyst. With these in mind, you are well-prepared to both apply RL to challenging problems and to contribute to the advancing frontier of reinforcement learning research.</p>"},{"location":"reinforcement/1_intro/","title":"1. Introduction to Reinforcement Learning","text":""},{"location":"reinforcement/1_intro/#chapter-1-introduction-to-reinforcement-learning","title":"Chapter 1: Introduction to Reinforcement Learning","text":"<p>Reinforcement Learning is a paradigm in machine learning where an agent learns to make sequential decisions through interaction with an environment. Unlike supervised learning, where the agent learns from labeled examples, or unsupervised learning, where it learns patterns from unlabeled data, reinforcement learning is driven by the goal of maximizing cumulative reward through trial and error. The agent is not told which actions to take but must discover them by exploring the consequences of its actions.</p> <p>Sequential decision-making under uncertainty is at the heart of reinforcement learning. The agent must balance exploration and exploitation. Exploration is needed to gather information about the environment, while exploitation uses this information to select actions that appear best.</p> <pre><code> RL vs Supervised Learning (Key Differences)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Supervised Learning:                                \u2502\n       \u2502   \u2013 Learns from labeled examples (input \u2192 target)   \u2502\n       \u2502   \u2013 Feedback is immediate and correct               \u2502\n       \u2502   \u2013 IID data; no sequential dependence              \u2502\n       \u2502                                                     \u2502\n       \u2502 Reinforcement Learning:                             \u2502\n       \u2502   \u2013 Learns from interaction (trial &amp; error)         \u2502\n       \u2502   \u2013 Feedback (reward) may be delayed or sparse      \u2502\n       \u2502   \u2013 Data depends on agent's actions (non-IID)       \u2502\n       \u2502   \u2013 Must balance exploration vs exploitation        \u2502\n       \u2502   \u2013 Must solve temporal credit assignment           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>A key characteristic of reinforcement learning is that the outcome of an action may not be immediately known. Rewards can be delayed, making it hard to determine which past actions are responsible for future outcomes. This challenge is known as temporal credit assignment. Successful reinforcement learning algorithms must learn to attribute long-term consequences to earlier decisions.</p> <p>At each time step, the agent observes some representation of the world, takes an action, and receives a reward. The world then transitions to a new state. This interaction continues over time, forming an experience trajectory:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, \\dots \\] <p>The agent\u2019s goal is to learn a policy, which is a mapping from states to actions, that maximizes the total reward it collects over time.</p> <p>The total future reward is defined through the notion of return. The most common formulation is the discounted return:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>where \\(0 \\le \\gamma \\le 1\\) is called the discount factor. It determines how much the agent values immediate rewards compared to future rewards. A smaller \\(\\gamma\\) encourages short-term decisions, while a larger \\(\\gamma\\) favors long-term planning.</p> <p>Reinforcement learning involves four fundamental challenges:</p> <ol> <li>Optimization: The agent must find an optimal policy that maximizes expected return.</li> <li>Delayed consequences: Actions can affect rewards far into the future, making credit assignment difficult.</li> <li>Exploration: The agent must try actions to learn their consequences, even though some actions may seem suboptimal in the short term.</li> <li>Generalization: The agent must use limited experience to generalize to states it has never seen before.</li> </ol> <p>The main components of a reinforcement learning system are the agent, the environment, actions, states, and rewards. The agent chooses an action based on its current state. The environment responds with the next state and a reward. From this interaction, the agent must infer how to improve its decisions over time.</p> <p>The concept of state is crucial. A state is a summary of information that can influence future outcomes. In theory, a state is Markov if it satisfies:</p> \\[ p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t) \\] <p>where \\(h_t\\) is the full history of past observations, actions, and rewards. This means that the future depends only on the current state, not on the entire past. The Markov property is important because it simplifies the learning problem and allows powerful mathematical tools to be applied.</p> <p>State (environment)  \u2192  Action (agent)  \u2192  Next State (environment)</p> <p>Reinforcement learning is particularly useful in domains where optimal behavior is not easily specified, data is limited or must be collected through interaction, and long-term consequences matter. Examples include robotics, autonomous vehicles, game playing, resource allocation, recommendation systems, and online decision-making.</p>"},{"location":"reinforcement/1_intro/#key-concepts","title":"Key Concepts:","text":""},{"location":"reinforcement/1_intro/#episodic-vs-continuing","title":"Episodic vs Continuing:","text":"<p>Reinforcement learning problems can be episodic or continuing. In episodic tasks, interactions end after a finite number of steps, and the agent resets for a new episode. In continuing tasks, the interactions never formally end, and the agent must learn to behave well indefinitely. In episodic settings, the return is naturally finite. In continuing tasks, discounting or average reward formulations are used to ensure the return is well-defined.</p>"},{"location":"reinforcement/1_intro/#types-of-rl-tasks","title":"Types of RL tasks:","text":"<p>There are several types of learning tasks in RL:</p> <ol> <li>Prediction/Policy Evaluation: Estimating how good a given policy is.</li> <li>Control: Finding an optimal policy that maximizes expected return.</li> <li>Planning: Computing optimal policies using a known model of the environment.</li> </ol>"},{"location":"reinforcement/1_intro/#model-based-vs-model-free","title":"Model based vs Model-free","text":"<p>Reinforcement learning algorithms can be classified into two major categories: model-based and model-free. Model-based methods assume that the transition dynamics and reward function of the environment are known or learned. They use this information for planning. Model-free methods do not assume access to this knowledge and must learn directly from interaction.</p>"},{"location":"reinforcement/1_intro/#on-policy-vs-off-policy","title":"On-policy vs off-policy","text":"<ul> <li> <p>On-policy learning:</p> <ul> <li>Direct experience.</li> <li>Learn to estimate and evaluate a policy from experience obtained from following that policy.</li> </ul> </li> <li> <p>Off-policy Learning</p> <ul> <li>Learn to estimate and evaluate a policy using experience gathered from following a different policy.</li> </ul> </li> </ul>"},{"location":"reinforcement/1_intro/#tabular-vs-function-approximation","title":"Tabular vs Function Approximation","text":"<p>In small environments with a limited number of states and actions, value functions and policies can be represented using tables. This is known as the tabular setting. However, real-world problems often involve very large or continuous state spaces, where it is impossible to maintain a separate entry for every state or action.</p> <p>In such cases, we approximate the value function or policy using a parameterized function, such as a linear model or neural network. This approach is called function approximation. Function approximation enables generalization: knowledge gained from one state can be applied to many similar states, making learning feasible in large or continuous environments.</p>"},{"location":"reinforcement/2_mdp/","title":"2. MDPs & Dynamic Programming","text":""},{"location":"reinforcement/2_mdp/#chapter-2-markov-decision-processes-and-dynamic-programming","title":"Chapter 2: Markov Decision Processes and Dynamic Programming","text":"<p>Reinforcement Learning relies on the mathematical framework of Markov Decision Processes (MDPs) to formalize sequential decision-making under uncertainty. The key idea is that an agent interacts with an environment, making decisions that influence both immediate and future rewards.</p> <p>Reinforcement Learning is about selecting actions over time to maximize long-term reward.</p>"},{"location":"reinforcement/2_mdp/#the-markovian-hierarchy","title":"The Markovian Hierarchy","text":"<p>The RL framework is built upon three foundational models, each adding complexity and agency.</p>"},{"location":"reinforcement/2_mdp/#the-markov-process","title":"The Markov Process","text":"<p>A Markov Process, or Markov Chain, is the simplest model, concerned only with the flow of states. It is defined by the set of States (\\(S\\)) and the Transition Model (\\(P(s' \\mid s)\\)). The defining characteristic is the Markov Property: the next state is independent of the past states, given only the current state.</p> \\[ P(s_{t+1} \\mid s_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t) \\] <p>The future is conditionally independent of the past given the present. Intuition: MPs describe what happens but do not assign any value to these events.</p>"},{"location":"reinforcement/2_mdp/#the-markov-reward-process-mrp","title":"The Markov Reward Process (MRP)","text":"<p>A Markov Reward Process (MRP) extends an MP by adding rewards and discounting. An MRP is a tuple \\((S, P, R, \\gamma)\\) where \\(R(s)\\) is the expected reward for being in state \\(s\\) and \\(\\gamma\\) is the discount factor. The return is:</p> \\[ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots \\] <p>The goal is to compute the value function, which is the expected return starting from a state \\(s\\):</p> \\[ V(s) = \\mathbb{E}[G_t | s_t = s] \\] <p>The value function satisfies the Bellman Expectation Equation:</p> \\[ V(s) = R(s) + \\gamma \\sum_{s'} P(s'|s)V(s') \\] <p>This recursive structure relates the value of a state to the values of its successor states.</p>"},{"location":"reinforcement/2_mdp/#the-markov-decision-process-mdp","title":"The Markov Decision Process (MDP)","text":"<p>An MDP introduces agency. Defined by the tuple \\((S, A, P, R, \\gamma)\\), it extends the MRP by giving the agent a set of Actions (\\(A\\)) to choose from.</p> <ul> <li>Action-Dependent Transition: \\(P(s' \\mid s, a)\\)</li> <li>Action-Dependent Reward: \\(R(s, a)\\)</li> </ul> <p>The agent's strategy is described by a Policy (\\(\\pi(a \\mid s)\\)), the probability of selecting action \\(a\\) in state \\(s\\). A key insight is that fixing any policy \\(\\pi\\) reduces an MDP back into an MRP, allowing all tools developed for MRPs to be applied to the MDP.</p> \\[ R_\\pi(s) = \\sum_a \\pi(a|s) R(s,a) \\] \\[ P_\\pi(s'|s) = \\sum_a \\pi(a|s) P(s'|s,a) \\] <p>Once actions are introduced in an MDP, it becomes useful to evaluate not only how good a state is, but how good a particular action is relative to the policy\u2019s expected behavior. This leads to the advantage function.</p> <p>The state-value function measures how good it is to be in a state: \\(V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\).</p> <p>The action-value function measures how good it is to take action \\(a\\) in state \\(s\\):\\(Q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s,\\; a_t = a]\\)</p> <p>The advantage function compares these two: \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s).\\)</p> <p>\\(V_\\pi(s)\\) is how well the policy performs on average from state \\(s\\).</p> <p>\\(Q_\\pi(s,a)\\) is how well it performs if it specifically takes action \\(a\\).</p> <p>Therefore, the advantage tells us: How much better or worse action \\(a\\) is compared to what the policy would normally do in state \\(s\\).</p>"},{"location":"reinforcement/2_mdp/#value-functions-and-expectation","title":"Value Functions and Expectation","text":"<p>To evaluate a fixed policy \\(\\pi\\), we define two inter-related value functions based on the Bellman Expectation Equations.</p>"},{"location":"reinforcement/2_mdp/#state-value-function-vpis","title":"State Value Function (\\(V^\\pi(s)\\))","text":"<p>\\(V^\\pi(s)\\) quantifies the long-term expected return starting from state \\(s\\) and strictly following policy \\(\\pi\\).  </p> <p>How much total reward should I expect if I start in state s and follow policy \\(\\pi\\): forever?</p>"},{"location":"reinforcement/2_mdp/#state-action-value-function-qpisa","title":"State-Action Value Function (\\(Q^\\pi(s,a)\\))","text":"<p>\\(Q^\\pi(s,a)\\) is a more granular measure, quantifying the expected return if the agent takes action \\(a\\) in state \\(s\\) first, and then follows policy \\(\\pi\\).  </p> <p>Intuition: The \\(Q\\)-function is the value of doing a specific action; the \\(V\\)-function is the value of being in a state (the weighted average of the \\(Q\\)-values offered by the policy \\(\\pi\\) in that state):  </p> <p>The Bellman Expectation Equation for \\(V^\\pi\\) links the value of a state to the values of the actions chosen by \\(\\pi\\) and the resulting future states:  </p>"},{"location":"reinforcement/2_mdp/#optimal-control-finding-pi","title":"Optimal Control: Finding \\(\\pi^*\\)","text":"<p>The ultimate goal of solving an MDP is to find the optimal policy (\\(\\pi^*\\)) that maximizes the expected return from every state \\(s\\).</p> \\[ \\pi^* = \\operatorname*{arg\\,max}_{\\pi} V^\\pi(s) \\quad \\text{for all } s \\in S \\] <p>This optimal policy is characterized by the Optimal Value Functions (\\(V^*\\) and \\(Q^*\\)).</p>"},{"location":"reinforcement/2_mdp/#the-bellman-optimality-equations","title":"The Bellman Optimality Equations","text":"<p>These equations are fundamental, describing the unique value functions that arise when acting optimally. Unlike the expectation equations, they contain a \\(\\max\\) operator, making them non-linear.</p> <ul> <li> <p>Optimal State Value (\\(V^*\\)): The optimal value of a state equals the maximum expected return achievable from any single action \\(a\\) taken from that state:</p> \\[ V^*(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right] \\] </li> <li> <p>Optimal Action-Value (\\(Q^*\\)): The optimal value of taking action \\(a\\) is the immediate reward plus the discounted value of the optimal subsequent actions (\\(\\max_{a'}\\)) in the next state \\(s'\\):</p> \\[ Q^*(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a') \\] </li> </ul> <p>Once \\(Q^*\\) is known, the optimal policy \\(\\pi^*\\) is easily extracted by simply choosing the action that maximizes \\(Q^*(s,a)\\) in every state:  </p> <p>These equations are non-linear due to the max operator and must be solved iteratively.</p>"},{"location":"reinforcement/2_mdp/#dynamic-programming-algorithms","title":"Dynamic Programming Algorithms","text":"<p>For MDPs where the model (\\(P\\) and \\(R\\)) is fully known, Dynamic Programming methods are used to solve the Bellman Optimality Equations iteratively.</p>"},{"location":"reinforcement/2_mdp/#policy-iteration","title":"Policy Iteration","text":"<p>Policy Iteration follows an alternating cycle of Evaluation and Improvement. It takes fewer, but more expensive, iterations to converge.</p> <ol> <li>Policy Evaluation: For the current policy \\(\\pi_k\\), compute \\(V^{\\pi_k}\\) by iteratively applying the Bellman Expectation Equation until full convergence. This is the computationally intensive step.      </li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}\\) by choosing an action that is greedy with respect to the fully converged \\(V^{\\pi_k}\\).      </li> </ol> <p>The process repeats until the policy stabilizes (\\(\\pi_{k+1} = \\pi_k\\)), guaranteeing convergence to \\(\\pi^*\\).</p>"},{"location":"reinforcement/2_mdp/#value-iteration","title":"Value Iteration","text":"<p>Value Iteration is a single, continuous process that combines evaluation and improvement by repeatedly applying the Bellman Optimality Equation. It takes many, but computationally cheap, iterations.</p> <ol> <li>Iterative Update: For every state \\(s\\), update the value function \\(V_k(s)\\) using the \\(\\max\\) operation. This immediately incorporates a greedy improvement step into the value update.      </li> <li>Convergence: The iterations stop when \\(V_{k+1}\\) is sufficiently close to \\(V^*\\).</li> <li>Extraction: The optimal policy \\(\\pi^*\\) is then extracted greedily from the final \\(V^*\\).</li> </ol>"},{"location":"reinforcement/2_mdp/#pi-vs-vi","title":"PI vs VI","text":"Feature Policy Iteration (PI) Value Iteration (VI) Core Idea Evaluate completely, then improve. Greedily improve values in every step. Equation Uses Bellman Expectation (inner loop) Uses Bellman Optimality (max) Convergence Few, large policy steps. Policy guaranteed to stabilize faster. Many, small value steps. Value function converges slowly to \\(V^*\\). Cost High cost per iteration (due to full evaluation). Low cost per iteration (due to one-step backup)."},{"location":"reinforcement/2_mdp/#mdps-mental-map","title":"MDPs Mental Map","text":"<pre><code>                   Markov Decision Processes (MDPs)\n        Formalizing Sequential Decision-Making under Uncertainty\n                                  \u2502\n                                  \u25bc\n                       Progression of Markov Models\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Process (MP): States &amp; Transition Probabilities \u2502\n       \u2502   [S, P(s'|s)] \u2014 No rewards, no decisions               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Reward Process (MRP): MP + Rewards + \u03b3          \u2502\n       \u2502  [S, P(s'|s), R(s), \u03b3]                                  \u2502\n       \u2502    Value Function: V(s) = E[Gt | st = s]                \u2502\n       \u2502     Bellman Expectation Eqn:                            \u2502\n       \u2502     V(s) = R(s) + \u03b3 \u2211 P(s'|s)V(s')                      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Markov Decision Process (MDP): MRP + Actions           \u2502\n       \u2502   [S, A, P(s'|s,a), R(s,a), \u03b3]                          \u2502\n       \u2502    Adds Agency: Agent chooses actions                   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                             Policy \u03c0(a|s)\n                        Agent\u2019s decision strategy\n                                  \u2502\n                                  \u25bc\n                          Value Functions\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 State Value V\u03c0(s): Expected return following \u03c0                 \u2502\n     \u2502 Q\u03c0(s,a): Expected return from (s,a) then follow \u03c0              \u2502\n     \u2502 Relationship: V\u03c0(s) = \u2211 \u03c0(a|s) Q\u03c0(s,a)                         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                    Bellman Expectation Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V\u03c0(s) = \u2211 \u03c0(a|s)[R(s,a) + \u03b3 \u2211 P(s'|s,a)V\u03c0(s')]                 \u2502\n     \u2502 Q\u03c0(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) V\u03c0(s')                        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n               Goal: Find Optimal Policy \u03c0*\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 \u03c0*(s) = argmax\u2090 Q*(s,a)                                     \u2502\n     \u2502 V*(s): Max possible value from state s under the optimal    |\n     |        policy                                               \u2502\n     \u2502 Q*(s,a): Max possible return state s by taking action a     |\n     |          and thereafter following the optimal policy        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                      Bellman Optimality Equations\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 V*(s) = max\u2090 [R(s,a) + \u03b3 \u2211 P(s'|s,a)V*(s')]                 \u2502\n     \u2502 Q*(s,a) = R(s,a) + \u03b3 \u2211 P(s'|s,a) max\u2090' Q*(s',a')            \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                 Solution when Model (P,R) is known:\n                    Dynamic Programming (DP)\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Policy Iteration                              \u2502 Value Iteration - \u2502\n     \u2502 (Alternating Evaluation &amp; Improvement)        \u2502 Single update step\u2502\n     \u2502                                               \u2502 repeatedly        \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy Eval     \u2502                          \u2502 Bellman Optimality      \u2502\n  \u2502 Using V\u03c0 until  \u2502                          \u2502 Update every iteration  \u2502\n  \u2502 convergence     \u2502                          \u2502 V_(k+1) = max_a[....]   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                               \u2502\n          \u25bc                                               \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Policy          \u2502                          \u2502 After convergence:      \u2502\n  \u2502 Improvement:    \u2502                          \u2502 extract \u03c0* from Q*      \u2502\n  \u2502 \u03c0_(k+1)=argmax Q\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n             Outcome: Optimal Policy and Value Functions\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03c0*(s) \u2014 Best action at each state                   \u2502\n       \u2502 V*(s) \u2014 Max return achievable                       \u2502\n       \u2502 Q*(s,a) \u2014 Max return from (s,a)                     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/3_modelfree/","title":"3. Model-Free Prediction","text":""},{"location":"reinforcement/3_modelfree/#chapter-3-model-free-policy-evaluation-learning-the-value-of-a-fixed-policy","title":"Chapter 3: Model-Free Policy Evaluation: Learning the Value of a Fixed Policy","text":"<p>In Dynamic Programming, value functions are computed using a known model of the environment. In reality, however, the model is almost always unknown. This necessitates a shift to Model-Free Reinforcement Learning, where the agent must learn the values of states and actions solely from direct experience (i.e., collecting trajectories of states, actions, and rewards). The goal is to estimate the value function \\(V^\\pi(s)\\) or \\(Q^\\pi(s,a)\\) for a given policy \\(\\pi\\) using data of the form:</p> \\[ s_0, a_0, r_1, s_1, a_1, r_2, s_2, \\dots \\] <p>The true value of a state under policy \\(\\pi\\) is still defined by the expected return:</p> \\[ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | s_t = s] \\] <p>but the agent must approximate this expectation using sampled experience.</p> <p>Model-Free methods can be divided into two main categories based on how they estimate returns:</p> <ol> <li>Monte Carlo (MC) methods: learn from complete episodes by averaging returns.</li> <li>Temporal Difference (TD) methods: learn from incomplete episodes by bootstrapping from existing estimates.</li> </ol>"},{"location":"reinforcement/3_modelfree/#monte-carlo-policy-evaluation","title":"Monte Carlo Policy Evaluation","text":"<p>MC methods are the simplest approach to model-free evaluation. The core idea is that since the true value function \\(V^\\pi(s)\\) is the expected return, we can approximate it by simply averaging the observed returns (\\(G_t\\)) from many episodes that start at state \\(s\\).</p> \\[ V^\\pi(s) \\approx \\text{Average of observed returns } G_t \\text{ starting from } s \\]"},{"location":"reinforcement/3_modelfree/#key-properties-of-mc","title":"Key Properties of MC","text":"<ol> <li>Episodic Requirement: MC can only be applied to episodic MDPs. An episode must terminate (\\(s_T\\)) to calculate the full return \\(G_t\\).</li> <li>Model-Free and Markovian Assumption: MC makes no assumption that the system is Markov in the observable state features. It merely averages the outcome of executing a policy.</li> </ol> <p>We can maintain the value estimates \\(V(s)\\) using counts and sums, or through incremental updates.</p>"},{"location":"reinforcement/3_modelfree/#a-first-visit-vs-every-visit-mc","title":"A. First-Visit vs. Every-Visit MC","text":"<p>When computing the return \\(G_t\\) for a state \\(s\\) in a single trajectory, a state might be visited multiple times.</p> <ul> <li>First-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) only the first time state \\(s\\) is visited in an episode.<ul> <li>Properties: First-Visit MC is an unbiased estimator of \\(V^\\pi(s)\\). It is also consistent (converges to the true value as data \\(\\rightarrow \\infty\\)) by the Law of Large Numbers.</li> </ul> </li> <li>Every-Visit MC: The return \\(G_t\\) is used to update \\(V(s)\\) every time state \\(s\\) is visited in an episode.<ul> <li>Properties: Every-Visit MC is a biased estimator because multiple updates within the same episode are correlated. However, it is also consistent and often exhibits better Mean Squared Error (MSE) due to utilizing more data.</li> </ul> </li> </ul>"},{"location":"reinforcement/3_modelfree/#b-incremental-monte-carlo","title":"B. Incremental Monte Carlo","text":"<p>For computational efficiency and to avoid storing all returns, MC updates can be performed incrementally using a running average. This looks like a standard learning update:</p> \\[ V(s) \\leftarrow V(s) + \\alpha \\left[ G_t - V(s) \\right] \\] <p>Where:</p> <ul> <li>\\(G_t\\): The actual observed return (our target).</li> <li>\\(V(s)\\): Our current estimate (our old value).</li> <li>\\(\\alpha\\): The learning rate (\\(\\alpha \\in (0, 1]\\)), which can be fixed or decayed.</li> </ul> <p>Consistency Guarantee: For incremental MC to guarantee convergence to the True Value (\\(V^\\pi\\)), the learning rate \\(\\alpha_t\\) (which may be \\(1/N(s)\\) or a fixed constant) must satisfy the following conditions:</p> <ol> <li>The sum of all learning rates for state \\(s\\) must diverge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s) = \\infty\\)</li> <li>The sum of the squared learning rates must converge: \\(\\sum_{t=1}^{\\infty} \\alpha_t(s)^2 &lt; \\infty\\)</li> </ol>"},{"location":"reinforcement/3_modelfree/#temporal-difference-td-learning","title":"Temporal Difference (TD) Learning","text":"<p>While MC uses the full return \\(G_t\\), TD learning is the fundamental shift in policy evaluation. It retains the concept of the incremental update but changes the target, introducing a technique called bootstrapping.</p>"},{"location":"reinforcement/3_modelfree/#bootstrapping-the-core-idea","title":"Bootstrapping: The Core Idea","text":"<p>Bootstrapping means updating a value estimate using another value estimate. In the context of Policy Evaluation, TD methods use the estimated value of the next state, \\(V(s_{t+1})\\), to update the value of the current state, \\(V(s_t)\\). The standard TD algorithm is TD(0) (or one-step TD).</p>"},{"location":"reinforcement/3_modelfree/#the-td0-update-rule","title":"The TD(0) Update Rule","text":"<p>The TD(0) update replaces the full return \\(G_t\\) with the TD Target (\\(r_t + \\gamma V(s_{t+1})\\)):</p> \\[ V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ \\underbrace{r_{t+1} + \\gamma V(s_{t+1})}_{\\text{TD Target}} - V(s_t) \\right] \\] <p>The term inside the brackets is the TD Error (\\(\\delta_t\\)):  This error is the difference between the estimated value of the current state and a better, bootstrapped estimate of that value.</p>"},{"location":"reinforcement/3_modelfree/#td-vs-monte-carlo","title":"TD vs. Monte Carlo","text":"<p>The distinction between TD and MC centers on what is used as the target value:</p> Feature Monte Carlo (MC) Temporal Difference (TD) Target \\(G_t\\) (Full observed return to episode end) \\(r_{t+1} + \\gamma V(s_{t+1})\\) (One-step return + estimated future value) Bootstrapping No (waits until episode end) Yes (uses \\(V(s_{t+1})\\)) Bias Unbiased (First-Visit MC) Biased (because \\(V(s_{t+1})\\) is an estimate) Variance High Variance (Return \\(G_t\\) is a sum of many random steps) Low Variance (TD target depends on only one random reward/next state) Convergence Consistent (converges to true \\(V^\\pi\\)) TD(0) converges to true \\(V^\\pi\\) in the tabular case <p>TD methods generally have a desirable trade-off, accepting a small bias in exchange for significantly lower variance. This often makes them more computationally and statistically efficient in practice. TD(0) is applicable to non-episodic (continuing) tasks, overcoming one of the major limitations of Monte Carlo.</p>"},{"location":"reinforcement/3_modelfree/#example-setup","title":"Example Setup","text":""},{"location":"reinforcement/3_modelfree/#parameters","title":"Parameters","text":"<ul> <li>States (\\(S\\)): \\(s_A, s_B, s_C\\)</li> <li>Discount Factor (\\(\\gamma\\)): \\(0.9\\)</li> <li>Learning Rate (\\(\\alpha\\)): \\(0.5\\) (Used for TD updates)</li> <li>Initial Value Estimates (\\(V_0\\)): \\(V(s_A)=0, V(s_B)=0, V(s_C)=0\\)</li> </ul>"},{"location":"reinforcement/3_modelfree/#episodes-and-returns","title":"Episodes and Returns","text":"<p>The full return (\\(G_t\\)) is calculated for every visit in every episode:</p> Episode (E) Trajectory (State \\(\\xrightarrow{r}\\) Next State) Visit Time (\\(t\\)) State (\\(s_t\\)) Full Return (\\(G_t\\)) E1 \\(s_A \\xrightarrow{r=1} s_B \\xrightarrow{r=0} s_C \\xrightarrow{r=5} s_B \\xrightarrow{r=2} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{6.508}\\) 1 \\(s_B\\) (1st) \\(\\mathbf{6.12}\\) 2 \\(s_C\\) \\(\\mathbf{6.8}\\) 3 \\(s_B\\) (2nd) \\(\\mathbf{2.0}\\) E2 \\(s_A \\xrightarrow{r=-2} s_C \\xrightarrow{r=8} \\text{T}\\) 0 \\(s_A\\) \\(\\mathbf{5.2}\\) 1 \\(s_C\\) \\(\\mathbf{8.0}\\) E3 \\(s_B \\xrightarrow{r=10} s_C \\xrightarrow{r=-5} s_B \\xrightarrow{r=1} \\text{T}\\) 0 \\(s_B\\) (1st) \\(\\mathbf{6.31}\\) 1 \\(s_C\\) \\(\\mathbf{-4.1}\\) 2 \\(s_B\\) (2nd) \\(\\mathbf{1.0}\\)"},{"location":"reinforcement/3_modelfree/#1-first-visit-monte-carlo-mc","title":"1. First-Visit Monte Carlo (MC)","text":"<p>Rule: Only the first return for a state in any given episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508\\) (E1), \\(5.2\\) (E2) \\(11.708\\) 2 \\(s_B\\) \\(6.12\\) (E1), \\(6.31\\) (E3) \\(12.43\\) 2 \\(s_C\\) \\(6.8\\) (E1), \\(8.0\\) (E2), \\(-4.1\\) (E3) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":""},{"location":"reinforcement/3_modelfree/#2-every-visit-monte-carlo-mc","title":"2. Every-Visit Monte Carlo (MC)","text":"<p>Rule: The return from every time a state is encountered in any episode is used.</p>"},{"location":"reinforcement/3_modelfree/#a-data-selection-and-counts-ns_1","title":"A. Data Selection and Counts (\\(N(s)\\))","text":"State (\\(s\\)) Returns Used (\\(G_t\\)) Total Sum (\\(\\sum G_t\\)) Count (\\(N(s)\\)) \\(s_A\\) \\(6.508, 5.2\\) \\(11.708\\) 2 \\(s_B\\) \\(6.12, 2.0, 6.31, 1.0\\) \\(15.43\\) 4 \\(s_C\\) \\(6.8, 8.0, -4.1\\) \\(10.7\\) 3"},{"location":"reinforcement/3_modelfree/#b-final-estimates-vs-sum-g_t-ns_1","title":"B. Final Estimates (\\(V(s) = \\sum G_t / N(s)\\))","text":"\\[ V(s_A) = \\frac{11.708}{2} = \\mathbf{5.854} \\\\ V(s_B) = \\frac{15.43}{4} = \\mathbf{3.858} \\\\ V(s_C) = \\frac{10.7}{3} = \\mathbf{3.567} \\]"},{"location":"reinforcement/3_modelfree/#3-temporal-difference-td0","title":"3. Temporal Difference (TD(0))","text":"<p>Rule: The value is updated after every step using the TD Target (\\(r_{t+1} + \\gamma V(s_{t+1})\\)) and the learning rate \\(\\alpha\\). The updated \\(V(s)\\) estimates are carried over to the next step and episode.</p>"},{"location":"reinforcement/3_modelfree/#a-step-by-step-td-calculation-summary","title":"A. Step-by-Step TD Calculation Summary","text":"Step Transition Old \\(V(s_t)\\) New \\(V(s_t)\\) \\(V(s_A)\\) \\(V(s_B)\\) \\(V(s_C)\\) E1-1 \\(s_A \\xrightarrow{r=1} s_B\\) 0 0.500 0.500 0.000 0.000 E1-2 \\(s_B \\xrightarrow{r=0} s_C\\) 0 0.000 0.500 0.000 0.000 E1-3 \\(s_C \\xrightarrow{r=5} s_B\\) 0 2.500 0.500 0.000 2.500 E1-4 \\(s_B \\xrightarrow{r=2} \\text{T}\\) 0.0 1.000 0.500 1.000 2.500 E2-1 \\(s_A \\xrightarrow{r=-2} s_C\\) 0.500 0.375 0.375 1.000 2.500 E2-2 \\(s_C \\xrightarrow{r=8} \\text{T}\\) 2.500 5.250 0.375 1.000 5.250 E3-1 \\(s_B \\xrightarrow{r=10} s_C\\) 1.000 7.863 0.375 7.863 5.250 E3-2 \\(s_C \\xrightarrow{r=-5} s_B\\) 5.250 3.663 0.375 7.863 3.663 E3-3 \\(s_B \\xrightarrow{r=1} \\text{T}\\) 7.863 4.431 0.375 4.431 3.663"},{"location":"reinforcement/3_modelfree/#b-final-estimates-v_td0","title":"B. Final Estimates (\\(V_{TD(0)}\\))","text":"\\[ V(s_A) = \\mathbf{0.375} \\\\ V(s_B) = \\mathbf{4.431} \\\\ V(s_C) = \\mathbf{3.663} \\]"},{"location":"reinforcement/3_modelfree/#comparison-of-results","title":"Comparison of Results","text":"State First-Visit MC Every-Visit MC TD(0) (\\(\\alpha=0.5, \\gamma=0.9\\)) Note \\(s_A\\) 5.854 5.854 0.375 TD heavily penalized \\(s_A\\) in E2 (Target 0.25), while MC averaged the full observed high returns. \\(s_B\\) 6.215 3.858 4.431 TD's result falls between the two MC methods, demonstrating a quicker convergence due to bootstrapping. \\(s_C\\) 3.567 3.567 3.663 All methods are close for \\(s_C\\). <p>This comparison illustrates the bias-variance trade-off: * MC uses the sample return (\\(G_t\\)), which has high variance but is an unbiased target (First-Visit). * TD uses a bootstrapped estimate (\\(r + \\gamma V(s')\\)), which has lower variance but introduces bias by relying on an estimated successor value.</p>"},{"location":"reinforcement/3_modelfree/#model-free-prediction-mental-map","title":"Model Free Prediction Mental Map","text":"<pre><code>            Model-Free Prediction\n     (Policy Evaluation without Model P or R)\n                        \u2502\n                        \u25bc\n                Goal: Estimate\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 State Value: V\u03c0(s)                \u2502\n       \u2502 Action Value: Q\u03c0(s,a)             \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n              Using Sampled Experience\n        (s\u2080,a\u2080,r\u2081,s\u2081,a\u2081,r\u2082,... from \u03c0)\n                        \u2502\n                        \u25bc\n            Two Families of Methods\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Monte Carlo (MC)              \u2502 Temporal Difference (TD)      \u2502\n    \u2502 \"Learn from full episodes\"    \u2502 \"Learn step-by-step\"          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u25bc                           \u25bc\n             Monte Carlo (MC)              Temporal Difference (TD)\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502 Needs full episodes     \u2502       \u2502Works on incomplete episodes\u2502\n      \u2502 No bootstrapping        \u2502       \u2502Uses bootstrapping          \u2502\n      \u2502 High variance           \u2502       \u2502Low variance                \u2502\n      \u2502 Unbiased (first visit)  \u2502       \u2502Biased                      \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n        \u2502                               \u2502           \u2502\n        \u25bc                               \u25bc           \u25bc\n First-Visit MC                  Every-Visit MC     TD(0) Update Rule\n (One update per episode          (Multiple updates \u2502 V(s) \u2190 V(s) +\n  per state)                      per episode)      \u2502 \u03b1[ r + \u03b3V(s') \u2212 V(s) ]\n                        \u2502                           \u2502\n                        \u2502                           \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                        Comparison (Bias\u2013Variance)\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 MC: Unbiased, High variance             \u2502\n               \u2502 TD: Biased, Lower variance              \u2502\n               \u2502 MC: Not bootstrapping                   \u2502\n               \u2502 TD: Bootstraps using V(s\u2019)              \u2502\n               \u2502 MC: Episodic only                       \u2502\n               \u2502 TD: Works for continuing tasks          \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n                      Outcome: Learned Value Function\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 V\u03c0(s) or Q\u03c0(s,a) from real experience \u2502\n               \u2502 (No model of environment required)    \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/4_model_free_control/","title":"4. Model-Free Control","text":""},{"location":"reinforcement/4_model_free_control/#chapter-4-model-free-control-learning-optimal-behavior-without-a-model","title":"Chapter 4: Model-Free Control: Learning Optimal Behavior Without a Model","text":"<p>In Chapter 3, we learned how to estimate the value of a fixed policy using Monte Carlo and Temporal Difference methods, but we did not address how to improve that policy. The goal of Model-Free Control is to discover the optimal policy \\(\\pi^*\\) without knowing the transition probabilities or reward function. To achieve this, we must learn not only to evaluate a policy, but also to improve it through interaction with the environment.</p>"},{"location":"reinforcement/4_model_free_control/#from-state-values-to-action-values","title":"From State Values to Action Values","text":"<p>In model-based methods like Dynamic Programming, policy improvement depends on knowing the environment model. To improve a policy, we use the Bellman optimality equation:</p> <p>  This update requires two things:</p> <ul> <li>the transition probabilities \\(P(s'|s,a)\\)</li> <li>the expected reward R(s,a)$</li> </ul> <p>If either of these is unknown, we cannot compute the right-hand side, so model-based policy improvement becomes impossible.</p> <p>Instead of learning the state-value function \\(V^\\pi(s)\\) and using the model to evaluate the effect of each action, model-free RL learns the value of actions themselves.  </p> <p>The Model-Free Policy Iteration loop:</p> <ol> <li>Policy Evaluation: Compute \\(Q^{\\pi}\\) from experience.</li> <li>Policy Improvement: Update the policy \\(\\pi\\) given the estimated \\(Q^{\\pi}\\).</li> </ol> <p>However, using a purely greedy policy creates a new problem: the agent will only experience actions it already believes are good, and may never discover better ones. This introduces the fundamental challenge of exploration.</p>"},{"location":"reinforcement/4_model_free_control/#exploration-vs-exploitation","title":"Exploration vs. Exploitation","text":"<p>To learn optimal behavior, the agent must balance two goals:</p> <ol> <li>Exploitation: choose actions believed to yield high rewards.</li> <li>Exploration: try actions whose consequences are uncertain or poorly understood.</li> </ol> <p>A common solution is the \\(\\epsilon\\)-greedy policy:</p> <p>With probability \\(1 - \\epsilon\\), choose the action with the highest estimated value. With probability \\(\\epsilon\\), choose a random action.</p> <p>Formally:</p> \\[ \\pi(a|s) = \\begin{cases} 1 - \\epsilon + \\frac{\\epsilon}{|A|} &amp; \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\ \\frac{\\epsilon}{|A|} &amp; \\text{otherwise} \\end{cases} \\] <p>This approach ensures that the agent both explores and exploits, learning from a wide range of actions while gradually improving its policy.</p>"},{"location":"reinforcement/4_model_free_control/#monte-carlo-control","title":"Monte Carlo Control","text":"<p>Monte Carlo Control extends the Monte Carlo methods from Chapter 3 to action-value learning. Instead of estimating \\(V(s)\\), it estimates \\(Q(s,a)\\) using sampled returns.</p> <p>Monte Carlo Policy Evaluation, Now for Q:       1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  \\(k=1\\),  Input \\(\\epsilon=1\\), \\(\\pi\\) 2: loop over epiosdes      3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 4: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 5: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 6: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 7: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 8: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 9: \\(\\quad\\quad\\)       end if 10: \\(\\quad\\)  end for 11: \\(\\quad\\) \\(k = k + 1\\) 12: end loop</p> <p>The simplest approach is On-Policy MC Control (also known as MC Exploring Starts), which follows the generalized policy iteration structure using \\(\\epsilon\\)-greedy policies for exploration.</p> <ul> <li>Policy Evaluation: \\(Q(s, a)\\) is updated using the full return (\\(G_t\\)) observed after the state-action pair \\((s_t, a_t)\\) has occurred in an episode. The incremental update uses the formula \\(Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\frac{1}{N(s,a)}(G_{t} - Q(s_t, a_t))\\). </li> <li>Policy Improvement: The new policy \\(\\pi_{k+1}\\) is set to be \\(\\epsilon\\)-greedy with respect to the updated \\(Q\\) function.</li> </ul>"},{"location":"reinforcement/4_model_free_control/#greedy-in-the-limit-of-infinite-exploration-glie","title":"Greedy in the Limit of Infinite Exploration (GLIE)","text":"<p>For Monte Carlo Control to converge to the optimal action-value function \\(Q^*(s, a)\\), the process must satisfy the Greedy in the Limit of Infinite Exploration (GLIE) conditions:</p> <ol> <li>Infinite Visits: All state-action pairs \\((s, a)\\) must be visited an infinite number of times (\\(\\lim_{i \\rightarrow \\infty} N_i(s, a) \\rightarrow \\infty\\)).</li> <li>Converging Greed: The behavior policy (the policy used to act and generate data) must eventually converge to a greedy policy.</li> </ol> <p>A simple strategy to satisfy GLIE is to use an \\(\\epsilon\\)-greedy policy where \\(\\epsilon\\) is decayed over time, such as \\(\\epsilon_i = 1/i\\) (where \\(i\\) is the episode number). Under the GLIE conditions, Monte-Carlo control converges to the optimal state-action value function \\(Q^*(s, a)\\).</p> <p>Monte Carlo Online Control/On Policy Improvement:    </p> <p>1: Initialize \\(Q(s,a)=0\\), \\(N(s,a)=0\\) \\(\\forall(s,a)\\),  Set \\(k=1\\), \\(\\epsilon=1\\).    2: \\(\\pi_k = \\epsilon - greedy (Q)\\) // Create initial \\(\\epsilon\\) - greedy policy. 3: loop over epiosdes    4: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,T})\\) given \\(\\pi\\) 5: \\(\\quad\\) Compute \\(G_{k,t} = r_{k,t} + \\gamma r_{k,t+1} + \\gamma^2 r_{k,t+2} + \\dots + \\gamma^{T-t-1} r_{k,T}\\) \\(\\forall t\\) 6: \\(\\quad\\)   for \\(t = 1, \\dots, T\\) do 7: \\(\\quad\\quad\\)      if First visit to \\((s,a)\\) in episode \\(k\\) then 8: \\(\\quad\\quad\\quad\\) \\(N(s,a) = N(s,a) + 1\\) 9: \\(\\quad\\quad\\quad\\) \\(Q(s_t,a_t) = Q(s_t,a_t) + \\dfrac{1}{N(s,a)}(G_{k,t} - Q(s_t,a_t))\\) 10: \\(\\quad\\quad\\)       end if  11: \\(\\quad\\)  end for   12:  \\(\\quad\\) \\(\\pi_k = \\epsilon - greedy (Q)\\) //Policy improvement 12: \\(\\quad\\) \\(k = k + 1\\) , \\(\\epsilon = \\frac{1}{k}\\)  13: end loop    </p> <p>This process gradually adjusts the policy and the value estimates until they converge.</p>"},{"location":"reinforcement/4_model_free_control/#iv-temporal-difference-td-control","title":"IV. Temporal Difference (TD) Control","text":"<p>TD control methods improve upon Monte Carlo control by updating action-value estimates after every step rather than at the end of an episode. They are more data-efficient and work in both episodic and continuing tasks.</p>"},{"location":"reinforcement/4_model_free_control/#on-policy-td-control-sarsa","title":"On-Policy TD Control: SARSA","text":"<p>SARSA is an on-policy TD control algorithm. It learns the value of the policy currently being followed (\\(\\pi\\)). Its name is derived from the sequence of steps used in its update rule: State, Action, Reward, State, Action.</p> <p>The update for the action-value \\(Q(s_t, a_t)\\) uses the value of the next state-action pair, \\((s_{t+1}, a_{t+1})\\), selected by the current policy \\(\\pi\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>The TD Target here is \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\). SARSA learns \\(Q^{\\pi}\\) while \\(\\pi\\) is improved greedily with respect to \\(Q^{\\pi}\\), allowing it to find the optimal policy \\(\\pi^*\\).</p> <p>1: Set initial \\(\\epsilon\\)-greedy policy \\(\\pi\\) randomly, \\(t=0\\), initial state \\(s_t=s_0\\)  2: Take \\(a_t \\sim \\pi(s_t)\\)  3: Observe \\((r_t, s_{t+1})\\)  4: loop        5: \\(\\quad\\) Take action \\(a_{t+1} \\sim \\pi(s_{t+1})\\) // Sample action from policy         6: \\(\\quad\\) Observe \\((r_{t+1}, s_{t+2})\\)  7: \\(\\quad\\) Update \\(Q\\) given \\((s_t, a_t, r_t, s_{t+1}, a_{t+1})\\):     8: \\(\\quad\\) Perform policy improvement: The policy is updated every step, making it more greedy according to new Q-values.</p> \\[\\forall s \\in S,\\;\\; \\pi(s) = \\begin{cases} \\arg\\max\\limits_a Q(s,a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{a random action}   &amp; \\text{with probability } \\epsilon \\end{cases}\\] <p>9: \\(\\quad\\) \\(t = t + 1\\) , \\(\\epsilon = \\frac{1}{t}\\)  10: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#b-off-policy-td-control-q-learning","title":"B. Off-Policy TD Control: Q-Learning","text":"<p>Q-Learning is the most widely known off-policy TD control algorithm. Off-policy learning means we estimate and evaluate an optimal policy (\\(\\pi^*\\), the target policy) using experience gathered by a different behavior policy (\\(\\pi_b\\)).</p> <p>In Q-Learning, the agent acts using a soft, exploratory \\(\\pi_b\\) (like \\(\\epsilon\\)-greedy) but the value function update is based on the best possible action from the next state, effectively estimating \\(Q^*\\).</p> \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right] \\] <p>The key difference is the target: Q-Learning uses the value of the max action (\\(\\max_{a'} Q(s_{t+1}, a')\\)), regardless of what action was actually taken in the next step. This makes it a greedy update towards \\(Q^*\\).</p> <p>Q-Learning (Off-Policy TD Control):</p> <p>1: Initialize \\(Q(s,a)=0 \\quad \\forall s \\in S, a \\in A\\), set \\(t = 0\\), initial state \\(s_t = s_0\\)  2: Set \\(\\pi_b\\) to be \\(\\epsilon\\)-greedy w.r.t. \\(Q\\)  3: loop    4: \\(\\quad\\) Take \\(a_t \\sim \\pi_b(s_t)\\) // Sample action from behavior policy    5: \\(\\quad\\) Observe \\((r_t, s_{t+1})\\)  6: \\(\\quad\\) \\(Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\\)  7: \\(\\quad\\) \\(\\pi(s_t) = \\begin{cases} \\arg\\max\\limits_a Q(s_t,a) &amp; \\text{with probability } 1 - \\epsilon \\ \\text{a random action} &amp; \\text{with probability } \\epsilon \\end{cases}\\)  8: \\(\\quad\\) \\(t = t + 1\\)  9: end loop     </p>"},{"location":"reinforcement/4_model_free_control/#value-function-approximation-vfa","title":"Value Function Approximation (VFA)","text":"<p>All methods discussed so far assume a tabular representation, where a separate entry for \\(Q(s, a)\\) is stored for every state-action pair. This is only feasible for MDPs with small, discrete state and action spaces.</p>"},{"location":"reinforcement/4_model_free_control/#motivation-for-approximation","title":"Motivation for Approximation","text":"<p>For environments with large or continuous state/action spaces (e.g., in robotics or image-based games like Atari), we face three critical issues:</p> <ol> <li>Memory: Explicitly storing every \\(V\\) or \\(Q\\) value is impossible.</li> <li>Computation: Computing or updating every value is too slow.</li> <li>Experience: It would take vast amounts of data to visit and learn every single state-action pair.</li> </ol> <p>Value Function Approximation addresses this by using a parameterized function (like a linear model or a neural network) to estimate the value function: \\(\\hat{Q}(s, a; \\mathbf{w}) \\approx Q(s, a)\\). The goal shifts from filling a table to finding the parameter vector \\(\\mathbf{w}\\) that minimizes the error between the true value and the estimate.</p> \\[ J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\left( Q^{\\pi}(s, a) - \\hat{Q}(s, a; \\mathbf{w}) \\right)^2 \\right] \\] <p>The parameter vector \\(\\mathbf{w}\\) is typically updated using Stochastic Gradient Descent (SGD), which uses a single sample to approximate the gradient of the loss function \\(J(\\mathbf{w})\\).</p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-with-vfa-policy-evaluation","title":"Model-Free Control with VFA Policy Evaluation","text":"<p>When using function approximation, we substitute the old \\(Q(s, a)\\) in the update rules (MC, SARSA, Q-Learning) with the function approximator \\(\\hat{Q}(s, a; \\mathbf{w})\\).</p> <ul> <li> <p>MC VFA for Policy Evaluation: </p> <p>The return \\(G_t\\) is used as the target in an SGD update: \\(\\Delta \\mathbf{w} \\propto \\alpha (G_t - \\hat{Q}(s_t, a_t; \\mathbf{w})) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t, a_t; \\mathbf{w})\\).</p> <p>1: Initialize \\(\\mathbf{w}\\), set \\(k = 1\\)  2: loop    3: \\(\\quad\\) Sample k-th episode \\((s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, \\dots, s_{k,L_k})\\) given \\(\\pi\\)  4: \\(\\quad\\) for \\(t = 1, \\dots, L_k\\) do      5: \\(\\quad\\quad\\) if First visit to \\((s,a)\\) in episode \\(k\\) then      6: \\(\\quad\\quad\\quad\\) \\(G_t(s,a) = \\sum_{j=t}^{L_k} r_{k,j}\\)  7: \\(\\quad\\quad\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 \\left[ G_t(s,a) - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w})\\) // Compute Gradient    8: \\(\\quad\\quad\\quad\\) Update weights: \\(\\Delta \\mathbf{w}\\)  9: \\(\\quad\\quad\\) end if 10: \\(\\quad\\) end for  11: \\(\\quad\\) \\(k = k + 1\\)  12: end loop    </p> </li> <li> <p>SARSA with VFA: The TD target is \\(r + \\gamma \\hat{Q}(s', a'; \\mathbf{w})\\), leveraging the current function approximation.</p> <p>1: Initialize \\(\\mathbf{w}\\), \\(s\\)  2: loop    3: \\(\\quad\\) Given \\(s\\), sample \\(a \\sim \\pi(s)\\), observe \\(r(s,a)\\), and \\(s' \\sim p(s'|s,a)\\)  4: \\(\\quad\\) \\(\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2 [r + \\gamma \\hat{V}(s';\\mathbf{w}) - \\hat{V}(s;\\mathbf{w})] \\nabla_{\\mathbf{w}} \\hat{V}(s;\\mathbf{w})\\)  5: \\(\\quad\\) Update weights \\(\\Delta \\mathbf{w}\\)  6: \\(\\quad\\) if \\(s'\\) is not a terminal state then    7: \\(\\quad\\quad\\) Set \\(s = s'\\)  8: \\(\\quad\\) else        9: \\(\\quad\\quad\\) Restart episode, sample initial state \\(s\\)  10: \\(\\quad\\) end if     11: end loop       * Q-Learning with VFA: The TD target is \\(r + \\gamma \\max_{a'} \\hat{Q}(s', a'; \\mathbf{w})\\).</p> </li> </ul>"},{"location":"reinforcement/4_model_free_control/#control-using-vfa","title":"Control using VFA","text":"<p>So far, we have used function approximation mainly for policy evaluation. However, the true goal of reinforcement learning is control, which means learning policies that maximize expected return. In control, the policy itself is continually improved based on the estimated action-value function. When we replace the tabular \\(Q(s,a)\\) with a function approximator \\(\\hat{Q}(s,a;\\mathbf{w})\\), we obtain Model-Free Control with Function Approximation, where both learning and acting are driven by \\(\\hat{Q}(s,a;\\mathbf{w})\\).</p> <p>Value Function Approximation is especially useful for control because it enables generalization across states, allowing the agent to learn effective behavior even in large or continuous state spaces. Instead of storing separate values for each \\((s,a)\\), the agent learns a parameter vector \\(\\mathbf{w}\\) that works across many states and actions. The objective is to make the approximation close to the true optimal action-value function \\(Q^*(s,a)\\).</p> <p>The learning problem becomes:</p> \\[ \\min_{\\mathbf{w}} \\; J(\\mathbf{w}) = \\mathbb{E} \\left[ \\left( Q^*(s,a) - \\hat{Q}(s,a;\\mathbf{w}) \\right)^2 \\right] \\] <p>Using stochastic gradient descent, we update the weights in the direction that reduces approximation error:</p> \\[ \\Delta \\mathbf{w} \\propto \\left( \\text{target} - \\hat{Q}(s_t,a_t;\\mathbf{w}) \\right) \\nabla_{\\mathbf{w}} \\hat{Q}(s_t,a_t;\\mathbf{w}) \\] <p>The most important difference in control is how we choose the target, which depends on the RL method being used:</p> Method Target for updating \\(\\mathbf{w}\\) Monte Carlo \\(G_t\\) SARSA \\(r + \\gamma \\hat{Q}(s',a';\\mathbf{w})\\) Q-Learning \\(r + \\gamma \\max_{a'} \\hat{Q}(s',a';\\mathbf{w})\\) <p>These methods now operate in the same way as before, except instead of updating a single \\(Q(s,a)\\) entry, we update the weights of the approximator. The update generalizes beyond the visited state, helping the agent learn faster in high-dimensional spaces.</p>"},{"location":"reinforcement/4_model_free_control/#challenges-the-deadly-triad","title":"Challenges: The Deadly Triad","text":"<p>When using function approximation for control, learning can become unstable or even diverge. Instability usually arises when these three components occur together:</p> \\[ \\text{Function Approximation} \\;+\\; \\text{Bootstrapping} \\;+\\; \\text{Off-policy Learning} \\] <p>This combination is known as the Deadly Triad .</p> <ul> <li>Function Approximation : Generalizes across states but may introduce bias.</li> <li>Bootstrapping : Uses existing estimates to update current estimates (as in TD methods).</li> <li>Off-policy Learning : Learning from a different behavior policy than the target policy.</li> </ul> <p>Q-Learning with neural networks (as in Deep Q-Learning) contains all three components, making it powerful but potentially unstable without stabilization techniques like  experience replay  and  target networks . Monte Carlo with function approximation is typically more stable because it does not use bootstrapping.</p> <p>Function approximation enables reinforcement learning to scale to complex environments, but it introduces new challenges in stability and convergence. The next step is to address how these ideas lead to  Deep Q-Learning (DQN) , which successfully applies neural networks to approximate \\(Q(s,a)\\).</p>"},{"location":"reinforcement/4_model_free_control/#deep-q-networks-dqn","title":"Deep Q-Networks (DQN)","text":"<p>The most prominent example of VFA for control is Deep Q-Learning, or Deep Q-Networks (DQN), where the action-value function \\(\\hat{Q}(s, a; \\mathbf{w})\\) is approximated by a deep neural network. DQN successfully solved control problems directly from raw sensory input (e.g., pixels from Atari games).</p> <p>DQN stabilizes the non-linear learning process using two critical techniques:</p> <ol> <li> <p>Experience Replay (ER): Transitions \\((s_t, a_t, r_t, s_{t+1})\\) are stored in a replay buffer (\\(\\mathcal{D}\\)). Instead of learning from sequential, correlated experiences, the algorithm samples a random mini-batch of past transitions from \\(\\mathcal{D}\\) for the update. This breaks correlations, making the data samples closer to i.i.d (independent and identically distributed).</p> </li> <li> <p>Fixed Q-Targets: The Q-Learning update requires a target value \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w})\\). To prevent the estimate \\(\\hat{Q}(s, a; \\mathbf{w})\\) from chasing its own rapidly changing target, the parameters \\(\\mathbf{w}^{-}\\) used to compute the target are fixed for a period of time, then synchronized with the current parameters \\(\\mathbf{w}\\). This provides a stable target \\(y_i = r_i + \\gamma \\max_{a'} \\hat{Q}(s_{i+1}, a'; \\mathbf{w}^{-})\\).</p> </li> </ol> <p>Deep Q-Network (DQN) Algorithm:</p> <p>1: Input \\(C\\), \\(\\alpha\\), \\(D = {}\\), Initialize \\(\\mathbf{w}\\), \\(\\mathbf{w}^- = \\mathbf{w}\\), \\(t = 0\\)  2: Get initial state \\(s_0\\)  3: loop        4: \\(\\quad\\) Sample action \\(a_t\\) using \\(\\epsilon\\)-greedy policy w.r.t. current \\(\\hat{Q}(s_t, a; \\mathbf{w})\\)  5: \\(\\quad\\) Observe reward \\(r_t\\) and next state \\(s_{t+1}\\)  6: \\(\\quad\\) Store transition \\((s_t, a_t, r_t, s_{t+1})\\) in replay buffer \\(D\\)  7: \\(\\quad\\) Sample a random minibatch of tuples \\((s_i, a_i, r_i, s'i)\\) from \\(D\\)  8: \\(\\quad\\) for \\(j\\) in minibatch do     9: \\(\\quad\\quad\\) if episode terminates at step \\(i+1\\) then       10: \\(\\quad\\quad\\quad\\) \\(y_i = r_i\\)  11: \\(\\quad\\quad\\) else      12: \\(\\quad\\quad\\quad\\) \\(y_i = r_i + \\gamma \\max\\limits{a'} \\hat{Q}(s'i, a'; \\mathbf{w}^-)\\)  13: \\(\\quad\\quad\\) end if    14: \\(\\quad\\quad\\) Update \\(\\mathbf{w}\\) using gradient descent:   \\(\\quad\\quad\\quad\\) \\(\\Delta \\mathbf{w} = \\alpha \\left( y_i - \\hat{Q}(s_i, a_i; \\mathbf{w}) \\right) \\nabla{\\mathbf{w}} \\hat{Q}(s_i, a_i; \\mathbf{w})\\)  15: \\(\\quad\\) end for        16: \\(\\quad\\) \\(t = t + 1\\)  17: \\(\\quad\\) if \\(t \\mod C == 0\\) then    18: \\(\\quad\\quad\\) \\(\\mathbf{w}^- \\leftarrow \\mathbf{w}\\)  19: \\(\\quad\\) end if     20: end loop        </p>"},{"location":"reinforcement/4_model_free_control/#model-free-control-mental-map","title":"Model Free Control Mental Map","text":"<pre><code>                     Model-Free Control\n    Goal: Learn the Optimal Policy \u03c0* without knowing P or R\n                               \u2502\n                               \u25bc\n           Key Concept: Action-Value Function Q(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Q\u03c0(s,a) = Expected return by taking action a \u2502\n       \u2502in state s and following policy \u03c0 thereafter \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                      No model \u2192 Learn Q directly\n                               \u2502\n                               \u25bc\n                   Generalized Policy Iteration\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Policy Evaluation       \u2502     Policy Improvement    \u2502\n       \u2502   Learn Q\u03c0(s,a)           \u2502   \u03c0 \u2190 greedy w.r.t Q      \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Challenge: Exploration vs. Exploitation\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502Greedy policy \u2192 Exploits but stops exploring          \u2502\n       \u2502\u03b5-greedy policy \u2192 Balances exploration &amp; exploitation \u2502\n       \u2502GLIE condition: \u03b5 \u2192 0 and \u221e exploration               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                Model-Free Control Families (Tabular)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502   Monte Carlo Control      \u2502      Temporal Difference   \u2502\n       \u2502   (Episode-based)          \u2502      (Step-based)          \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                                        \u25bc\n Monte Carlo Control:                       TD Control:\n Estimates Q from full returns          Estimates Q usingbootstrapped targets\n Uses \u03b5-greedy policy                   Works online, faster, low variance\n Episodic only                          Works for episodic &amp; continuing\n          \u2502                                        \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 GLIE MC Control   \u2502             \u2502 On-Policy TD: SARSA        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 Off-Policy TD: Q-Learning  \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                    |\n                                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n| On-Policy TD \u2014 SARSA                     |  Off-Policy TD \u2014 Q-Learning       |\n| Learns Q\u03c0 for the policy being followed  |  Learns Q* while following \u03c0_b    |\n| Update uses next action from \u03c0           |  Update uses max action (greedy)  |\n| Update Target:                           |  Update Target:                   |\n|  r + \u03b3 Q(s',a')                          |  r + \u03b3 max\u2090 Q(s',a)               |\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n      Value Function Approximation (Large/Continuous spaces)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Replace Q(s,a) with Q\u0302(s,a;w) using function approx   \u2502\n       \u2502 Generalization across states                         \u2502\n       \u2502 Gradient-based updates (SGD)                         \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Deep Q-Learning (DQN) \u2014 Stable VFA Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Experience Replay \u2014 decorrelate samples             \u2502\n       \u2502 Target Networks \u2014 stabilize bootstrapped targets    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Final Outcome of Model-Free Control\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly from experience without model       \u2502\n       \u2502 Learn Q*(s,a) through MC, SARSA, or Q-Learning        \u2502\n       \u2502 Scale to large spaces using function approximation    \u2502\n       \u2502 DQN enables deep RL in complex environments           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/5_policy_gradient/","title":"5. Policy Gradient Methods","text":""},{"location":"reinforcement/5_policy_gradient/#chapter-5-policy-gradient-methods","title":"Chapter 5: Policy Gradient Methods","text":"<p>In previous chapters, we derived policies indirectly from value functions using greedy or \u03b5-greedy strategies. However, value-based RL has several challenges:</p> <ul> <li>Does not naturally support stochastic policies  </li> <li>Struggles in continuous action spaces  </li> <li>Optimizing through value functions is often indirect and unstable</li> </ul> <p>Policy Gradient Methods directly optimize the policy itself:</p> \\[ \\pi_\\theta(a|s) = P(a \\mid s; \\theta) \\] <p>Our goal becomes:</p> \\[ \\theta^* = \\arg\\max_\\theta V(\\theta) \\] <p>That is, learn policy parameters \u03b8 that maximize expected return.</p> <p>Value-based methods struggle in these cases because they do not directly learn the policy. Instead, they estimate action values \\(Q(s,a)\\) and derive a policy using greedy or \u03b5-greedy strategies. This makes the policy indirect and unstable. Small changes in \\(Q(s,a)\\) can suddenly change the best action, making learning discontinuous and erratic \u2014 especially with function approximation like neural networks. Furthermore, value-based methods do not naturally support stochastic or continuous action spaces, since computing \\(\\arg\\max_a Q(s,a)\\) is infeasible when actions are continuous or infinite. Policy-based methods solve this problem by directly modeling and learning the policy, such as using a softmax distribution for discrete actions or Gaussian distributions for continuous actions.</p>"},{"location":"reinforcement/5_policy_gradient/#value-based-vs-policy-based-rl","title":"Value-Based vs Policy-Based RL","text":"Approach What is Learned? Policy Type Works in Continuous Actions? Value-Based \\(V(s)\\) or \\(Q(s,a)\\) Indirect (\u03b5-greedy, greedy) No Policy-Based \\(\\pi_\\theta(a/s)\\) Direct, stochastic Yes Actor-Critic Both Direct &amp; learned Yes"},{"location":"reinforcement/5_policy_gradient/#policy-optimization-objective","title":"Policy Optimization Objective","text":"<p>In policy-based reinforcement learning, the policy itself is directly parameterized as \\(\\pi_\\theta(a|s)\\), and our goal is to find the parameters \\(\\theta\\) that produce the best possible behavior. For episodic tasks starting at initial state \\(s_0\\), the quality of a policy is measured by its expected return:</p> \\[ V(\\theta) = V_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[G_0] \\] <p>Therefore, policy optimization can be formulated as an optimization problem, where the goal is to find the policy parameters \\(\\theta\\) that maximize the expected return:</p> \\[ \\theta^* = \\arg\\max_\\theta V(s_0, \\theta) \\] <p>This optimization does not necessarily require gradients.  We can also use gradient-free (derivative-free) optimization methods such as:</p> <ul> <li>Hill Climbing \u2013 Iteratively adjusts parameters in small random directions and keeps changes that improve performance.</li> <li>Simplex / Amoeba / Nelder-Mead \u2013 Uses a geometric shape (simplex) to explore the parameter space and moves it towards higher-performing regions.</li> <li>Genetic Algorithms \u2013 Evolves a population of candidate policies using selection, crossover, and mutation, inspired by natural evolution.</li> <li>Cross-Entropy Method (CEM) \u2013 Samples multiple policy candidates, selects the top performers, and updates the sampling distribution towards them.</li> <li>Covariance Matrix Adaptation (CMA) \u2013 Adapts both the mean and covariance of a Gaussian distribution to efficiently search complex, high-dimensional policy spaces.</li> </ul> <p>Gradient-free policy optimization methods are often excellent and simple baselines to try.  They are highly flexible, can work with any policy parameterization (including non-differentiable ones), and are easy to parallelize, as policies can be evaluated independently across multiple environments. However, these methods are typically less sample efficient because they treat each policy evaluation as a black box and ignore the temporal structure of trajectories. They do not make use of gradients, value functions, or bootstrapping.</p> <p>To improve efficiency, we can use gradient-based optimization techniques, which exploit  the structure of the return function and update parameters using local information. Common gradient-based optimizers include:</p> <ul> <li>Gradient Descent</li> <li>Conjugate Gradient</li> <li>Quasi-Newton Methods (e.g., BFGS, L-BFGS)</li> </ul> <p>These methods are generally more sample efficient, especially in large or continuous state-action spaces.</p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient","title":"Policy Gradient","text":"<p>The goal is to find parameters \\(\\theta\\) that maximize the expected return. Policy gradient algorithms search for a local maximum of \\(V(\\theta)\\) by performing gradient ascent:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta V(s_0, \\theta) \\] <p>where \\(\\alpha\\) is the step-size (learning rate) and \\(\\nabla_\\theta V(s_0, \\theta)\\) is the policy gradient.</p> <p>Assuming an episodic MDP with discount factor \\(\\gamma = 1\\), the value of a parameterized policy \\(\\pi_\\theta\\) starting from state \\(s_0\\) is</p> \\[ V(s_0,\\theta)  = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T} r(s_t, a_t); \\; s_0, \\pi_\\theta \\right], \\] <p>where the expectation is taken over the states and actions visited when following \\(\\pi_\\theta\\). This policy value can be re-expressed in multiple ways.  </p> <ul> <li> <p>First, in terms of the action-value function:      </p> </li> <li> <p>Second, in terms of full trajectories. Let a state\u2013action trajectory be:</p> <p>\\(\\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\\dots,s_{T-1},a_{T-1},r_T),\\)</p> <p>and define</p> <p>  as the sum of rewards of trajectory \\(\\tau\\).  </p> <p>Let \\(P(\\tau;\\theta)\\) denote the probability of trajectory \\(\\tau\\) when starting in \\(s_0\\) and following policy \\(\\pi_\\theta\\). Then:</p> \\[ V(s_0,\\theta) = \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau).\\] </li> </ul> <p>In this trajectory notation, our optimization objective becomes</p> \\[ \\theta^*  = \\arg\\max_{\\theta} V(s_0,\\theta)  = \\arg\\max_{\\theta} \\sum_{\\tau} P(\\tau;\\theta) \\, R(\\tau). \\] <p>Taking gradient yields:</p> \\[ \\nabla_\\theta V(\\theta) =  \\sum_{\\tau} P(\\tau|\\theta)R(\\tau)  \\nabla_\\theta \\log P(\\tau|\\theta) \\] <p>Using sampled trajectories:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m}  R(\\tau^{(i)}) \\nabla_\\theta \\log P(\\tau^{(i)}|\\theta) \\] <p>Trajectory probability:</p> \\[ P(\\tau|\\theta) = P(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t)\\cdot P(s_{t+1}|s_t,a_t) \\] <p>Since dynamics are independent of \\(\\theta\\):</p> \\[ \\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\] <p>Thus:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^{m} R(\\tau^{(i)}) \\sum_{t=0}^{T-1}  \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) \\] <p>The term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) is called the score function.  It is the gradient of the log of a parameterized probability distribution and measures how sensitive the policy\u2019s action probability is to changes in the parameters \\(\\theta\\).  It plays a central role in policy gradient methods because it allows us to estimate gradients without knowing the environment dynamics, using only samples from the policy.</p>"},{"location":"reinforcement/5_policy_gradient/#softmax-policy-discrete-action-spaces","title":"Softmax Policy (Discrete Action Spaces)","text":"<p>In discrete action spaces, a common parameterization of the policy is the softmax policy, which assigns probabilities based on exponentiated weighted features. Each action is represented using feature vector \\(\\phi(s,a)\\), and the policy is defined as:</p> \\[ \\pi_\\theta(a|s) =  \\frac{e^{\\phi(s,a)^T \\theta}}      {\\sum_{a'} e^{\\phi(s,a')^T \\theta}} \\] <p>The corresponding score function is:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\phi(s,a) - \\mathbb{E}_{a' \\sim \\pi_\\theta}[\\phi(s,a')] \\] <p>This means the gradient increases the probability of the selected action's features and decreases the probability of competing actions based on their expected feature values.</p>"},{"location":"reinforcement/5_policy_gradient/#gaussian-policy-continuous-action-spaces","title":"Gaussian Policy (Continuous Action Spaces)","text":"<p>For continuous action spaces, the Gaussian policy is a natural choice.  The policy outputs actions by sampling from a normal distribution:</p> \\[ a \\sim \\mathcal{N}(\\mu(s), \\sigma^2) \\] <p>The mean is a linear function of state features:</p> \\[ \\mu(s) = \\phi(s)^T \\theta \\] <p>If we assume a fixed variance \\(\\sigma^2\\), the score function becomes:</p> \\[ \\nabla_\\theta \\log \\pi_\\theta(a|s) = \\frac{(a - \\mu(s))}{\\sigma^2} \\, \\phi(s) \\] <p>This tells us that the gradient increases the likelihood of actions that are close to the mean \\(\\mu(s)\\) and reduces the probability of actions that deviate from it.</p> <p>Deep neural networks (and other differentiable models) can also be used to represent \\(\\pi_\\theta(a|s)\\), allowing score functions to be computed automatically using backpropagation.</p> <p>Intution:  Think of a sample trajectory \\(\\tau\\) as something we tried \u2014 a sequence of states, actions, and rewards collected during an episode. The return \\(R(\\tau)\\) tells us how good that sample was (higher return means better behavior). The gradient term \\(\\nabla_\\theta \\log P(\\tau|\\theta)\\) tells us how to adjust the policy parameters \\(\\theta\\) to make the trajectory more or less likely. So, when we multiply them:  we are effectively saying: If a trajectory was good, update the policy to make it more likely to occur again. If it was bad, update the policy to make it less likely. This simple idea is the core of policy gradient methods.</p>"},{"location":"reinforcement/5_policy_gradient/#reinforce-algorithm-monte-carlo-policy-gradient","title":"REINFORCE Algorithm (Monte Carlo Policy Gradient)","text":"<p>Update rule:</p> \\[ \\Delta \\theta = \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t \\] <p>Algorithm:</p> <p>1: Initialize policy parameters \\(\\theta\\) 2: loop (for each episode) 3: \\(\\quad\\) Generate a trajectory \\(\\tau = (s_0, a_0, r_1, \\dots, s_T)\\) using \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each time step \\(t\\) in \\(\\tau\\) 5: \\(\\quad\\quad\\) Compute return: \\(\\qquad R_t = \\sum_{k=t}^{T-1} \\gamma^{\\,k-t} r_{k+1}\\) 6: \\(\\quad\\quad\\) Compute policy gradient term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\) 7: \\(\\quad\\quad\\) Update policy parameters: \\(\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, R_t\\) 8: \\(\\quad\\) end for 9: end loop  </p>"},{"location":"reinforcement/5_policy_gradient/#policy-gradient-methods-mental-map","title":"Policy Gradient Methods \u2014 Mental Map","text":"<pre><code>                     Policy Gradient Methods\n    Goal: Learn the optimal policy \u03c0* directly (no Q or V tables)\n                               \u2502\n                               \u25bc\n         Key Concept: Parameterized Policy \u03c0\u03b8(a|s)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy is a function with parameters \u03b8      \u2502\n       \u2502 \u03c0\u03b8(a|s) gives probability of taking action a\u2502\n       \u2502 Optimization targets J(\u03b8)=Expected Return   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                    Direct Policy Optimization\n                               \u2502\n                               \u25bc\n                 Optimization Objective (J(\u03b8))\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 \u03b8* = argmax\u03b8 V(\u03b8) = argmax\u03b8 E\u03c0\u03b8[G\u2080]         \u2502\n       \u2502 Search in parameter space for best policy   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Two Families of Policy Optimization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  Gradient-Free Methods     \u2502   Gradient-Based Methods   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                      \u2502\n                \u2502                                      \u25bc\n                \u2502                          Policy Gradient Methods\n                \u2502                                      \u2502\n                \u25bc                                      \u25bc\n   No gradient needed                     Uses \u2207\u03b8 log \u03c0\u03b8(a|s) * Return\n   \u2013 Hill Climbing                        \u2013 REINFORCE\n   \u2013 CEM, CMA                             \u2013 Actor-Critic\n   \u2013 Genetic Algorithms                   \u2013 Advantage Methods\n   \u2502                                      \u2502\n   \u2514\u2500\u2500\u2500\u2500 Flexible &amp; parallelizable        \u2514\u2500\u2500\u2500\u2500 Sample efficient\n                               \u2502\n                               \u25bc\n                   Policy Gradient Core Idea\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Increase probability of good actions          \u2502\n       \u2502 Decrease probability of poor actions          \u2502\n       \u2502 Gradient term: \u2207\u03b8 log \u03c0\u03b8(a|s)(score function) \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     REINFORCE Algorithm (MC)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Sample full episodes (Monte Carlo)            \u2502\n       \u2502 Compute return Gt at each time step           \u2502\n       \u2502 Update: \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8 log \u03c0\u03b8(a_t|s_t) * G_t    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                      Policy Parameterization\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Softmax Policy (Discrete)  \u2502 Gaussian Policy(Continuous)\u2502\n       \u2502 \u03c0\u03b8(a|s) = exp(...)         \u2502 a ~ N(\u03bc(s), \u03c3\u00b2)            \u2502\n       \u2502 \u2207\u03b8 log \u03c0\u03b8 = \u03c6 - E\u03c6         \u2502 \u2207\u03b8 log \u03c0\u03b8 = (a-\u03bc)/\u03c3\u00b2 * \u03c6   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Final Outcome\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Learn \u03c0* directly (no need for Q or V tables)   \u2502\n       \u2502 Works naturally with stochastic &amp; continuous    \u2502\n       \u2502 Supports neural network policy parameterization \u2502\n       \u2502 Foundation of modern deep RL (PPO, A3C, DDPG)   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/6_pg2/","title":"6. Policy Gradient Variance Reduction and Actor-Critic","text":""},{"location":"reinforcement/6_pg2/#chapter-6-policy-gradient-variance-reduction-and-actor-critic","title":"Chapter 6: Policy Gradient Variance Reduction and Actor-Critic","text":"<p>In previous chapters, we introduced policy gradient methods as a way to directly optimize the policy \\(\\pi_\\theta(a|s)\\) in an MDP, rather than deriving a policy from value functions. We saw the basic REINFORCE algorithm (a Monte Carlo policy gradient) which adjusts policy parameters in the direction of higher returns. While policy gradient methods offer a direct approach to learning stochastic policies (even in continuous action spaces), naive implementations face several practical challenges:</p> <ul> <li> <p>High Variance in Gradient Estimates: The gradient estimator from REINFORCE can have very high variance because the return \\(G_t\\) depends on the entire trajectory. Different episodes produce widely varying returns, making updates noisy and slow to converge.</p> </li> <li> <p>Poor Sample Efficiency: Each trajectory (episode) is typically used for a single gradient update and then discarded. Because the policy changes after each update, data from past iterations cannot be directly reused without correction. This on-policy nature means learning requires many environment interactions.</p> </li> <li> <p>Sensitive to Step Size: Policy performance can be unstable if the learning rate (step size) is not tuned carefully. A too-large update can drastically change the policy (even collapse performance), while a too-small update makes learning exceedingly slow.</p> </li> <li> <p>Parameter Space vs Policy Space Mismatch: A small change in policy parameters \\(\\theta\\) does not always translate to a small change in the policy\u2019s behavior. For example, in a two-action policy with probability \\(\\pi_\\theta(a=1)=\\sigma(\\theta)\\) (sigmoid), a slight shift in \\(\\theta\\) can swing the action probabilities significantly if \\(\\theta\\) is in a sensitive region. In general, distance in parameter space does not correspond to distance in policy space. This means even tiny parameter updates can flip action preferences or make a stochastic policy almost deterministic, leading to large drops in reward.</p> </li> </ul> <p>The combination of these issues makes vanilla policy gradient methods hard to train reliably. This chapter introduces foundational techniques to address these problems: using baselines to reduce variance, adopting an actor\u2013critic architecture to improve sample efficiency, and building intuition for more stable update rules (to motivate trust-region methods covered later). Throughout, we will connect these ideas back to earlier concepts like MDP returns and model-free value learning.</p>"},{"location":"reinforcement/6_pg2/#policy-gradient-theorem-and-reinforce","title":"Policy Gradient Theorem and REINFORCE","text":"<p>n the previous chapter, we derived an expression for the gradient of the policy objective:</p> \\[ \\nabla_\\theta V(\\theta) \\approx \\frac{1}{m} \\sum_{i=1}^{m} R(\\tau^{(i)})  \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)}), \\] <p>where each trajectory \\(\\tau^{(i)}\\) is generated by the current policy \\(\\pi_\\theta\\).</p> <p>This Monte Carlo policy gradient estimator is unbiased: in expectation, it points in the correct gradient direction. However, it suffers from high variance:</p> <ul> <li>The return \\(R(\\tau)\\) depends on the entire trajectory.</li> <li>Different trajectories can have very different returns.</li> <li>Updates become noisy, unstable, and slow to converge.</li> </ul> <p>In reinforcement learning, data for learning comes from interacting with the environment using the current (possibly poor) policy. Every piece of experience is expensive: it requires actual environment steps, which may be slow, costly, or risky.</p> <p>Goal: Learn an effective policy with as few interactions (samples or episodes) as possible, and converge as quickly and reliably as possible to a good (local) optimum.</p>"},{"location":"reinforcement/6_pg2/#reducing-variance-with-baselines","title":"Reducing Variance with Baselines","text":"<p>One of the simplest and most effective ways to reduce variance in policy gradient estimates is to subtract a baseline function from the returns. The idea is to measure each action\u2019s outcome relative to an expected outcome, so that the update focuses on the advantage of the action rather than the raw return. Mathematically, we modify the gradient as follows:</p> \\[ \\nabla_\\theta \\mathbb{E}_\\tau [R] \\;=\\; \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; \\left(\\sum_{t'=t}^{T-1} r_{t'} - b(s_t)\\right) \\right]. \\] <p>where \\(b(s_t)\\) is an arbitrary baseline that depends on the state \\(s_t\\) (but not on the action). This adjusted estimator remains unbiased for any choice of $b(s), because the baseline is simply subtracted inside the expectation. Intuitively, \\(b(s_t)\\) represents a reference level for the return at state \\(s_t\\); the term \\((G_t - b(s_t))\\) is asking: did the action at \\(s_t\\) do better or worse than this baseline expectation?</p> <p>A particularly good choice for the baseline is the value function under the current policy, \\(b(s_t) \\approx V_{\\pi}(s_t)\\). This is the expected return from state \\(s_t\\) if we continue following the current policy. Using \\(V_{\\pi}(s_t)\\) as \\(b(s_t)\\) minimizes variance because it subtracts out the expected part of \\(G_t\\), leaving only the unexpected advantage of the action \\(a_t\\). Using a value function baseline leads to defining the advantage function:  where \\(Q(s_t,a_t)\\) is the expected return for taking action \\(a_t\\) in \\(s_t\\) and following the policy thereafter, and \\(V(s_t)\\) is the expected return from \\(s_t\\) on average. The advantage \\(A(s_t,a_t)\\) tells us how much better or worse the chosen action was compared to the policy\u2019s typical action at that state. If \\(A(s_t,a_t)\\) is positive, the action did better than expected; if negative, it did worse than expected. Using \\(A(s_t,a_t)\\) in the gradient update focuses learning on the deviations from usual outcomes.</p> <p>Benefits of Using a Baseline (Advantage): 1.  Variance Reduction: Subtracting \\(V(s_t)\\) removes the predictable part of the return, reducing the variability of the term \\((G_t - b(s_t))\\). The policy update then depends on advantage, which typically has lower variance than raw returns. 2.  Focused Learning: The update ignores outcomes that are \u201cas expected\u201d and emphasizes surprises. In other words, only the excess reward beyond the baseline (positive or negative) contributes to the gradient, which helps credit assignment. 3.  Unbiased Gradient: Because \\(b(s_t)\\) does not depend on the action, the expected value of \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\, b(s_t)\\) is zero. Thus, adding or subtracting the baseline does not change the expected gradient, only its variance</p> <p>Using a baseline in practice usually means we need to estimate the value function \\(V_{\\pi}(s)\\) for the current policy. This can be done by fitting a critic (e.g. a neural network) to predict returns. After each batch of episodes, one can regress \\(b(s)\\) toward the observed returns \\(G_t\\) to improve the baseline estimate.</p> <p>Algorithm: Policy Gradient with Baseline (Advantage Estimation)</p> <p>1: Initialize policy parameter \\(\\theta\\), baseline \\(b(s)\\) 2: for iteration \\(= 1, 2, \\dots\\) do 3: \\(\\quad\\) Collect a set of trajectories by executing the current policy \\(\\pi_\\theta\\) 4: \\(\\quad\\) for each trajectory \\(\\tau^{(i)}\\) and each timestep \\(t\\) do 5: \\(\\quad\\quad\\) Compute return: \\(\\quad\\quad\\quad G_t^{(i)} = \\sum_{t'=t}^{T-1} r_{t'}^{(i)}\\) 6: \\(\\quad\\quad\\) Compute advantage estimate: \\(\\quad\\quad\\quad \\hat{A}_t^{(i)} = G_t^{(i)} - b(s_t^{(i)})\\) 7: \\(\\quad\\) end for 8: \\(\\quad\\) Re-fit baseline by minimizing: \\(\\quad\\quad \\sum_i \\sum_t \\big(b(s_t^{(i)}) - G_t^{(i)}\\big)^2\\) 9: \\(\\quad\\) Update policy parameters using gradient estimate: \\(\\quad\\quad \\theta \\leftarrow \\theta + \\alpha \\sum_{i,t} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)} \\mid s_t^{(i)})\\, \\hat{A}_t^{(i)}\\) 10: \\(\\quad\\) (Plug into SGD or Adam optimizer) 11: end for  </p> <p>This algorithm is essentially the REINFORCE policy gradient with an added learned baseline. If the baseline is perfect (\\(b(s)=V_\\pi(s)\\)), then \\(\\hat{A}t = G_t - V\\pi(s_t)\\) is an estimate of the advantage \\(A(s_t,a_t)\\). Even an imperfect baseline can significantly stabilize training. The baseline does not introduce bias; it simply provides a reference point so that the policy gradient update increases log-probability of actions that outperform the baseline and decreases it for actions that underperform.</p>"},{"location":"reinforcement/6_pg2/#actorcritic-methods","title":"Actor\u2013Critic Methods","text":"<p>Using a learned baseline brings us to the idea of actor\u2013critic algorithms. In the policy gradient with baseline above, the policy is the \"actor\" and the value function baseline is a \"critic\" that evaluates the actor\u2019s decisions. Actor\u2013critic methods generalize this by allowing the critic to be learned online with temporal-difference methods, combining the strengths of policy-based and value-based learning.</p> <ul> <li>Actor: the policy \\(\\pi_\\theta(a|s)\\) that selects actions and is updated by gradient ascent.</li> <li>Critic: a value function \\(V_w(s)\\) (with parameters \\(w\\)) that estimates the return from state \\(s\\) under the current policy. The critic provides the baseline or advantage estimates used in the actor\u2019s update.</li> </ul> <p>Instead of waiting for full episode returns \\(G_t\\), an actor\u2013critic uses the critic\u2019s bootstrapped estimate to get a lower-variance estimate of advantage at each step. The actor is updated using the critic\u2019s current estimate of advantage \\(A(s_t,a_t)\\), and the critic is updated by minimizing the temporal-difference (TD) error similar to value iteration.</p> <p>Actor update: The policy (actor) update is similar to before, but using the critic\u2019s advantage estimate \\(A_t\\) at time \\(t\\):  Here \\(A_t \\approx Q(s_t,a_t) - V(s_t)\\) is provided by the critic. This update nudges the policy to choose actions that the critic deems better than average (positive \\(A_t\\)) and away from actions that seem worse than expected.</p> <p>Critic update: The critic (value function) is learned by a value-learning rule. Typically, we use the TD error \\(\\delta_t\\) to update \\(w\\):  which measures the discrepancy between the predicted value at \\(s_t\\) and the reward plus discounted value of the next state. The critic\u2019s parameters \\(w\\) are updated by a gradient step proportional to \\(\\delta_t \\nabla_w V_w(s_t)\\) (this is essentially a TD(0) update). In practice:  with \\(\\beta\\) a critic learning rate. This update pushes the critic\u2019s value estimate \\(V_w(s_t)\\) toward the observed reward plus the estimated value of \\(s_{t+1}\\). The TD error \\(\\delta_t\\) is also used as an advantage estimate for the actor: notice \\(\\delta_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\) serves as a one-step advantage (immediate reward plus expected next value minus current value). Over multiple steps, the critic can provide smoother, lower-variance advantage estimates than raw returns.</p> <p>Why Actor\u2013Critic? Actor\u2013critic methods marry the best of both worlds: the actor benefits from the stable learning and lower variance of value-based (TD) methods, and the critic benefits from focusing only on value estimation while the actor handles policy improvement. Compared to pure REINFORCE (which is like a Monte Carlo actor-only method), actor\u2013critic algorithms tend to: - Learn faster (lower variance updates thanks to the critic\u2019s guidance) - Be more sample-efficient (they can update the policy every time step with TD feedback rather than waiting until an episode ends) - Naturally handle continuing (non-episodic) tasks via the critic\u2019s ongoing value estimates - Still allow stochastic policies and continuous actions (since the actor is explicit)</p> <p>However, actor\u2013critics introduce bias through the critic\u2019s approximation and can become unstable if the critic is poorly trained. In practice, careful tuning and using experience replay or other tricks might be needed for stability (though replay is generally off-policy and less common with vanilla actor\u2013critic; later algorithms address this).</p> <p>Advantage Estimation: In an actor\u2013critic, one often uses n-step returns or more generally \\(\\lambda\\)-returns to strike a balance between bias and variance in advantage estimates. For example, rather than using the full Monte Carlo return or the 1-step TD error alone, we can use a mix: e.g. a 3-step return \\(R^{(3)}t = r_t + \\gamma r{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V(s_{t+3})\\), and define \\(\\hat{A}_t = R^{(3)}_t - V(s_t)\\). Smaller \\(n\\) gives lower variance but more bias; larger \\(n\\) gives higher variance but less bias. A technique called Generalized Advantage Estimation (GAE) (covered in the next chapter) will formalize this idea of mixing multi-step returns to get the best of both worlds.</p> <p>In summary, the actor\u2013critic architecture provides a powerful framework: the actor optimizes the policy directly, while the critic provides critical feedback on the policy\u2019s behavior. By using a learned value function as an evolving baseline, we significantly reduce variance and can make updates more frequently (every step rather than every episode). This sets the stage for modern policy gradient methods, which further refine how we estimate advantages and how we constrain policy updates for stability.</p>"},{"location":"reinforcement/6_pg2/#limitations-of-vanilla-policy-gradient-and-trust-region-motivation","title":"Limitations of Vanilla Policy Gradient and Trust-Region Motivation","text":"<p>Despite using baselines and even actor\u2013critic methods, vanilla policy gradient algorithms (including basic actor\u2013critic) still face some fundamental limitations. Understanding these issues motivates the development of more advanced algorithms (like TRPO and PPO) that enforce cautious updates. The key limitations are:</p> <ul> <li> <p>On-Policy Data Inefficiency: Vanilla policy gradient is inherently on-policy, meaning it requires fresh data from the current policy for each update. Each batch of trajectories is used only once for a gradient step and then discarded. Reusing samples collected from an older policy \\(\\pi_{\\text{old}}\\) to update the current policy \\(\\pi_{\\text{new}}\\) introduces bias, because the gradient formula assumes data comes from \\(\\pi_{\\text{new}}\\). In short, standard policy gradients waste a lot of data, making them sample-inefficient.</p> </li> <li> <p>Importance Sampling and Large Policy Shifts: We can correct off-policy data (from an old policy) by weighting with importance sampling ratios \\(r_t = \\frac{\\pi_{\\text{new}}(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}\\). This reweights trajectories to account for the new policy. While mathematically unbiased, importance sampling can blow up in variance if \\(\\pi_{\\text{new}}\\) differs significantly from \\(\\pi_{\\text{old}}\\). A few outlier actions with tiny old-policy probability and larger new-policy probability can dominate the estimate. Thus, without constraints, reusing data for multiple updates can lead to wildly unstable gradients. In practice, naively reusing past data is dangerous unless the policy changes only a little.</p> </li> <li> <p>Sensitivity to Step Size: As mentioned earlier choosing the right learning rate is critical. If the policy update step is too large, the new policy can be much worse than the old (policy collapse). The training can oscillate or diverge. On the other hand, very small steps waste time. This sensitivity makes training fragile \u2013 even with adaptive optimizers, a single bad update can ruin performance. Ideally, we want a way to automatically ensure each update is not \u201ctoo large\u201d in terms of its impact on the policy\u2019s behavior.</p> </li> <li> <p>Parameter Updates vs. Policy Changes: A fundamental issue is that the metric we care about is policy performance, but we update parameters in \\(\\theta\\)-space. A small change in parameters can lead to a disproportionate change in the policy\u2019s action distribution (especially in nonlinear function approximators). For example, tweaking a weight in a neural network policy might cause it to prefer completely different actions for many states. Conversely, some large parameter changes might have minimal effect on action probabilities. Because small parameter changes \u2260 small policy changes, it\u2019s hard to set a fixed step size that reliably guarantees safe policy updates in all situations.</p> </li> </ul> <p>These limitations suggest that we need a more restrained, robust approach to updating policies. The core idea is to restrict how far the new policy can deviate from the old policy on each update \u2013 in other words, to stay within a \u201ctrust region\u201d around the current policy. By measuring the change in terms of the policy distribution itself (for instance, using Kullback\u2013Leibler divergence as a distance between \\(\\pi_{\\text{new}}\\) and \\(\\pi_{\\text{old}}\\)), we can ensure updates do not move the policy too abruptly. This leads to trust-region policy optimization methods, which use constrained optimization or clipped objectives to keep the policy change small but significant. The next chapter will introduce KL-divergence-constrained policy optimization algorithms (notably TRPO and PPO) and Generalized Advantage Estimation, which together address the stability and efficiency issues discussed here. These advanced methods build directly on the foundations of baselines and actor\u2013critic learning introduced above, combining them with principled constraints to achieve more stable and sample-efficient policy learning.</p>"},{"location":"reinforcement/7_gae/","title":"7. Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":""},{"location":"reinforcement/7_gae/#chapter-7-advances-in-policy-optimization-gae-trpo-and-ppo","title":"Chapter 7: Advances in Policy Optimization \u2013 GAE, TRPO, and PPO","text":"<p>In the previous chapter, we improved the foundation of policy gradients by reducing variance (using baselines) and introducing actor\u2013critic methods. We also noted that unrestricted policy updates can be unstable and sample-inefficient. In this chapter, we present modern advances in policy optimization that build on those ideas to achieve much better performance in practice. We focus on two main developments: Generalized Advantage Estimation (GAE), which refines how we estimate advantages to balance bias and variance, and trust-region methods (specifically Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)) that ensure updates do not destabilize the policy. These techniques enable more sample-efficient, stable learning by reusing data safely and preventing large detrimental policy shifts.</p>"},{"location":"reinforcement/7_gae/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>Accurate and low-variance advantage estimates are crucial for effective policy gradient updates. Recall that the policy gradient update uses the term \\(\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\,A(s_t,a_t)\\) \u2013 if \\(A(s_t,a_t)\\) is noisy or biased, it can severely affect learning. Advantage can be estimated via: - Monte Carlo returns: \\(A_t = G_t - V(s_t)\\) using the full return \\(G_t\\) (summing all future rewards until episode end). This is an unbiased estimator of the true advantage, but it has very high variance because it includes all random future outcomes.</p> <ul> <li> <p>One-step TD returns: \\(A_t \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)\\), using the critic\u2019s bootstrapped estimate of the future. This one-step advantage (equivalently the TD error \\(\\delta_t\\)) has much lower variance (it relies on the learned value for the next state) but is biased by function approximation and by truncating the return after one step.</p> </li> <li> <p>n-Step returns:We can also use intermediate approaches, for example a 2-step return \\(R^{(2)}t = r_t + \\gamma r{t+1} + \\gamma^2 V(s_{t+2})\\) giving an advantage \\(\\hat{A}^{(2)}_t = R^{(2)}_t - V(s_t)\\). In general, an n-step advantage estimator can be written as:</p> \\[A^{t}(n) = \\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i+1} + \\gamma^{n} V(s_{t+n}) -V(s_{t})\\] <p>which blends \\(n\\) actual rewards with a bootstrap at time \\(t+n\\). Smaller \\(n\\) (like 1) means more bias (due to heavy reliance on \\(V\\)) but low variance; larger \\(n\\) (approaching the episode length) reduces bias but increases variance.</p> </li> </ul> <p>The pattern becomes clearer if we express these in terms of the TD error \\(\\delta_t\\) (the one-step advantage at \\(t\\)):</p> <p>  - For a 1-step return, \\(\\hat{A}^{(1)}_t = \\delta_t\\). - For a 2-step return, \\(\\hat{A}^{(2)}t = \\delta_t + \\gamma\\,\\delta\\). - For an \\(n\\)-step return, \\(\\hat{A}^{(n)}t = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + \\cdots + \\gamma^{n-1}\\delta_{t+n-1}\\)</p> <p>Each additional term \\(\\gamma^i \\delta_{t+i}\\) extends the return by one more step of real reward before bootstrapping, increasing bias a bit (since it assumes the later \\(\\delta\\) terms are based on an approximate \\(V\\)) but capturing more actual reward outcomes (reducing variance less).</p> <p>Generalized Advantage Estimation (GAE) takes this idea to its logical conclusion by forming a weighted sum of all n-step advantages, with exponentially decreasing weights. Instead of picking a fixed \\(n\\), GAE uses a parameter \\(0 \\le \\lambda \\le 1\\) to blend advantages of different lengths:</p> \\[\\hat{A}^{\\text{GAE}(\\gamma,\\lambda)}_t \\;=\\; (1-\\lambda)\\Big(\\hat{A}^{(1)}_t + \\lambda\\,\\hat{A}^{(2)}_t + \\lambda^2\\,\\hat{A}^{(3)}_t + \\cdots\\Big)\\] <p>This infinite series can be shown to simplify to a very convenient form:</p> \\[\\hat{A}_t^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i\\delta_{t+i}\\] <p>which is an exponentially-weighted sum of the future TD errors. In practice, this is implemented with a simple recursion running backward through each trajectory (since it\u2019s a sum of discounted TD errors).</p> <p>Key intuition: \\(\\lambda\\) controls the bias\u2013variance trade-off in advantage estimation:</p> <ul> <li> <p>\\(\\lambda = 0\\) uses only the one-step TD error: \\(\\hat{A}^{\\text{GAE}(0)}_t = \\delta_t\\). This is the lowest-variance, highest-bias estimator (similar to TD(0) advantage)[21].</p> </li> <li> <p>\\(\\lambda = 1\\) uses an infinitely long sum of un-discounted TD errors, which in theory equals the full Monte Carlo return advantage (since all bootstrapping is deferred to the end). This is unbiased (in the limit of exact \\(V\\)) but highest variance \u2013 essentially Monte Carlo estimation.</p> </li> <li> <p>Intermediate \\(0&lt;\\lambda&lt;1\\) gives a mixture. A typical choice is \\(\\lambda = 0.95\\) in many applications, which provides a good balance (mostly long-horizon returns with a bit of bootstrapping to damp variance).</p> </li> </ul> <p>GAE is not introducing a new kind of return; rather, it generalizes existing returns. It smoothly interpolates between TD and Monte Carlo methods. When \\(\\lambda\\) is low, GAE trusts the critic more (using more bootstrapped estimates); when \\(\\lambda\\) is high, GAE leans toward actual returns over many steps. In modern actor\u2013critic algorithms (including TRPO and PPO), GAE is used to compute the advantage for each state-action in a batch. A typical implementation for each iteration is:</p> <ol> <li>Collect trajectories using the current policy \\(\\pi_{\\theta}\\) (e.g. run \\(N\\) episodes or \\(T\\) time steps of experience).</li> <li>Compute state values \\(V(s_t)\\) for each state visited (using the current value function estimate).</li> <li>Compute TD residuals \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) for each time step.</li> <li> <p>Apply GAE formula: going from \\(t=T-1\\) down to \\(0\\), accumulate \\(\\hat{A}_t = \\delta_t + \\gamma \\lambda, \\hat{A}{t+1}\\), with \\(\\hat{A}_{T} = 0\\). This yields \\(\\hat{A}_t \\approx \\sum{i\\ge0} (\\gamma \\lambda)^i \\delta{t+i}\\).</p> </li> <li> <p>Use Advantages for Update: These \\(\\hat{A}_t\\) values serve as the advantage estimates in the policy gradient update. Simultaneously, you can compute proxy returns for the critic by adding \\(\\hat{A}_t\\) to the baseline \\(V(s_t)\\) (i.e. \\(\\hat{R}_t = \\hat{A}_t + V(s_t)\\), an estimate of the actual return) and use those to update the value function parameters.</p> </li> </ol> <p>The result of GAE is a much smoother, lower-variance advantage signal for the actor, without introducing too much bias. Empirically, this greatly stabilizes training: the policy doesn\u2019t overreact to single high-return episodes, and it doesn\u2019t ignore long-term outcomes either. GAE essentially bridges the gap between the high-variance Monte Carlo world of Chapter 5 and the low-variance TD world of Chapter 3\u20134, and it has become a standard component in virtually all modern policy optimization algorithms.</p>"},{"location":"reinforcement/7_gae/#kl-divergence-constraints-and-surrogate-objectives","title":"KL Divergence Constraints and Surrogate Objectives","text":"<p>We now turn to the question of stable policy updates. As discussed, a major issue with vanilla policy gradient is that a single update can accidentally push the policy into a disastrous region (because the gradient is computed at the current policy but we might step too far). To make updates safer, we want to constrain how much the policy changes at each step. A natural way to measure change between the old policy \\(\\pi_{\\text{old}}\\) and a new policy \\(\\pi_{\\text{new}}\\) is to use the Kullback\u2013Leibler (KL) divergence. For example, we can require:</p> \\[\\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}}(\\cdot \\mid s)\\,\\|\\,\\pi_{\\text{old}}(\\cdot \\mid s)\\bigr) \\right] \\le \\delta\\] <p>for some small \\(\\delta\\). This means that on average over states (under the old policy\u2019s state distribution \\(d_{\\pi_{\\text{old}}}\\)), the new policy\u2019s probability distribution is not too far from the old policy\u2019s distribution. A small KL divergence ensures the policies behave similarly, limiting the \u201csurprise\u201d from one update.</p> <p>But how do we optimize under such a constraint? We need an objective function that tells us whether \\(\\pi_{\\text{new}}\\) is better than \\(\\pi_{\\text{old}}\\). Fortunately, theory provides a useful tool: a surrogate objective that approximates the change in performance if the policy change is small. One version, derived from the policy performance difference lemma and monotonic improvement theorem, is:</p> \\[L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) = \\mathbb{E}_{s,a \\sim \\pi_{\\text{old}}} \\left[ \\frac{\\pi_{\\text{new}}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} A_{\\pi_{\\text{old}}}(s,a) \\right]\\] <p>This is an objective functional\u2014it evaluates the new policy using samples from the old policy, weighting rewards by the importance ratio \\(r(s,a) = \\pi_{\\text{new}}(a|s)/\\pi_{\\text{old}}(a|s)\\). Intuitively, \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) is asking: if the old policy visited state \\(s\\) and took action \\(a\\), how good would that decision be under the new policy\u2019s probabilities? Actions that the new policy wants to do more of (\\(r &gt; 1\\)) will contribute their advantage (good or bad) proportionally more.</p> <p>Critically, one can show that if \\(\\pi_{\\text{new}}\\) is very close to \\(\\pi_{\\text{old}}\\) (in KL terms), then improving this surrogate \\(L\\) guarantees an improvement in the true return \\(J(\\pi)\\). Specifically, there is a bound such that:</p> \\[J(\\pi_{\\text{new}}) \\ge J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - C \\, \\mathbb{E}_{s \\sim d^{\\pi_{\\text{old}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\text{new}} \\,\\|\\, \\pi_{\\text{old}}\\bigr)[s] \\right]\\] <p>for some constant \\(C\\) related to horizon and policy support. When the KL divergence is small, the last term is second-order (negligible), so roughly we get \\(J(\\pi_{\\text{new}}) \\gtrapprox J(\\pi_{\\text{old}}) + L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\). In other words, maximizing \\(L\\) while keeping KL small ensures monotonic improvement: each update should not reduce true performance.</p> <p>This insight leads directly to a constrained optimization formulation for safe policy updates:</p> <ul> <li>Objective: Maximize the surrogate \\(L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})\\) (i.e. maximize expected advantage-weighted probability ratios).</li> <li>Constraint: Limit the policy divergence via \\(D_{\\mathrm{KL}}(\\pi_{\\text{new}}\\Vert \\pi_{\\text{old}}) \\le \\delta\\) (for some small \\(\\delta\\)). Algorithms that implement this idea are called trust-region methods, because they optimize the policy within a trust region of the old policy. Next, we discuss two prominent algorithms: TRPO, which tackles the constrained problem directly (with some approximations), and PPO, which simplifies it into an easier unconstrained loss function.</li> </ul>"},{"location":"reinforcement/7_gae/#trust-region-policy-optimization","title":"Trust Region Policy Optimization","text":"<p>Trust Region Policy Optimization (TRPO) is a seminal algorithm that explicitly embodies the constrained update approach. TRPO chooses a new policy by approximately solving:</p> \\[\\max_{\\theta_{\\text{new}}} \\; L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}}) \\quad \\text{s.t.} \\quad \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_{\\text{old}}}}} \\left[ D_{\\mathrm{KL}}\\bigl(\\pi_{\\theta_{\\text{new}}} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\bigr) \\right] \\le \\delta\\] <p>where \\(L_{\\theta_{\\text{old}}}(\\theta_{\\text{new}})\\) is the surrogate objective defined above, and \\(\\delta\\) is a small trust-region threshold. In practice, solving this exactly is difficult due to the infinite-dimensional policy space. TRPO makes it tractable by using a few key ideas:</p> <ul> <li> <p>Approximating the constraint via a quadratic expansion of the KL divergence (which yields a Fisher Information Matrix). This turns the problem into something like a second-order update (a natural gradient step). In fact, TRPO\u2019s solution can be shown to correspond to a natural gradient ascent:</p> <p>\\(\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\sqrt{\\frac{2\\delta}{g^T F^{-1} g}}\\; F^{-1} g\\)$</p> <p>where \\(g = \\nabla_\\theta L\\) and \\(F\\) is the Fisher matrix. This ensures the KL constraint is satisfied approximately, and is equivalent to scaling the gradient by \\(F^{-1}\\). In simpler terms, TRPO updates \\(\\theta\\) in a direction that accounts for the curvature of the policy space, so that the change in policy (KL) is proportional to the step size.</p> </li> <li> <p>Using a line search to ensure the new policy actually improves \\(J(\\pi)\\). TRPO will back off the step size if the updated policy violates the constraint or fails to achieve a performance improvement. This safeguard maintains the monotonic improvement guarantee in practice.</p> </li> </ul> <p>A simplified outline of TRPO is:</p> <ol> <li>Collect trajectories with the current policy \\(\\pi_{\\theta_{\\text{old}}}\\).</li> <li>Estimate advantages \\(\\hat{A}_t\\) for each time step (using GAE or another method for high-quality advantage estimates).</li> <li>Compute surrogate objective \\(L(\\theta) = \\mathbb{E}[r_t(\\theta), \\hat{A}t]\\) where \\(r_t(\\theta) = \\frac{\\pi{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\).</li> <li>Approximate KL constraint: Compute the policy gradient \\(\\nabla_\\theta L\\) and the Fisher matrix \\(F\\) (via sample-based estimation of the Hessian of the KL divergence). Solve for the update direction \\(p \\approx F^{-1} \\nabla_\\theta L\\) (e.g. using conjugate gradient).</li> <li>Line search: Scale and apply the update step \\(\\theta \\leftarrow \\theta + p\\) gradually, checking the KL and improvement. Stop when the KL constraint or improvement criterion is satisfied.</li> </ol> <p>TRPO\u2019s updates are therefore conservative by design \u2013 they will only take as large a step as can be trusted not to degrade performance. TRPO was influential because it demonstrated much more stable and reliable training on complex continuous control tasks than vanilla policy gradient.</p> <p>Strengths and Weaknesses of TRPO: TRPO offers a theoretical guarantee of non-destructive updates \u2013 under certain assumptions, each iteration is guaranteed to improve or at least not decrease performance. It uses a natural gradient approach that respects the geometry of policy space, which is more effective than an arbitrary gradient in parameter space. However, TRPO comes at a cost: it requires calculating second-order information (the Fisher matrix), and implementing the conjugate gradient solver and line search adds complexity. The algorithm can be slower per iteration and is more complex to code and tune. In practice, TRPO, while effective, proved somewhat cumbersome for large-scale problems due to these complexities.</p>"},{"location":"reinforcement/7_gae/#proximal-policy-optimization","title":"Proximal Policy Optimization","text":"<p>Proximal Policy Optimization (PPO) was introduced as a simpler, more user-friendly variant of TRPO that achieves similar results with only first-order optimization. The core idea of PPO is to keep the spirit of trust-region updates (don\u2019t move the policy too far in one go) but implement it via a relaxed objective that can be optimized with standard stochastic gradient descent. There are two main variants of PPO:</p>"},{"location":"reinforcement/7_gae/#kl-penalty-objective","title":"KL-Penalty Objective:","text":"<p>One version of PPO adds the KL-divergence as a penalty to the objective rather than a hard constraint. The objective becomes:</p> \\[J_{\\text{PPO-KL}}(\\theta) = \\mathbb{E}\\!\\left[ r_t(\\theta)\\, \\hat{A}^t \\right] - \\beta \\,\\mathbb{E}\\!\\left[ D_{\\mathrm{KL}}\\!\\left(\\pi_{\\theta} \\,\\|\\, \\pi_{\\theta_{\\text{old}}}\\right) \\right]\\] <p>where \\(\\beta\\) is a coefficient determining how strongly to penalize deviation from the old policy. If the KL divergence in an update becomes too large, \\(\\beta\\) can be adjusted (increased) to enforce smaller steps in subsequent updates. This approach maintains a soft notion of a trust region.</p>"},{"location":"reinforcement/7_gae/#algorithm-ppo-with-kl-penalty","title":"Algorithm (PPO with KL Penalty)","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), initial KL penalty \\(\\beta_0\\), target KL-divergence \\(\\delta\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^t_k\\) using any advantage estimation algorithm 5: \\(\\quad\\) Compute policy update by approximately solving \\(\\quad\\quad\\) \\(\\theta_{k+1} = \\arg\\max_\\theta \\; L_{\\theta_k}(\\theta) - \\beta_k \\hat{D}_{KL}(\\theta \\,\\|\\, \\theta_k)\\) 6: \\(\\quad\\) Implement this optimization with \\(K\\) steps of minibatch SGD (e.g., Adam) 7: \\(\\quad\\) Measure actual KL: \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k)\\) 8: \\(\\quad\\) if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\ge 1.5\\delta\\) then 9: \\(\\quad\\quad\\) Increase penalty: \\(\\beta_{k+1} = 2\\beta_k\\) 10: \\(\\quad\\) else if \\(\\hat{D}_{KL}(\\theta_{k+1}\\|\\theta_k) \\le \\delta/1.5\\) then 11: \\(\\quad\\quad\\) Decrease penalty: \\(\\beta_{k+1} = \\beta_k/2\\) 12: \\(\\quad\\) end if 13: end for  </p>"},{"location":"reinforcement/7_gae/#clipped-surrogate-objective-ppo-clip","title":"Clipped Surrogate Objective (PPO-Clip):","text":"<p>The more popular variant of PPO uses a clipped surrogate objective to restrict policy updates:</p> \\[L^\\text{CLIP}(\\theta) = \\mathbb{E}_{t}\\!\\left[ \\min\\!\\Big( r_t(\\theta)\\,\\hat{A}^t,\\; \\text{clip}\\!\\big(r_t(\\theta),\\, 1-\\epsilon,\\, 1+\\epsilon\\big)\\,\\hat{A}^t \\Big) \\right]\\] <p>where \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\) as before, and \\(\\epsilon\\) is a small hyperparameter (e.g. 0.1 or 0.2) that defines the clipping range. This objective says: if the new policy\u2019s probability ratio \\(r_t(\\theta)\\) stays within \\([1-\\epsilon,\\,1+\\epsilon]\\), we use the normal surrogate \\(r_t \\hat{A}_t\\). But if \\(r_t\\) tries to go outside this range (meaning the policy probability for an action has changed dramatically), we clip \\(r_t\\) to either \\(1+\\epsilon\\) or \\(1-\\epsilon\\) before multiplying by \\(\\hat{A}_t\\). Effectively, the advantage contribution is capped once the policy deviates too much from the old policy.</p> <p>The clipped objective is not exactly the original constrained problem, but it serves a similar purpose: it removes the incentive for the optimizer to push \\(r_t\\) outside of \\([1-\\epsilon,1+\\epsilon]\\). If increasing \\(|\\theta|\\) further doesn\u2019t increase the objective (because the min() will select the clipped term), then overly large policy changes are discouraged.</p> <p>Why Clipping Works: Clipping is a simple heuristic, but it has proven extremely effective:</p> <ul> <li> <p>It enforces a soft trust region by preventing extreme updates for any single state-action probability. The policy can still change, but not so much that any one probability ratio blows up.</p> </li> <li> <p>It avoids the complexity of solving a constrained optimization or computing second-order derivatives \u2013 we can just do standard SGD on \\(L^{CLIP}(\\theta)\\).</p> </li> <li> <p>It keeps importance sampling ratios near 1, which means the algorithm can safely perform multiple epochs of updates on the same batch of data without the estimates drifting too far. This directly improves sample efficiency (unlike vanilla policy gradient, PPO typically updates each batch for several epochs).</p> </li> </ul>"},{"location":"reinforcement/7_gae/#ppo-clipped-algorithm","title":"PPO (Clipped) Algorithm","text":"<p>1: Input: initial policy parameters \\(\\theta_0\\), clipping threshold \\(\\epsilon\\) 2: for \\(k = 0, 1, 2, \\ldots\\) do 3: \\(\\quad\\) Collect a set of partial trajectories \\(\\mathcal{D}_k\\) using policy \\(\\pi_k = \\pi(\\theta_k)\\) 4: \\(\\quad\\) Estimate advantages \\(\\hat{A}^{\\,t}_k\\) using any advantage estimation algorithm (e.g., GAE) 5: \\(\\quad\\) Define the clipped surrogate objective \\(\\quad\\quad\\)  6: \\(\\quad\\) Update policy parameters with several epochs of minibatch SGD to approximately maximize \\(\\mathcal{L}^{\\text{CLIP}}_{\\theta_k}(\\theta)\\) 7: \\(\\quad\\) Set \\(\\theta_{k+1}\\) to the resulting parameters 8: end for  </p> <p>In practice, PPO with clipping has become one of the most widely used RL algorithms because it strikes a good balance between performance and simplicity. It is relatively easy to implement (compared to TRPO) and has been found to be robust across many tasks and hyperparameters. While it doesn\u2019t guarantee monotonic improvement in theory, in practice it achieves stable training behavior very similar to TRPO.</p> <p>In modern practice, PPO is the dominant choice for policy optimization in deep RL, due to its relative simplicity and strong performance across many environments. TRPO is still important conceptually (and sometimes used in scenarios where theoretical guarantees are desired), but PPO\u2019s convenience usually wins out.</p>"},{"location":"reinforcement/7_gae/#putting-it-together-sample-efficiency-stability-and-monotonic-improvement","title":"Putting It Together: Sample Efficiency, Stability, and Monotonic Improvement","text":"<p>The advances covered in this chapter are often used together in state-of-the-art algorithms:</p> <ul> <li> <p>Generalized Advantage Estimation (GAE) provides high-quality advantage estimates that significantly reduce variance without too much bias. This means we can get away with smaller batch sizes or fewer episodes to get a good learning signal \u2013 improving sample efficiency.</p> </li> <li> <p>Trust-region update rules (TRPO/PPO) ensure that each policy update is safe and stable \u2013 the policy doesn\u2019t change erratically, preventing the kind of catastrophic drops in reward that naive policy gradients can suffer. By keeping policy changes small (via KL constraints or clipping), these methods enable multiple updates on the same batch of data (improving data efficiency) and maintain policy monotonicity, i.e. each update is expected to improve or at least not significantly degrade performance.</p> </li> <li> <p>In practice, an algorithm like PPO with GAE is an actor\u2013critic method that uses all these ideas: an actor policy updated with a clipped surrogate objective (making updates stable), a critic to approximate \\(V(s)\\) (enabling advantage estimation), GAE to compute advantages (trading off bias/variance), and typically multiple gradient epochs per batch to squeeze more learning out of each sample. This combination has proven remarkably successful in domains from simulated control tasks to games.</p> </li> </ul> <p>By building on the foundational policy gradient framework and addressing its shortcomings, GAE and trust-region approaches have made deep reinforcement learning much more practical and reliable. They illustrate how theoretical insights (performance bounds, policy geometry) and practical tricks (advantage normalization, clipping) come together to yield algorithms that can solve challenging RL problems while using reasonable amounts of training data and maintaining stability throughout learning. Each component \u2013 be it advantage estimation or constrained updates \u2013 plays a role in ensuring that learning is as efficient, stable, and monotonic as possible. Together, they represent the state-of-the-art toolkit for policy optimization in reinforcement learning.</p> Method Key Idea Pros Cons REINFORCE MC return-based policy gradient Simple, unbiased Very high variance Actor\u2013Critic TD baseline value function More sample-efficient Requires critic Advantage Actor\u2013Critic Uses \\(A(s,a)\\) for updates Best bias\u2013variance trade Needs accurate value est. TRPO Trust-region with KL constraint Strong theory, stable Complex, second-order PPO Clipped/penalized surrogate objective Simple, stable, popular Heuristic, tuning needed"},{"location":"reinforcement/7_gae/#mental-map","title":"Mental Map","text":"<pre><code>                  Advanced Policy Gradient Methods\n     Goal: Fix limitations of vanilla PG (variance, stability, KL control)\n                               \u2502\n                               \u25bc\n             Core Challenges in Policy Gradient Methods\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 High variance (MC returns)                             \u2502\n       \u2502 Poor sample efficiency (on-policy only)                \u2502\n       \u2502 Sensitive to step size \u2192 catastrophic policy collapse  \u2502\n       \u2502 Small \u03b8 change \u2260 small policy change                   \u2502\n       \u2502 Reusing old data is unstable                           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Variance Reduction (Baselines)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Introduce baseline b(s) \u2192 subtract expectation         \u2502\n       \u2502 Keeps estimator unbiased                               \u2502\n       \u2502 Good choice: b(s)= V(s) \u2192 yields Advantage A(s,a)      \u2502\n       \u2502 Update based on: how much action outperformed expected \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                       Advantage Function A(s,a)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 A(s,a) = Q(s,a) \u2013 V(s)                                 \u2502\n       \u2502 Measures how much BETTER the action was vs average     \u2502\n       \u2502 Positive \u2192 increase \u03c0\u03b8(a|s); Negative \u2192 decrease it    \u2502\n       \u2502 Major variance reduction \u2013 foundation of Actor\u2013Critic  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         Actor\u2013Critic Framework\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Actor: policy \u03c0\u03b8(a|s)                                  \u2502\n       \u2502 Critic: value function V(s;w) estimates baseline       \u2502\n       \u2502 TD error \u03b4t reduces variance (bootstrapping)           \u2502\n       \u2502 Faster, more sample-efficient than REINFORCE           \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                     Target Estimation for the Critic\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Monte Carlo (\u221e-step)       \u2502  TD (1-step)               \u2502\n       \u2502 + Unbiased                 \u2502  + Low variance            \u2502\n       \u2502 \u2013 High variance            \u2502  \u2013 Biased                  \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n       \u2502 n-Step Returns: Blend of TD and MC                      \u2502\n       \u2502 Control bias\u2013variance by choosing n                     \u2502\n       \u2502 Larger n \u2192 MC-like; smaller n \u2192 TD-like                 \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n             Fundamental Problems with Vanilla Policy Gradient\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Uses each batch for ONE gradient step (on-policy)      \u2502\n       \u2502 Step size is unstable \u2192 huge performance collapse      \u2502\n       \u2502 Small changes in \u03b8 \u2192 large unintended policy changes   \u2502\n       \u2502 Need mechanism to limit POLICY CHANGE, not \u03b8 change    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n            Safe Policy Improvement Theory \u2192 TRPO &amp; PPO\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Policy Performance Difference Lemma                    \u2502\n       \u2502   J(\u03c0') \u2212 J(\u03c0) = E\u03c0' [A\u03c0(s,a)]                         \u2502\n       \u2502 KL Divergence as policy distance metric                \u2502\n       \u2502   D_KL(\u03c0'||\u03c0) small \u2192 safe update                      \u2502\n       \u2502 Monotonic Improvement Bound                            \u2502\n       \u2502   Lower bound on J(\u03c0') using surrogate loss L\u03c0(\u03c0')     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                   Surrogate Objective for Safe Updates\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 L\u03c0(\u03c0') = E[ (\u03c0'(a|s)/\u03c0(a|s)) * A\u03c0(s,a) ]               \u2502\n       \u2502 Importance sampling + KL regularization                \u2502\n       \u2502 Foundation of Trust-Region Policy Optimization (TRPO)  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n              Proximal Policy Optimization (PPO) \u2013 Key Ideas\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 PPO-KL Penalty             \u2502 PPO-Clipped Objective      \u2502\n       \u2502 Adds \u03b2\u00b7KL to loss          \u2502 Clips ratio r_t(\u03b8) to      \u2502\n       \u2502 Adjust \u03b2 adaptively        \u2502 [1\u2212\u03b5, 1+\u03b5] to prevent      \u2502\n       \u2502 Prevents large updates     \u2502 destructive policy jumps   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                         PPO Algorithm Summary\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 1. Collect trajectories from old policy                \u2502\n       \u2502 2. Estimate advantages A\u0302_t (GAE, TD, etc.)            \u2502\n       \u2502 3. Optimize clipped surrogate for many epochs          \u2502\n       \u2502 4. Update parameters safely                            \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502\n                               \u25bc\n                          Final Outcome (Chapter 6)\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 Stable and efficient policy optimization               \u2502\n       \u2502 Reuse data safely across multiple updates              \u2502\n       \u2502 Avoid catastrophic policy collapse                     \u2502\n       \u2502 Foundation of modern deep RL algorithms                \u2502\n       \u2502 (PPO, TRPO, A3C, IMPALA, SAC, etc.)                    \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/8_imitation_learning/","title":"8. Imitation Learning","text":""},{"location":"reinforcement/8_imitation_learning/#chapter-8-imitation-learning","title":"Chapter 8: Imitation Learning","text":"<p>In previous chapters, we focused on reinforcement learning with explicit reward signals guiding the agent's behavior. We assumed that a well-defined reward function \\(R(s,a)\\) was provided as part of the MDP, and the agent\u2019s goal was to learn a policy that maximizes cumulative reward. But what if specifying the reward is difficult or the agent cannot safely explore to learn from reward? Imitation Learning (IL) addresses these scenarios by leveraging expert demonstrations instead of explicit rewards.</p> <p>Imitation Learning allows an agent to learn how to act by mimicking an expert\u2019s behavior, rather than by maximizing a hand-crafted reward.</p>"},{"location":"reinforcement/8_imitation_learning/#motivation-the-case-for-learning-from-demonstrations","title":"Motivation: The Case for Learning from Demonstrations","text":"<p>Designing a reward function that truly captures the desired behavior can be extremely challenging. A misspecified reward can lead to unintended behaviors (reward hacking) or require exhaustive tuning. Even with a good reward, some environments present sparse rewards (e.g. only a success/failure signal at the very end of an episode) \u2013 making pure trial-and-error learning inefficient. In other cases, unsafe exploration is a concern: letting an agent freely explore (as classic RL would) could be dangerous or costly (imagine a self-driving car learning by crashing to discover that crashing is bad).</p> <p>However, in many of these settings expert behavior is available: we might have logs of human drivers driving safely, or demonstrations of a robot performing the task. Imitation Learning leverages this data. Instead of specifying what to do via a reward function, we show the agent how to do it via example trajectories. The agent's objective is then to imitate the expert as closely as possible.</p> <p>This paradigm contrasts with reward-based RL in key ways:</p> <ul> <li> <p>Reward-Based RL: The agent explores and learns by trial-and-error, guided by a numeric reward signal for feedback. It requires careful reward design and often extensive exploration.</p> </li> <li> <p>Imitation Learning: The agent learns from demonstrations of the desired behavior, treating the expert\u2019s actions as ground truth. No explicit reward is needed to train; learning is driven by matching the expert's behavior.</p> </li> </ul> <p>By learning from an expert, IL can produce competent policies much faster and safer in these scenarios. It essentially sidesteps the credit assignment problem of RL (because the \"right\" action is directly provided by the expert) and avoids dangerous exploration. In domains like autonomous driving, robotics, or any task where a human can demonstrate the skill, IL offers a powerful shortcut to get an agent up to a reasonable performance.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-problem-setup","title":"Imitation Learning Problem Setup","text":"<p>Formally, we can describe the imitation learning scenario using the same environment structure as an MDP \\((S, A, P, R, \\gamma)\\) except that the reward function \\(R\\) is unknown or not used. The agent still has a state space \\(S\\), an action space \\(A\\), and the environment transition dynamics \\(P(s' \\mid s, a)\\). What we do have, instead of \\(R\\), is access to expert demonstrations. An expert (which could be a human or a pre-trained optimal agent) provides example trajectories:</p> \\[ \\tau_E = (s_0, a_0, s_1, a_1, \\dots , s_T) \\] <p>collected by following the expert\u2019s policy \\(\\pi_E\\) in the environment. We may have a dataset \\(D\\) of these expert trajectories (or simply a set of state-action pairs drawn from expert behavior). The key point is that in IL, the agent does not receive numeric rewards from the environment. Instead, success is measured by how well the agent\u2019s behavior matches the expert\u2019s behavior.</p> <p>The goal of imitation learning can be stated as: find a policy \\(\\pi\\) for the agent that reproduces the expert's behavior (and ideally, achieves similar performance on the task). If the expert is optimal or highly skilled, we hope \\(\\pi\\) will achieve near-optimal results as well. This is an alternative path to finding a good policy without ever specifying a reward function explicitly or performing unguided exploration.</p> <p>(If we imagine there was some true but unknown reward \\(R\\) the expert is optimizing, then ideally \\(\\pi\\) should perform nearly as well as \\(\\pi_E\\) on that reward. IL attempts to reach that outcome via demonstrations rather than explicit reward feedback.)</p>"},{"location":"reinforcement/8_imitation_learning/#3-behavioral-cloning-learning-by-supervised-imitation","title":"3. Behavioral Cloning: Learning by Supervised Imitation","text":"<p>The most direct approach to imitation learning is Behavioral Cloning. Behavioral cloning treats imitation as a pure supervised learning problem: we train a policy to map states to the expert\u2019s actions, using the expert demonstrations as labeled examples. In essence, the agent \"clones\" the expert's behavior by learning to predict the expert's action in any given state.</p> <p>BC: Learn state to action mappings using expert demonstrations.</p> <p>In practice, we parameterize a policy \\(\\pi_\\theta(a\\mid s)\\) (e.g. a neural network with parameters \\(\\theta\\)) and adjust \\(\\theta\\) so that \\(\\pi_\\theta(\\cdot\\mid s)\\) is as close as possible to the expert\u2019s action choice in state \\(s\\). We define a loss function on the dataset of state-action pairs. For example:</p> <ul> <li>Discrete actions: Use cross-entropy (negative log-likelihood) of the expert\u2019s action.</li> </ul> \\[L(\\theta) = - \\mathbb{E}_{(s,a)\\sim D}\\left[ \\log \\pi_{\\theta}(a \\mid s) \\right]\\] <ul> <li>Continuous actions: Use mean squared error (regression loss).</li> </ul> \\[L(\\theta) = \\mathbb{E}_{(s,a)\\sim D} \\left[ \\left( \\pi_\\theta(s) - a \\right)^2 \\right]\\] <p>Minimizing these losses drives the policy to imitate the expert decisions on the training set.</p> <p>Training a behavioral cloning agent typically involves three steps:</p> <ol> <li> <p>Collect demonstrations: Gather a dataset \\(D = {(s_i, a_i)}\\) of expert state-action examples by observing the expert \\(\\pi_E\\) in the environment.</p> </li> <li> <p>Supervised learning on \\((s, a)\\) pairs: Choose a policy representation for \\(\\pi_\\theta\\) and use the collected data to adjust \\(\\theta\\). For each example \\((s_i, a_i)\\), we update \\(\\pi_\\theta\\) to reduce the error between its prediction \\(\\pi_\\theta(s_i)\\) and the expert\u2019s action \\(a_i\\). (For instance, if actions are discrete, we increase the probability \\(\\pi_\\theta(a_i \\mid s_i)\\) for the expert\u2019s action; if continuous, we move \\(\\pi_\\theta(s_i)\\) closer to \\(a_i\\) in value.)</p> </li> <li> <p>Deployment: Once the policy is trained (approximating \\(\\pi_E\\)), we fix \\(\\theta\\). The agent then acts autonomously: at each state \\(s\\), it outputs \\(a = \\pi_\\theta(s)\\) as its action. Ideally, this learned policy will behave similarly to the expert in the environment.</p> </li> </ol> <p>If the expert demonstrations are representative of the situations the agent will face, behavioral cloning can yield a policy that mimics the expert\u2019s behavior effectively. BC has some clear advantages:</p> <ul> <li> <p>Simplicity: It reduces policy learning to standard supervised learning, for which many stable algorithms and optimizations exist.</p> </li> <li> <p>Offline training: The model can be trained entirely from pre-recorded expert data, without requiring interactive environment feedback. This makes it data-efficient in terms of environment interactions.</p> </li> <li> <p>Safety: No random exploration is needed. The agent never tries highly suboptimal actions during training, since it always learns from demonstrated good behavior (critical in safety-sensitive domains).</p> </li> </ul> <p>However, purely copying the expert also comes with important limitations.</p>"},{"location":"reinforcement/8_imitation_learning/#covariate-shift-and-compounding-errors","title":"Covariate Shift and Compounding Errors","text":"<p>The main problem with behavioral cloning is that the training distribution of states can differ from the test distribution when the agent actually runs. During training, \\(\\pi_\\theta\\) is only exposed to states that the expert visited. But once the agent is deployed, if it ever deviates even slightly from the expert\u2019s trajectory, it may enter states not seen in the training data. In those unfamiliar states, the policy\u2019s predictions may be unreliable, leading to errors that cause it to drift further from expert-like behavior.</p> <p>A small mistake can snowball: once the agent strays from what the expert would do, it encounters novel situations where its learned policy might be very poor. One error leads to another, and the agent can cascade into failure because it was never taught how to recover.</p> <p>This phenomenon is known as covariate shift or distributional shift. The learner is trained on the state distribution induced by the expert policy \\(\\pi_E\\), but it is testing on the state distribution induced by its own policy \\(\\pi_\\theta\\). Unless \\(\\pi_\\theta\\) is perfect, these distributions will diverge over time, and the divergence can grow unchecked. In other words, the agent might handle situations similar to the expert's trajectories well, but if it finds itself in a situation the expert never encountered (often a result of a prior mistake), it has no guidance on what to do and can rapidly veer off course. This is often illustrated by the example of a self-driving car learned by BC: if it slightly misjudges a turn and drifts, it may end up in a part of the road it never saw during training, leading to more errors (compounding until possibly a crash).</p> <p>Another limitation is that BC does not inherently guarantee optimality or improvement beyond the expert: the policy is only as good as the demonstration data. If the expert is suboptimal or the dataset doesn\u2019t cover certain scenarios, the cloned policy will reflect those shortcomings and cannot improve by itself (since it has no feedback signal like reward to further refine its behavior). In reinforcement learning terms, BC has no notion of feedback for success or failure; it merely apes the expert, so it cannot discover better strategies or correct mistakes outside the expert's shadow.</p> <p>Researchers have developed strategies to mitigate the covariate shift problem. One approach is Dataset Aggregation (DAgger), which is an iterative algorithm: after training an initial policy via BC, let the policy interact with the environment and observe where it makes mistakes or visits unseen states; then have the expert provide the correct actions for those states, add these state-action pairs to the training set, and retrain the policy. By repeating this process, the policy\u2019s training distribution is gradually brought closer to the distribution it will encounter when it controls the agent. DAgger can significantly reduce compounding errors, but it requires ongoing access to an expert for feedback during training.</p> <p>In summary, behavioral cloning is a powerful first step for imitation learning\u2014it's straightforward and avoids many challenges of pure RL. But one must be mindful of its limitations: a blindly cloned policy can fail catastrophically when it encounters situations outside the expert\u2019s experience. This motivates more sophisticated imitation learning methods that incorporate the dynamics of the environment and attempt to infer the intent behind expert actions, rather than just copying them. We turn to those next.</p>"},{"location":"reinforcement/8_imitation_learning/#inverse-reinforcement-learning-learning-the-why","title":"Inverse Reinforcement Learning: Learning the \"Why\"","text":"<p>Behavioral cloning directly learns what to do (mapping states to actions) but does not capture why those actions are desirable. Inverse Reinforcement Learning (IRL) instead asks: Given expert behavior, what underlying reward function \\(R\\) could explain it? In other words, IRL attempts to reverse-engineer the expert's objectives from its observed behavior.</p> <p>In IRL, we assume that the expert \\(\\pi_E\\) is (approximately) optimal for some unknown reward function \\(R^*\\). The goal is to infer a reward function \\(\\hat{R}\\) such that, if an agent were to optimize \\(\\hat{R}\\), it would reproduce the expert\u2019s behavior. Formally, we want \\(\\pi_E\\) to be the optimal policy under the learned reward:</p> \\[\\pi_E = \\arg\\max_{\\pi} \\, V_R^{\\pi}\\] <p>where \\(V^{\\pi}_{\\hat{R}}\\) is the expected return of policy \\(\\pi\\) under the reward function \\(\\hat{R}\\). In words, the expert should have higher cumulative reward (according to \\(\\hat{R}\\)) than any other policy. If we can find such an \\(\\hat{R}\\), we have explained the expert\u2019s behavior in terms of incentives.</p> <p>Intuition: IRL flips the reinforcement learning problem on its head. Rather than starting with a reward and finding a policy, we start with a policy (the expert's) and try to find a reward that this policy optimizes. It's like observing an expert driver and deducing that they must be implicitly trading off goals like \"reach the destination quickly\" and \"avoid collisions\" because their driving balances speed and safety.</p> <p>One challenge is that IRL is inherently an under-defined (ill-posed) problem: many possible reward functions might make \\(\\pi_E\\) appear optimal. To resolve this ambiguity, IRL algorithms introduce additional criteria or regularization. For example, they might prefer the simplest reward function that explains the behavior, or in the case of maximum entropy IRL, prefer a reward that leads to the most random (maximally entropic) policy among those that match the expert's behavior \u2013 this avoids overly narrow explanations and spreads probability over possible behaviors unless forced by data.</p> <p>Once a candidate reward function \\(\\hat{R}(s,a)\\) is learned through IRL, the process typically continues as follows: we plug \\(\\hat{R}\\) back into the environment and solve a forward RL problem (using any suitable algorithm from earlier chapters) to obtain a policy \\(\\pi_{\\hat{R}}\\) that maximizes this recovered reward. Ideally, \\(\\pi_{\\hat{R}}\\) will then behave similarly to the expert's policy \\(\\pi_E\\) (since \\(\\hat{R}\\) was chosen to explain \\(\\pi_E\\)). The end result is an agent that not only imitates the expert, but also has an explicit reward model of the task it is performing.</p> <p>IRL is usually more complex and computationally expensive than behavioral cloning, because it often involves a nested loop: for each candidate reward function, the algorithm may need to perform an inner optimization (solving an MDP) to evaluate how well that reward explains the expert. However, IRL provides several potential benefits:</p> <ul> <li> <p>It yields a reward function, which is a portable definition of the task. This inferred reward can then be reused: for example, to train new agents from scratch, to evaluate different policies, or to modify the task (by tweaking the reward) in a principled way.</p> </li> <li> <p>It can generalize better to new situations. If the environment changes in dynamics or constraints, having \\(\\hat{R}\\) allows us to re-optimize and find a new optimal policy for the new conditions. A policy learned by pure BC might not adapt well beyond the situations it was shown, whereas a reward captures the goal and can be re-optimized.</p> </li> <li> <p>It may allow the agent to exceed the demonstrator\u2019s performance. Since IRL ultimately produces a reward function, an agent can continue to improve with further RL optimization. If the expert was suboptimal or noisy, a sufficiently good RL algorithm might find a policy that achieves an even higher reward (i.e. fine-tunes the behavior) while still aligning with the expert\u2019s intent encoded in \\(\\hat{R}\\).</p> </li> </ul> <p>In summary, IRL shifts the imitation learning problem from policy regression to reward inference. It answers a fundamentally different question: instead of directly cloning actions, infer the hidden goals that the expert is pursuing. With \\(\\hat{R}\\) in hand, we then fall back on standard RL techniques (like those from Chapters 4\u20138) to derive a policy. IRL is especially appealing in scenarios where we suspect the expert\u2019s behavior is optimizing some elegant underlying objective, and we want to uncover that objective for reuse or interpretation. The cost of IRL is the added complexity of the learning process, but the payoff is a deeper understanding of the task and potentially greater robustness and optimality of the learned policy.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-reinforcement-learning","title":"Maximum Entropy Inverse Reinforcement Learning","text":""},{"location":"reinforcement/8_imitation_learning/#principle-of-maximum-entropy","title":"Principle of Maximum Entropy","text":"<p>The entropy of a distribution \\(p(s)\\) is defined as:</p> \\[H(p) = -\\sum_{s} p(s)\\log p(s)\\] <p>The principle of maximum entropy states: The probability distribution that best represents our state of knowledge is the one with the largest entropy, given the constraints of precisely stated prior data. Consider all probability distributions consistent with the observed data. Select the one with maximum entropy\u2014i.e., the least biased distribution that fits what we know while assuming nothing extra.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-applied-to-irl","title":"Maximum Entropy Applied to IRL","text":"<p>We seek a distribution over trajectories \\(P(\\tau)\\) that:</p> <ol> <li>Has maximum entropy, and</li> <li>Matches expert feature expectations.</li> </ol> <p>Formally, we maximize:</p> \\[\\max_{P} -\\sum_{\\tau} P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)\\mu(\\tau) = \\frac{1}{|D|}\\sum_{\\tau_i \\in D} \\mu(\\tau_i)\\] \\[\\sum_{\\tau} P(\\tau) = 1\\] <p>Here:</p> <ul> <li>\\(\\mu(\\tau)\\) represents feature counts for trajectory \\(\\tau\\)</li> <li>\\(D\\) is the expert demonstration set</li> </ul> <p>This says: among all possible distributions consistent with observed expert feature averages, choose the one with maximum uncertainty.</p>"},{"location":"reinforcement/8_imitation_learning/#matching-rewards","title":"Matching Rewards","text":"<p>In linear reward IRL, we assume rewards take the form:</p> \\[r_\\phi(\\tau) = \\phi^\\top \\mu(\\tau)\\] <p>We want a policy \\(\\pi\\) that induces a trajectory distribution \\(P(\\tau)\\) matching the expert\u2019s expected reward under \\(r_\\phi\\):</p> \\[\\max_{P(\\tau)} -\\sum_{\\tau}P(\\tau)\\log P(\\tau)\\] <p>subject to:</p> \\[\\sum_{\\tau} P(\\tau)r_\\phi(\\tau) = \\sum_{\\tau} \\hat{P}(\\tau)r_\\phi(\\tau)\\] \\[\\sum_{\\tau}P(\\tau)=1\\] <p>This aligns the learner\u2019s expected reward with the expert\u2019s reward estimate.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-exponential-family-distributions","title":"Maximum Entropy \u21d2 Exponential Family Distributions","text":"<p>Using constrained optimization (Lagrangians), we obtain:</p> \\[\\log P(\\tau) = \\lambda_1 r_\\phi(\\tau) - 1 - \\lambda_0\\] <p>Thus:</p> \\[P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\] <p>This reveals a key result: The maximum entropy distribution consistent with constraints belongs to the exponential family.</p> <p>That is,</p> \\[p(\\tau|\\phi) = \\frac{1}{Z(\\phi)}\\exp(r_\\phi(\\tau))\\] <p>where</p> \\[Z(\\phi)=\\sum_{\\tau}\\exp(r_\\phi(\\tau))\\] <p>This means we can now learn \\(\\phi\\) by maximizing likelihood of observed expert data, because the trajectory distribution becomes a normalized exponential model.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-over-tau-equals-maximum-likelihood-of-observed-data-under-max-entropy-exponential-family-distribution","title":"Maximum Entropy Over \\(\\tau\\) Equals Maximum Likelihood of Observed Data Under Max Entropy (Exponential Family) Distribution","text":"<p>Jaynes (1957) showed: Maximizing entropy over trajectories = maximizing likelihood of data under the maximum-entropy distribution.</p> <p>So we:</p> <ol> <li>Assume \\(p(\\tau|\\phi)\\) has exponential form</li> <li>Learn \\(\\phi\\) by maximizing:</li> </ol> \\[\\max_{\\phi} \\prod_{\\tau \\in D} p(\\tau|\\phi)\\] <p>This allows IRL to treat expert demonstrations as data to be probabilistically explained.</p>"},{"location":"reinforcement/8_imitation_learning/#maximum-entropy-inverse-rl-algorithm","title":"Maximum Entropy Inverse RL Algorithm","text":"<p>Assuming known dynamics and linear rewards:</p> <ol> <li>Input: expert demonstrations \\(\\mathcal{D}\\)</li> <li>Initialize reward weights \\(r_\\phi\\)</li> <li>Compute optimal policy \\(\\pi(a|s)\\) given \\(r_\\phi\\) (via dynamic programming / value iteration)</li> <li>Compute state visitation frequencies \\(\\rho(s|\\phi,T)\\)</li> <li> <p>Compute gradient on reward parameters:</p> <p>\\(\\nabla J(\\phi) = \\frac{1}{N}\\sum_{\\tau_i \\in \\mathcal{D}} \\mu(\\tau_i) - \\sum_{s}\\rho(s|\\phi,T)\\mu(s)\\)</p> </li> <li> <p>Update \\(\\phi\\) via gradient step</p> </li> <li>Repeat from Step 3</li> </ol> <p>Maximum Entropy IRL assumes experts act stochastically but optimally. Instead of selecting a single best policy, it finds a distribution over trajectories consistent with expert behavior. The resulting trajectory probabilities follow: \\(\\(P(\\tau) \\propto \\exp(r_\\phi(\\tau))\\)\\)</p> <p>Learning becomes maximum likelihood estimation: find reward parameters \\(\\phi\\) that best explain expert demonstrations.</p>"},{"location":"reinforcement/8_imitation_learning/#apprenticeship-learning","title":"Apprenticeship Learning","text":"<p>Apprenticeship Learning usually refers to the scenario where an agent learns to perform a task by iteratively improving its policy using expert demonstrations as a reference. In many contexts, this term is used when an IRL algorithm is combined with policy learning: the agent behaves as an apprentice to the expert, gradually mastering the task. The classic formulation by Abbeel and Ng (2004) introduced apprenticeship learning via IRL, which guarantees that the learner\u2019s policy will perform nearly as well as the expert\u2019s, given enough demonstration data.</p> <p>One way to think of apprenticeship learning is as follows: rather than directly cloning actions, we try to match the feature expectations of the expert. Suppose we have some features \\(\\phi(s)\\) of states (or state-action pairs) that capture what we care about in the task (for example, in driving, features might include lane deviation, speed, collision count, etc.). The expert will have some expected cumulative feature values . Apprenticeship learning methods aim for the learner to achieve similar feature expectations.</p> <p>A prototypical apprenticeship learning algorithm proceeds like this:</p> <ol> <li> <p>Initialize a candidate policy (it could even start random).</p> </li> <li> <p>Evaluate how this policy behaves in terms of features (run it in simulation to estimate \\(\\mathbb{E}_{\\pi}\\left[\\sum_t \\phi(s_t)\\right]\\)).</p> </li> <li> <p>Compare the policy\u2019s behavior to the expert\u2019s behavior. Identify the biggest discrepancy in feature expectations.</p> </li> <li> <p>Adjust the reward (implicitly defined as a weighted sum of features) to penalize the discrepancy. In other words, find reward weights \\(w\\) such that the expert\u2019s advantage over the apprentice in those feature dimensions is highlighted.</p> </li> <li> <p>Optimize a new policy for this updated reward function (solve the MDP with the new \\(w\\) to get \\(\\pi_{\\text{new}}\\) that maximizes \\(w \\cdot \\phi\\)).</p> </li> <li> <p>Set this \\(\\pi_{\\text{new}}\\) as the apprentice\u2019s policy and repeat the evaluation -&gt; comparison -&gt; reward adjustment cycle.</p> </li> </ol> <p>Each iteration pushes the apprentice to close the gap on the feature that most distinguishes it from the expert. After a few iterations, this process yields a policy that matches the expert on all key feature dimensions within some tolerance. At that point, the apprentice is essentially as good as the expert with respect to any reward expressible as a combination of those features.</p> <p>The term apprenticeship learning highlights that the agent is not just mimicking blindly but is engaged in a process of improvement guided by the expert\u2019s example. Importantly, the focus is on achieving at least the expert\u2019s level of performance. We don\u2019t necessarily care about identifying the exact reward the expert had; we care that our apprentice\u2019s policy is successful. In fact, in the algorithm above, the reward weights \\(w\\) found in each iteration are intermediate tools \u2013 at the end, one can take the final policy and deploy it, without needing to stick to a single explicit reward interpretation.</p> <p>In relation to IRL, apprenticeship learning can be seen as a practical approach to use IRL for control: IRL finds a reward that explains the expert, and then the agent learns a policy for that reward; if it\u2019s not yet good enough, adjust and repeat. Modern developments in imitation learning often follow this spirit. For example, Generative Adversarial Imitation Learning (GAIL) is a more recent technique where the agent learns a policy by trying to fool a discriminator into thinking the agent\u2019s trajectories are from the expert \u2013 conceptually, the discriminator\u2019s judgment provides a sort of reward signal telling the agent how \"expert-like\" its behavior is. This can be viewed as a form of apprenticeship learning, since the agent is iteratively tweaking its policy to become indistinguishable from the expert.</p> <p>In summary, apprenticeship learning is about learning by iteratively comparing to an expert and closing the gap. It often uses IRL under the hood, but its end goal is the policy (the apprentice\u2019s skill), not necessarily the reward. It underscores a key point: in imitation learning, sometimes we care more about performing as well as the expert (a direct goal), and sometimes we care about understanding the expert\u2019s intentions (the indirect goal via IRL). Apprenticeship learning emphasizes the former.</p>"},{"location":"reinforcement/8_imitation_learning/#imitation-learning-in-the-rl-landscape","title":"Imitation Learning in the RL Landscape","text":"<p>Imitation learning fills an important niche in the overall reinforcement learning framework. It is especially useful when:</p> <ol> <li> <p>Rewards are difficult to specify: If it's unclear how to craft a reward that captures all aspects of the desired behavior, providing demonstrations can bypass this. IL shines in complex tasks (e.g. high-level driving maneuvers, dexterous robot manipulation) where manually writing a reward function would be cumbersome or prone to error.</p> </li> <li> <p>Rewards are sparse or delayed: When reward feedback is very rare or only given at the end of an episode, a pure RL agent might struggle to get enough signal to learn. An expert trajectory provides dense guidance at every time step (state-action pairs), effectively providing a shaped signal through imitation. This can jump-start learning in tasks that are otherwise too sparse for RL to crack (Chapter 4 discussed how sparse rewards make value estimation difficult \u2013 IL sidesteps that by using expert knowledge).</p> </li> <li> <p>Exploration is risky or expensive: In real-world environments like robotics, autonomous driving, or healthcare, exploring with random or untrained policies can be dangerous or costly. IL allows learning a policy without the agent ever taking unguided actions in the real environment; it learns from safe, successful behaviors demonstrated by the expert. This makes it an attractive approach when safety is a hard constraint.</p> </li> </ol> <p>It\u2019s important to note that IL is not necessarily a replacement for reward-based RL, but rather a complement to it. A common practical approach is to bootstrap an agent with imitation learning and then fine-tune it with reinforcement learning. For example, one might first use behavioral cloning to teach a robot arm the basics of a task from human demonstrations, getting it into a reasonable regime of behavior; then, if a reward function is available (even a sparse one for success), use RL to further improve the policy, possibly surpassing the human expert's performance or adapting to slight changes in the task. The initial IL phase provides a good policy prior (saving time and avoiding dangerous exploration), and the subsequent RL phase lets the agent optimize and explore around that policy to refine skills.</p> <p>On the flip side, imitation learning does require expert data. If obtaining demonstrations is hard (or if no expert exists for a brand-new task), IL might not be applicable. Moreover, if the expert demonstrations are of varying quality or contain noise, the agent will faithfully learn those imperfections unless additional measures (like filtering data or combining with RL optimization) are taken. In contrast, a pure RL approach, given a well-defined reward and enough exploration, can in principle discover superior strategies that no demonstrator provided. Thus, in practice, there is a trade-off: IL can dramatically speed up learning and improve safety given an expert, whereas RL remains the go-to when we only have a reward signal and the freedom to explore.</p> <p>Imitation learning has become a critical part of the toolbox for solving real-world sequential decision problems. It enables success in domains that might be intractable for pure reinforcement learning by providing an external source of guidance. By learning directly from expert behavior \u2013 through methods like behavioral cloning (learning the policy directly) or inverse reinforcement learning (learning the underlying reward and then the policy) \u2013 an agent can shortcut the trial-and-error process. Of course, IL introduces its own challenges (distribution shift, reliance on demonstration coverage, potential suboptimality of the expert), but these can often be managed with algorithmic innovations (DAgger, combining IL with RL, etc.). In summary, imitation learning serves as a powerful paradigm for training agents in cases where designing rewards or allowing extensive exploration is impractical, and it often works hand-in-hand with traditional RL to achieve the best results in complex environments.</p>"},{"location":"reinforcement/8_imitation_learning/#mental-map","title":"Mental map","text":"<pre><code>                    Imitation Learning (IL)\n      Goal: Learn behavior from expert demonstrations\n                     instead of explicit rewards\n                                \u2502\n                                \u25bc\n             Why Imitation Learning? (Motivation)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Hard to design rewards \u2192 reward hacking, tuning           \u2502\n \u2502 Sparse rewards \u2192 inefficient trial &amp; error                \u2502\n \u2502 Unsafe exploration (robots, driving, healthcare)          \u2502\n \u2502 Expert data available \u2192 demonstrations as guidance        \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   IL vs Reward-Based RL\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Reward-Based RL             \u2502 Imitation Learning           \u2502\n \u2502 + Explores actively         \u2502 + Learns from expert         \u2502\n \u2502 + Needs reward design       \u2502 + No explicit reward         \u2502\n \u2502 \u2013 Unsafe / inefficient      \u2502 \u2013 Depends on demo quality   \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                         IL Problem Setup\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 MDP without reward function                                \u2502\n \u2502 Access to expert trajectories \u03c4E (s,a pairs)               \u2502\n \u2502 Goal \u2192 Learn policy \u03c0 that mimics \u03c0E                       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                 Core Method 1: Behavioral Cloning (BC)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Treat imitation as supervised learning                    \u2502\n \u2502 Train \u03c0\u03b8(s) \u2192 aE using dataset D                          \u2502\n \u2502 Discrete: cross-entropy loss                              \u2502\n \u2502 Continuous: mean squared error                            \u2502\n \u2502 Advantages: simple, offline, safe                         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Key BC Problem: Covariate / Distribution Shift\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Trained only on expert states                             \u2502\n \u2502 When deployed, policy errors lead to unseen states        \u2502\n \u2502 \u2192 Poor decisions \u2192 more drift \u2192 compounding failure       \u2502\n \u2502 BC cannot recover or improve beyond expert                \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                    Fixing BC: DAgger (Idea)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Let policy act, collect mistakes                          \u2502\n \u2502 Ask expert for correct action                             \u2502\n \u2502 Add to dataset and retrain                                \u2502\n \u2502 \u2192 brings training data closer to deployment distribution  \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n       Core Method 2: Inverse Reinforcement Learning (IRL)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Learn the \u201cwhy\u201d behind actions \u2192 infer hidden reward R*   \u2502\n \u2502 Expert assumed optimal                                     \u2502\n \u2502 Solve inverse problem: \u03c0E \u2248 optimal for R*                 \u2502\n \u2502 After reward recovered \u2192 run normal RL to learn policy     \u2502\n \u2502 Benefits: generalization, interpretability, improve expert \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                Core Method 3: Apprenticeship Learning\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 Iteratively improve policy via comparing to expert        \u2502\n \u2502 Match feature expectations \u03c6(s)                           \u2502\n \u2502 Reweights reward \u2192 optimize \u2192 evaluate \u2192 repeat           \u2502\n \u2502 Goal: perform at least as well as expert                  \u2502\n \u2502 Often implemented via IRL (e.g., GAIL conceptually)       \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n           Role of IL within broader RL landscape\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 When IL is useful:                                        \u2502\n \u2502 - Reward hard to design                                   \u2502\n \u2502 - Unsafe or costly to explore                             \u2502\n \u2502 - Sparse reward tasks                                     \u2502\n \u2502 IL + RL hybrid: BC warm-start \u2192 RL fine-tune beyond expert\u2502\n \u2502 Limitations: need expert, demos may be suboptimal         \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                   Final Takeaway (Chapter Summary)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 IL bypasses reward engineering &amp; risky exploration         \u2502\n \u2502 BC learns \u201cwhat,\u201d IRL learns \u201cwhy,\u201d apprenticeship learns  \u2502\n \u2502 \u201chow to get as good as expert.\u201d                           \u2502\n \u2502 IL often combined with RL for best performance.           \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reinforcement/9_rlhf/","title":"9. RLHF","text":""},{"location":"reinforcement/9_rlhf/#chapter-9-reinforcement-learning-from-human-feedback-and-value-alignment","title":"Chapter 9: Reinforcement Learning from Human Feedback and Value Alignment","text":"<p>Designing a reward function that captures exactly what we want from a  model is extremely difficult. In open-ended tasks such as in langugae models for dialogue or summarization, we cannot easily hand-craft a numeric reward for \u201cgood\u201d behavior. This is where Reinforcement Learning from Human Feedback (RLHF) comes in. RLHF is a strategy to achieve value alignment \u2013 ensuring an AI\u2019s behavior aligns with human preferences and values \u2013 by using human feedback as the source of reward. Instead of explicitly writing a reward function, we ask humans to compare or rank outputs, and use those preferences as a training signal. Humans find it much easier to choose which of two responses is better than to define a precise numerical reward for each outcome. For example, it's simpler for a person to say which of two summaries is more accurate and polite than to assign an absolute \u201cscore\u201d to a single summary. By leveraging these relative judgments, RLHF turns human preference data into a reward model that guides the training of our policy (the language model) toward preferred behaviors.</p> <p>Pairwise preference is an intermediary point between humans having to label the correct action at every step, as in DAgger, and having to provide very dense, hand-crafted rewards. Instead of specifying what the right action is at each moment or assigning numeric rewards, humans simply compare two outputs and indicate which one they prefer. This makes the feedback process much more natural and less burdensome, while still providing a meaningful training signal beyond raw demonstrations.</p>"},{"location":"reinforcement/9_rlhf/#bradleyterry-preference-modeling-in-rlhf","title":"Bradley\u2013Terry Preference Modeling in RLHF","text":"<p>To convert human pairwise preferences into a learnable reward signal, RLHF commonly relies on the Bradley\u2013Terry model, a probabilistic model for noisy comparisons. </p> <p>Consider a \\(K\\)-armed bandit with actions \\(b_1, b_2, \\dots, b_K\\), and no state or context. A human provides noisy pairwise comparisons between actions. The probability that the human prefers action \\(b_i\\) over \\(b_j\\) is modeled as:</p> \\[ P(b_i \\succ b_j) = \\frac{\\exp(r(b_i))}{\\exp(r(b_i)) + \\exp(r(b_j))} = p_{ij} \\] <p>where \\(r(b)\\) is an unobserved scalar reward associated with action \\(b\\). Higher reward implies a higher probability of being preferred, but comparisons remain stochastic to reflect human noise and ambiguity.</p> <p>Assume we collect a dataset \\(\\mathcal{D}\\) of \\(N\\) comparisons of the form \\((b_i, b_j, \\mu)\\), where:</p> <ul> <li>\\(\\mu(1) = 1\\) if the human marked \\(b_i \\succ b_j\\)</li> <li>\\(\\mu(1) = 0.5\\) if the human marked \\(b_i = b_j\\)</li> <li>\\(\\mu(1) = 0\\) if the human marked \\(b_j \\succ b_i\\)</li> </ul> <p>We fit the reward model by maximizing the likelihood of these observations, which corresponds to minimizing the cross-entropy loss:</p> \\[ \\mathcal{L} = - \\sum_{(b_i,b_j,\\mu)\\in\\mathcal{D}} \\left[ \\mu(1)\\log P(b_i \\succ b_j) + \\mu(2)\\log P(b_j \\succ b_i) \\right] \\] <p>Optimizing this loss adjusts the reward function \\(r(\\cdot)\\) so that preferred outputs receive higher scores than dispreferred ones. This learned reward model then serves as a surrogate for human preferences.</p> <p>Once the reward model is trained using the Bradley\u2013Terry objective, it can be plugged into the RLHF pipeline. In the standard approach, the policy (language model) is optimized with PPO to maximize the learned reward while remaining close to a reference model. Conceptually, the Bradley\u2013Terry model is the critical bridge: it translates qualitative human judgments into a quantitative reward function that reinforcement learning algorithms can optimize.</p>"},{"location":"reinforcement/9_rlhf/#the-rlhf-training-pipeline","title":"The RLHF Training Pipeline","text":"<p>To train a language model with human feedback, practitioners usually follow a three-stage pipeline. Each stage uses a different training paradigm (supervised learning or reinforcement learning) to gradually align the model with what humans prefer:</p> <ol> <li> <p>Supervised Fine-Tuning (SFT) \u2013 Start with a pretrained model and fine-tune it on demonstrations of the desired behavior. For example, using a dataset of high-quality question-answer pairs or summaries written by humans, we train the model to imitate these responses. This teacher forcing stage grounds the model in roughly the right style and tone (as discussed in earlier chapters on imitation learning). By the end of SFT, the model (often called the reference model) is a strong starting point that produces decent responses, but it may not perfectly adhere to all subtle preferences or values because it was only trained to imitate the data.</p> </li> <li> <p>Reward Model Training from Human Preferences \u2013 Next, we collect human feedback in the form of pairwise preference comparisons. For many prompts, humans are shown two model-generated responses and asked which one is better (or if they are equally good). From these comparisons, we learn a reward function \\(r_\\phi(x,y)\\) (parameterized by \\(\\phi\\)) that predicts which response is more preferable for a given input x using Bradley\u2013Terry model.</p> </li> <li> <p>Reinforcement Learning Fine-Tuning \u2013 In the final stage, we use the learned reward model as a surrogate reward signal to fine-tune the policy (the language model) via reinforcement learning. The policy \\(\\pi_\\theta(y|x)\\) (with parameters \\(\\theta\\)) is updated to maximize the expected reward \\(r_\\phi(x,y)\\) of its outputs, while also staying close to the behavior of the reference model from stage 1. This last point is crucial: if we purely maximize the reward model\u2019s score, the policy might exploit flaws in \\(r_\\phi\\) (a form of \u201creward hacking\u201d) or produce unnatural outputs that, for example, repeat certain high-reward phrases. To prevent the policy from straying too far, RLHF algorithms introduce a Kullback\u2013Leibler (KL) penalty that keeps the new policy \\(\\pi_\\theta\\) close to the reference policy \\(\\pi_{\\text{ref}}\\) (often the SFT model). In summary, the RL objective can be written as:</p> \\[\\max_{\\pi_\\theta} ( \\underbrace{ \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi_\\theta(y \\mid x)} }_{\\text{Sample from policy}} \\left[ \\underbrace{ r_\\phi(x,y) }_{\\text{Want high reward}} \\right] - \\underbrace{ \\beta \\, \\mathbb{D}_{\\mathrm{KL}} \\left[ \\pi_\\theta(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\right] }_{\\text{Keep KL to original model small}}) \\] <p>where \\(\\beta&gt;0\\) controls the strength of the penalty. Intuitively, this objective asks the new policy to generate high-reward answers on the training prompts, but it subtracts points if \\(\\pi_\\theta\\) deviates too much from the original model\u2019s distribution (as measured by KL divergence). The KL term thus acts as a regularizer encouraging conservatism: the policy should only change as needed to gain reward, and not forget its broadly learned language skills or go out-of-distribution. In practice, this RL optimization is performed using Proximal Policy Optimization (PPO) (introduced in Chapter 7) or a similar policy gradient method. PPO is well-suited here because it naturally limits the size of each policy update (via the clipping mechanism), complementing the KL penalty to maintain stability.</p> </li> </ol> <p>Through this pipeline \u2013 SFT, reward modeling, and RL fine-tuning \u2013 we obtain a policy that hopefully excels at the task as defined implicitly by human preferences. Indeed, RLHF has enabled large language models to better follow instructions, avoid blatantly harmful content, and generally be more helpful and aligned with user expectations than they would be out-of-the-box. That said, the full RLHF procedure involves training multiple models (a reward model and the policy) and carefully tuning hyperparameters (like \\(\\beta\\) and PPO clip thresholds). The process can be unstable; for instance, if \\(\\beta\\) is too low, the policy might mode-collapse to only a narrow set of high-reward answers, whereas if \\(\\beta\\) is too high, the policy might hardly improve at all. Researchers have described RLHF as a \u201ccomplex and often unstable procedure\u201d that requires balancing between reward optimization and avoiding model drift. This complexity has spurred interest in whether we can achieve similar alignment benefits without a full reinforcement learning loop. </p>"},{"location":"reinforcement/9_rlhf/#direct-preference-optimization-rlhf-without-rl","title":"Direct Preference Optimization: RLHF without RL?","text":"<p>Direct Preference Optimization (DPO) is a recently introduced alternative to the standard RLHF fine-tuning stage. The key idea of DPO is to solve the RLHF objective in closed-form, and then optimize that solution directly via supervised learning. DPO manages to sidestep the need for sampling-based RL (like PPO) by leveraging the mathematical structure of the RLHF objective we defined above.</p> <p>Recall that in the RLHF setting, our goal is to find a policy \\(\\pi^*(y|x)\\) that maximizes reward while staying close to a reference policy. Conceptually, we can write the optimal policy for a given reward function in a Boltzmann (exponential) form. In fact, it can be shown (see e.g. prior work on KL-regularized RL) that the optimizer of \\(J(\\pi)\\) occurs when \\(\\pi\\) is proportional to the reference policy times an exponential of the reward:</p> \\[\\pi^*(y \\mid x) \\propto \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{1}{\\beta}\\, r_\\phi(x, y)\\right)\\] <p>This equation gives a closed-form solution for the optimal policy in terms of the reward function \\(r_\\phi\\). It makes sense: actions \\(y\\) that have higher human-derived reward should be taken with higher probability, but we temper this by \\(\\beta\\) and weight by the reference probabilities \\(\\pi_{\\text{ref}}(y|x)\\) so that we don\u2019t stray too far. If we were to normalize the right-hand side, we\u2019d write:</p> \\[\\pi^*(y \\mid x) = \\frac{ \\pi_{\\text{ref}}(y \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y)}{\\beta}\\right) }{ \\sum_{y'} \\pi_{\\text{ref}}(y' \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x,y')}{\\beta}\\right) }\\] <p>Here the denominator is a partition functionsumming over all possible responses \\(y'\\) for input \\(x\\). This normalization involves a sum over the entire response space, which is astronomically large for language models \u2013 hence we cannot directly compute \\(\\pi^*(y|x)\\) in practice. This intractable sum is exactly why the original RLHF approach uses sampling-based optimization (PPO updates) to approximate the effect of this solution without computing it explicitly.</p> <p>DPO\u2019s insight is that although we cannot evaluate the normalizing constant easily, we can still work with relative probabilities. In particular, for any two candidate responses \\(y_+\\) (preferred) and \\(y_-\\) (dispreferred) for the same context \\(x\\), the normalization cancels out if we look at the ratio of the optimal policy probabilities. Using the form above:</p> \\[\\frac{\\pi^*(y^+ \\mid x)}{\\pi^\\ast(y^- \\mid x)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^+)}{\\beta}\\right)} {\\pi_{\\text{ref}}(y^- \\mid x)\\, \\exp\\!\\left(\\frac{r_\\phi(x, y^-)}{\\beta}\\right)} = \\frac{\\pi_{\\text{ref}}(y^+ \\mid x)} {\\pi_{\\text{ref}}(y^- \\mid x)} \\exp\\!\\left( \\frac{1}{\\beta} \\big[ r_\\phi(x, y^+) - r_\\phi(x, y^-) \\big] \\right)\\] <p>Taking the log of both sides, we get a neat relationship:</p> \\[\\frac{1}{\\beta} \\big( r_\\phi(x, y^{+}) - r_\\phi(x, y^{-}) \\big) = \\big[ \\log \\pi^\\ast(y^{+} \\mid x) - \\log \\pi^\\ast(y^{-} \\mid x) \\big] - \\big[ \\log \\pi_{\\text{ref}}(y^{+} \\mid x) - \\log \\pi_{\\text{ref}}(y^{-} \\mid x) \\big]\\] <p>The term in brackets on the right is the difference in log-probabilities that the optimal policy \\(\\pi^*\\) assigns to the two responses (which in turn would equal the difference in our learned policy\u2019s log-probabilities if we can achieve optimality). What this equation tells us is: the difference in reward between a preferred and a rejected response equals the difference in log odds under the optimal policy (minus a known term from the reference model). In other words, if \\(y_+\\) is better than \\(y_-\\) by some amount of reward, then the optimal policy should tilt its probabilities in favor of \\(y_+\\) by a corresponding factor.</p> <p>Crucially, the troublesome normalization is gone in this ratio. We can rearrange this relationship to directly solve for policy probabilities in terms of rewards, or vice-versa. DPO leverages this to cut out the middleman (explicit RL). Instead of updating the policy via trial-and-error with PPO, DPO directly adjusts \\(\\pi_\\theta\\) to satisfy these pairwise preference constraints. Specifically, DPO treats the problem as a binary classification: given a context \\(x\\) and two candidate outputs \\(y_+\\) (human-preferred) and \\(y_-\\) (human-dispreferred), we want the model to assign a higher probability to \\(y_+\\) than to \\(y_-\\), with a confidence that grows with the margin of preference. We can achieve this by maximizing the log-likelihood of the human preferences under a sigmoid model of the log-probability difference.</p> <p>In practice, the DPO loss for a pair \\((x, y_+, y_-)\\) is something like:</p> \\[\\ell_{\\text{DPO}}(\\theta) = - \\log \\sigma \\!\\left( \\beta\\, \\big[ \\log \\pi_\\theta(y^{+} \\mid x) - \\log \\pi_\\theta(y^{-} \\mid x) \\big] \\right)\\] <p>where \\(\\sigma\\) is the sigmoid function. This loss is low (i.e. good) when \\(\\log \\pi_\\theta(y_+|x) \\gg \\log \\pi_\\theta(y_-|x)\\), meaning the model assigns much higher probability to the preferred outcome \u2013 which is what we want. If the model hasn\u2019t yet learned the preference, the loss will be higher, and gradient descent on this loss will push \\(\\pi_\\theta\\) to increase the probability of \\(y_+\\) and decrease that of \\(y_-\\). Notice that this is very analogous to the Bradley-Terry formulation earlier, except now we embed the reward model inside the policy\u2019s logits: effectively, \\(\\log \\pi_\\theta(y|x)\\) plays the role of a reward score for how good \\(y\\) is, up to the scaling factor \\(1/\\beta\\). In fact, the DPO derivation can be seen as combining the preference loss on \\(r_\\phi\\) with the \\(\\pi^*\\) solution formula to produce a preference loss on \\(\\pi_\\theta\\). The original DPO paper calls this approach \u201cyour language model is secretly a reward model\u201d \u2013 by training the language model with this loss, we are directly teaching it to act as if it were the reward model trying to distinguish preferred vs. non-preferred outputs.</p> <p>## Mental map</p> <p><code>text          Reinforcement Learning from Human Feedback (RLHF)    Goal: Align model behavior with human preferences and values           when explicit reward design is impractical                                 \u2502                                 \u25bc            Why Dense Rewards Are Hard for Language Models  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Open-ended tasks (dialogue, summarization, reasoning)     \u2502  \u2502 No clear numeric notion of \u201cgood\u201d behavior                \u2502  \u2502 Hand-crafted dense rewards \u2192 miss nuance, reward hacking  \u2502  \u2502 Metrics (BLEU, ROUGE, length) poorly reflect human values \u2502  \u2502 Human values are subjective, contextual, and fuzzy        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc             From Imitation Learning to Human Preferences  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Behavioral Cloning (IL): imitate demonstrations           \u2502  \u2502 + Simple, safe, no reward needed                          \u2502  \u2502 \u2013 Cannot exceed expert, sensitive to distribution shift   \u2502  \u2502 DAgger: fixes BC but requires step-by-step human labeling \u2502  \u2502 Pairwise preferences = middle ground                      \u2502  \u2502 \u2192 no dense rewards, no per-step supervision               \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc            Pairwise Preference Feedback (Key Idea)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Humans compare two outputs and choose the better one      \u2502  \u2502 Easier than assigning numeric rewards                     \u2502  \u2502 More informative than raw demonstrations                  \u2502  \u2502 Scales to complex, open-ended behaviors                   \u2502  \u2502 Forms basis of reward learning in RLHF                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc         Bradley\u2013Terry Model: Preferences \u2192 Reward Signal  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Model noisy human comparisons probabilistically           \u2502  \u2502 P(b_i \u227b b_j) = exp(r(b_i)) / (exp(r(b_i))+exp(r(b_j)))    \u2502  \u2502 r(b): latent scalar reward                                \u2502  \u2502 Fit r(\u00b7) by maximizing likelihood / cross-entropy         \u2502  \u2502 Preferred outputs get higher reward scores                \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc              RLHF Training Pipeline (3 Stages)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 1. Supervised Fine-Tuning (SFT)                           \u2502  \u2502    \u2013 Behavioral cloning on human-written demos            \u2502  \u2502    \u2013 Produces reference policy \u03c0_ref                      \u2502  \u2502                                                           \u2502  \u2502 2. Reward Model Training                                  \u2502  \u2502    \u2013 Human pairwise preferences                           \u2502  \u2502    \u2013 Train r_\u03c6(x,y) via Bradley\u2013Terry loss                \u2502  \u2502                                                           \u2502  \u2502 3. RL Fine-Tuning (PPO)                                   \u2502  \u2502    \u2013 Maximize reward r_\u03c6(x,y)                             \u2502  \u2502    \u2013 KL penalty keeps \u03c0_\u03b8 close to \u03c0_ref                  \u2502  \u2502    \u2013 Prevents reward hacking &amp; language drift             \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               RLHF Objective (KL-Regularized RL)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Maximize:                                                 \u2502  \u2502   E[r_\u03c6(x,y)] \u2212 \u03b2 \u00b7 KL(\u03c0_\u03b8 || \u03c0_ref)                      \u2502  \u2502 \u03b2 controls tradeoff:                                      \u2502  \u2502   Low \u03b2 \u2192 reward hacking / mode collapse                  \u2502  \u2502   High \u03b2 \u2192 little improvement over SFT                    \u2502  \u2502 PPO provides stable policy updates                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc           Limitations of Standard RLHF (PPO-based)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Requires training multiple models                         \u2502  \u2502 Many hyperparameters (\u03b2, PPO clip, value loss, etc.)      \u2502  \u2502 Sampling-based RL can be unstable                         \u2502  \u2502 Expensive and complex pipeline                            \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc       Direct Preference Optimization (DPO): RLHF without RL  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Solve RLHF objective in closed form                       \u2502  \u2502 Optimal policy:                                           \u2502  \u2502   \u03c0*(y|x) \u221d \u03c0_ref(y|x) \u00b7 exp(r_\u03c6(x,y)/\u03b2)                  |  \u2502 Use probability ratios \u2192 normalization cancels            \u2502  \u2502 Train \u03c0_\u03b8 directly on preference pairs                    \u2502  \u2502 Loss: sigmoid on log-prob difference                      \u2502  \u2502 \u201cYour LM is secretly a reward model\u201d                      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               DPO vs PPO-based RLHF  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 RLHF (PPO)                  \u2502 DPO                         \u2502  \u2502 + Explicit RL optimization  \u2502 + Pure supervised learning  \u2502  \u2502 \u2013 Complex &amp; unstable        \u2502 \u2013 Assumes KL-optimal form   \u2502  \u2502 \u2013 Many hyperparameters      \u2502 + Simple, stable, efficient \u2502  \u2502                             \u2502 + No separate reward model  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502                                 \u25bc               Final Takeaway (Chapter Summary)  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 Dense rewards are hard for language tasks                 \u2502  \u2502 Pairwise preferences provide natural human feedback       \u2502  \u2502 RLHF learns rewards from preferences + optimizes policy   \u2502  \u2502 DPO simplifies RLHF by removing explicit RL               \u2502  \u2502 Together, they extend imitation learning toward           \u2502  \u2502 scalable value alignment for modern language models       \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</code></p>"},{"location":"reinforcement/readme/","title":"Readme","text":"<p>https://www.youtube.com/watch?v=L6OVEmV3NcE&amp;list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX&amp;index=5</p> <p>https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;index=1</p> <p>https://www.youtube.com/watch?v=WxRDyObrm_M</p>"}]}